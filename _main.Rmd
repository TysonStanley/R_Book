---
title: "R for Researchers: An Introduction"
author: "Tyson S. Barrett, PhD"
date: "`r Sys.Date()`"
output:
  bookdown::tufte_book2:
    latex_engine: "xelatex"
  bookdown::gitbook:
    download: [epub, pdf]
documentclass: book
link-citations: yes
bibliography: [book.bib]
biblio-style: apalike
description: "This book introduces the R statistical language for researchers in the health, behavioral, educational, and psychological sciences. It is designed for those that have little background in statistical programming but would like to use the powerful statistical and visualization tool that R offers at no cost. The light-hearted design of this book allows a researcher to investigate and begin using R relatively stress-free with intuitive and interesting examples."
github-repo: "tysonstanley"
cover-image: "Cover.jpg"
---


```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(rio)
library(furniture)
```


# Preface {-}

Before beginning the book, you will need to download the `R` software [www.r-project.org](https://www.r-project.org/) and then the RStudio software [www.rstudio.com](https://www.rstudio.com/). `R` will just need to be on your computer but you won't interact with it directly. `RStudio`[^free] is actually how we will end up using `R`. You can think of `R` as the computer processor while `RStudio` is the keyboard and screen. `R` actually does the work but you'll only use it by using `RStudio`.

> In general, I will refer to anything you do in `RStudio` as using `R`. So, when I say things like: "To make a plot in `R`, we are going to ...", I do not mean to open `R` and start doing stuff there. Instead, I mean to keep using `RStudio` but will not refer to the things we do as in `RStudio` but as in `R`.

Once both are installed (helps on installing the software can be found on [www.rstudio.com](https://www.rstudio.com/), [www.r-bloggers.com](www.r-bloggers.com), and [www.statmethods.net](www.statmethods.net)) you are good to go. The remainder of the book will be about actually using it.

The book is divided into three parts.

## Part I {-}

1. Chapter 1: The Basics
2. Chapter 2: Working with and Cleaning Your Data
3. Chapter 3: Understanding Your Data (summary statistics, ggplot2)

## Part II {-}

4. Chapter 4: Basic Statistical Analyses (T-tests, ANOVA, Linear Regression)
5. Chapter 5: Generalized Linear Models
6. Chapter 6: Multilevel Modeling
7. Chapter 7: Other Modeling Techniques

## Part III {-}

8. Chapter 8: Advanced data manipulation
9. Chapter 9: Advanced plotting
10. Chapter 10: Where to go from here

At the end of the book, you should be able to: 1) use `R` to perform your data cleaning and data analyses and 2) understand online helps (e.g. [www.stackoverflow.com](www.stackoverflow.com), [www.r-bloggers.com](www.r-bloggers.com)) so your potential in `R` becomes nearly limitless.

```{r, echo = FALSE, eval = FALSE}
# To build the books
bookdown::render_book(input = list.files(pattern = "*.Rmd"),
                      output_format = "bookdown::gitbook")
bookdown::render_book(input = list.files(pattern = "*.Rmd"),
                      output_format = "bookdown::tufte_book2", 
                      latex_engine = "xelatex")
bookdown::render_book(input = list.files(pattern = "*.Rmd"),
                      output_format = "bookdown::epub_book")
```

Enjoy![^return]


[^free]: Get the free version of `Rstudio`. Believe me, it doesn't feel like it should be free software. 

[^return]: Note that to return to Tyson's blog, you can click [here][blog]

[blog]: https://tysonstanley.github.io




<!--chapter:end:00_index.Rmd-->


# Chapter 1: The Basics {-}

> "Success is neither magical nor mysterious. Success is the natural consequence of consistently applying the basic fundamentals." --- Jim Rohn

`R` is an open source statistical software made by statisticians. This means it generally speaks the language of statistics. This is very helpful when it comes running analyses but can be confusing when starting to understand the code. 

This book was written to help my students begin to use `R` for research across the health, behavioral, educational, and psychological sciences. The best way to begin to learn `R` (in my opinion) is by jumping right into it and using it. To do so, as we learn new concepts, we will go through two main sections: 

1. Introduction to the concept with examples of its use
3. Application of the concept through projects provided at [tysonbarrett.com](http://tysonbarrett.com)

As we start working with data in `R`, we will go through several main themes, including importing and cleaning data, reshaping and otherwise wrangling[^wrangle] data, assessing data via visualizations and tables, and running common statistical tests.

This chapter will provide the background and foundation to start using `R`. This background revolves around data frames and functions---two general types of objects. 

## Objects {-}

`R` is built on a few different types of virtual objects. An object, just like in the physical world, is something you can do things with. In the real world, we have objects that we use regularly. For example, we have chairs. Chairs are great for some things (sitting on, sleeping on, enjoying the beach) and horrible at others (playing basketball, flying, floating in the ocean). Similarly, in `R` each type of object is useful for certain things. The data types that we will discuss below are certain types of objects.

![Chair](chair.jpg)

Because this is so analogous to the real world, it becomes quite natural to work with. You can have many objects in the computer's memory, which allows flexbility in analyzing many different things simply within a single `R` session.[^rsession] The main object types that you'll work with are presented in the following table (others exist that will come up here and there).

```{r, echo=FALSE}
df = data.frame(
  "Object" = c("Vector", "Data Frame", "Function", "Operator"),
  "Description" = c("A single column of data ('a variable')",
                    "Multiple vectors put together with observations as rows and variables as columns (much like a Spreadsheet that you have probably used before)",
                    "Takes input and produces output---the workhorse of R",
                    "A special type of function (e.g. `<-`)")
)
knitr::kable(df)
```

Each of these objects will be introduced in this chapter, highlighting their definition and use. For your work, the first objects you work with will be data in various forms. Below, we explain the different data types and how they can combine into what is known as a `data.frame`.

> Early Advice: Don't get overwhelmed. It may feel like there is a lot to learn, but taking things one at a time will work surprisingly quickly. I've designed this book to discuss what you need to know from the beginning. Other topics that are not discussed are things you can learn later and do not need to be of your immediate concern.


## Data Types {-}

To begin understanding data in `R`, you must know about vectors. Vectors are, in essence, a single column of data---a variable. In `R` there are three main vector data types (variable types) that you'll work with in research:

- numeric
- factor
- character

The first, **numeric**, is just that: numbers. In `R`, you can make a numeric variable with the code below:

```{r numeric-example}
x <- c(10.1, 2.1, 4.6, 2.3, 8.9, 4.5, 7.2, 7.9, 3.6, 2.0)
```

The `c()` is a **function** [^function] that stands for "concatenate" which basically glues the values inside the paratheses together into one. We use `<-` to put it into `x`. So in this case, `x` (which we could have named anything) is saving those values so we can work with them[^obj]. If we ran this code, the `x` object would be in the working memory of `R` and will stay there unless we remove it or until the end of the `R` session (i.e., we close `R`).

A **factor** variable is a categorical variable (i.e., only a limited number of options exist). For example, race/ethnicity is a factor variable.

```{r}
race <- c(1, 3, 2, 1, 1, 2, 1, 3, 4, 2)
```

The code above actually produces a numeric vector (since it was only provided numbers). We can quickly tell `R` that it is indeed supposed to be a factor.

```{r}
race <- factor(race, 
               labels = c("white", "black", "hispanic", "asian"))
```

The `factor()` function tells `R` that the first thing---`race`---is actually a factor. The additional argument `labels` tells `R` what each of the values means. If we print out `race` we see that `R` has replaced the numeric values with the labels.

```{r}
race
```

Finally, and maybe less relevantly, there are character variables. These are words (known as strings). In research this is often where subjects give open responses to a question. These can be used somewhat like factors in a lot of situations.

```{r}
ch <- c("I think this is great.", 
        "I would suggest you learn R.", 
        "You seem quite smart.")
```

When we combine multiple variables into one, we create a **data.frame**. A data frame is like a spreadsheet table, like the ones you have probably seen in Microsoft's Excel and IBM's SPSS. Here's a simple example:

```{r}
df <- data.frame(x, race)
df
```

We can do quite a bit with the `data.frame` that we called `df`[^df]. Once again, we could have called this data frame anything (e.g., `jimmy`, `susan`, `rock`, `scissors` all would work), although I recommend short names. If we hadn't already told `R` that `race` was a factor, we could do this once it is inside of `df` by:  

```{r}
df$race <- factor(df$race, 
               labels = c("white", "black", "hispanic", "asian"))
```

In the above code, the `$` reaches into `df` to grab a variable (or column). There are other ways of doing this as is almost always the case in `R`. For example, you may see individuals using:

```{r, eval=FALSE}
df[["race"]] <- factor(df[["race"]] , 
                       labels = c("white", "black", "hispanic", "asian"))
```

`df[["race"]]` grabs the `race` variable just like `df$race`. Although there are many ways of doing this, we are going to focus on the modern and intuitive ways of doing this more often.

In my experience, the first element of uncomfort that researchers face is the fact that our data is essentially invisible to us. It is "housed" in `df` (not totally accurate but can be thought of this way for now) but it is hard to know exactly what is in `df`. First, we can use:

```{r, eval=FALSE}
View(df)
```

which prints out the data in spreadsheet-type form to see the data. We can also get nice summaries using the following functions.

```{r}
names(df)
str(df)
summary(df)
```

## Functions {-}

Earlier we mentioned that `c()` was a "function." Functions are how we do things with our data. There are probably hundreds of thousands of functions at your reach. In fact, you can create your own! We'll discuss that more in later chapters.

For now, know that each named function has a name (the function name of `c()` is "c"), arguments, and output of some sort. Arguments are the information that you provide the function between the parenthases (e.g. we gave `c()` a bunch of numbers; we gave `factor()` two arguments---the variable and the labels for the variable's levels). Output from a function varies to a massive degree but, in general, the output is what you are using the function for (e.g., for `c()` we wanted to create a vector---a variable---of data where the arguments we gave it are glued together into a variable). 

At any point, by typing:
```{r, eval=FALSE}
?functionname
```
we get information in the "Help" window. It provides information on how to use the function, including arguments, output, and examples.

Sometimes `R` by itself does not have a function that you need. We can download and use functions in **packages**[^packages]. We will be using several functions from several packages. To install them, we can use the `install.packages()` function,

```{r, eval = FALSE}
install.packages("rio")
```

This puts the package called `rio` on your computer for you but in order to use the functions it provides, we use

```{r, eval = FALSE}
library(rio)
```

We now can use the functions from `rio` in this `R` session (i.e. from when you open `R` to when you close it).

After a quick note about operators, you will be shown several functions for both importing and saving data. Note that each have a name, arguments, and output of each. 

### Operators {-}

A special type of function is called an operator. These take two inputs---a left hand side and a right hand side---and output some value. A very common operator is `<-`, known as the assignment operator. It takes what is on the right hand side and assigns it to the left hand side. We saw this earlier with vectors and data frames. Other operators exist, a few of which we will introduce in the following chapter. But again, an operator is just a special function. 

## Importing Data {-}

Most of the time you'll want to import data into `R` rather than manually entering it line by line, variable by variable.

We will rely on the `rio` package to do the majority of our data importing. In general, researchers in our fields use `.csv`, `.txt`, `.sav`, and `.xlxs`. For the most part, importing will be straightforward. Regardless of the file type, we will use `rio::import()`. This notation means that the package `rio` has a function called `import()`.

```{r, eval = FALSE}
## Import a CSV file located in the same folder as your RMarkdown file
our_data <- rio::import("your_data_file.csv")
```

This will work for `.csv`, `.txt`, `.sav`, `.xlsx`, and [many others files](https://thomasleeper.com/rio/articles/rio.html). In the example, we assigned our imported file to `df` using `<-`. This is necessary to let `R` remember the data for it to use for other stuff. The object `our_data` now contains the data saved in the CSV file called `"your_data_file.csv"`. Note that at the end of the lines you see that I left a **comment** using `#`. I used two for stylistic purposes but only one is necessary. Anything after a `#` is not read by the computer; it's just for us humans.

> Heads up! It is important to know where your data file is located. In my experience, this is where students struggle the most at the very beginning. If the file is in the same folder as the RStudio project, we can use the `here::here()` function to point to the project's folder. More will be discussed about RStudio projects after the discussion on saving data.

If you have another type of data file to import that `rio::import()` doesn't work well with, online helps found on sites like [www.stackoverflow.com](www.stackoverflow.com) and [www.r-bloggers.com](www.r-bloggers.com) often have the solution.


## Saving Data {-}

Occassionally you'll want to save some data that you've worked with (usually this is not necessary). When necessary, you can use `rio::export()`.

```{r, eval=FALSE}
rio::export(df, file="file.csv")  ## to create a CSV data file
```

`R` automatically saves missing data as `NA` since that is what it is in `R`. But often when we write a CSV file, we might want it as blank or some other value. If that's the case, we can add another argument `na = " "` after the file argument.

Again, if you ever have questions about the specific arguments that a certain function has, you can simply run `?functionname`. So, if you were curious about the different arguments in `export` simply run: `?export`. In the pane with the files, plots, packages, etc. a document will show up to give you more informaton.


## Apply it {-}

[This link](http://tysonbarrett.com/DataR/Chapter1.zip) contains a folder complete with an Rstudio project file, an RMarkdown file, and a few data files. Download it and unzip it to do the following steps.


### Step 1 {-}

Open the `Chapter1.Rproj` file. This will open up RStudio for you.

### Step 2 {-}

Once RStudio has started, in the panel on the lower-right, there is a `Files` tab. Click on that to see the project folder. You should see the data files and the `Chapter1.Rmd` file. Click on the `Chapter1.Rmd` file to open it. It should open in the upper-left panel. It has words and code already put in it. Starting at the first code chunk (has <code>```{r}</code> right above it), click on the first line and run the code. The code can be run using "Cmd + "Enter" or the Run button at the top of RStudio.

### Step 3 {-}

Run the code to import the CSV file. At the end of that code chunk there is a link that is simply `df`. Run that line. It should print out some of the data in `df`. If you see the data printed out, you have done these steps correctly. Feel free to play around with this file and import the other data file types and run other code we've shown in this chapter.



## Conclusions {-}

`R` is designed to be flexible and do just about anything with data that you'll need to do as a researcher. With this chapter under your belt, you can now read basic `R` code, import and save your data. The next chapter will introduce the "tidyverse" of methods that can help you join, reshape, summarize, group, and much more.



[^rsession]: An `R` session is any time you open `R` do work and then close `R`. Unless you are saving your workspace (which, in general you shouldn't do), it starts the slate clean--no objects are in memory and no packages are loaded. This is why we use scripts. Also, it makes your workflow extra transparent and replicable.

[^function]: `R` is all about functions. Functions tell `R` what to do with the data. You'll see many more examples throughout the book.

[^obj]: This is a great feature of `R`. It is called "object oriented" which basically means `R` creates objects to work with. I discuss this more in 1.2. Also, the fact that I said `x` "saves" the information is not entirely true but is useful to think this way.

[^df]: I used this name since `df` is common in online helps and other resources.

[^packages]: A package is an extension to `R` that gives you more functions--abilities--to work with data. Anyone can write a package, although to get it on the Comprehensive R Archive Network (CRAN) it needs to be vetted to a large degree. In fact, after some practice, you could write a package to help you more easily do your work.

[^delim]: The delimiter is what separates the pieces of data.

[^wrangle]: The hip term for working with data including selecting variables, filtering observations, creating new variables, etc.

<!--chapter:end:01-TheLanguage.Rmd-->


# Chapter 2: Working with and Cleaning Your Data {-}

> "Organizing is what you do before you do something, so that when you do it, it is not all mixed up." --- A. A. Milne

In order to work with and clean your data in the most modern and straightforward way, we are going to be using the "tidyverse" group of methods. The tidyverse[^hadley] is a group of packages[^pack] that provide a simple syntax that can do many basic (and complex) data manipulating. They form a sort of "grammar" of data manipulation that simplifies both the coding approach and the way researchers think about working with data. 

The group of packages can be downloaded via:
```{r, eval=FALSE}
install.packages("tidyverse")
```

After downloading it, to use its functions, simply use:
```{r}
library(tidyverse)
```

Note that when we loaded tidyverse it loaded several packages and told you of "conflicts". These conflicts are where two or more functions across different packages have the same name. These functions with the same name will almost invariably differ in what they do. In this situation, the last loaded package is the one that `R` will use by default. For example, if we loaded two packages---`awesome` and `amazing`---and both had the function `make_really_cool` and we loaded `awesome` and then `amazing` as so:
```{r, eval=FALSE}
library(awesome)
library(amazing)
```
`R` will automatically use the function from `amazing`. We can still access the `awesome` version of the function (because, again, even though the name is the same, they won't necessarily do the same things for you). We can do this by:

```{r, eval=FALSE}
awesome::make_really_cool(args)
```

In essence, the `::` grabs the function from inside of the package and let's you use that. 

That's a bit of an aside, but know that you can always get at a function even if it is "masked" from your current session.

## Tidy Methods {-}

I'm introducing this to you for a couple reasons.

1. It simplifies the code and makes the code more readable. It is often worthwhile to make sure the code is readable for, as the saying goes, there are always at least two collaborators on any project: you and future you.
2. It is the cutting edge of all things `R`. The most influential individuals in the `R` world, including the makers and maintainers of `RStudio`, use these methods and syntax.

The majority of what you'll need to do with data as a researcher will be covered by these functions. The goal of these packages is to help tidy up your data. *Tidy data* is based on columns being variables and rows being observations. This depends largely on your data and research design but the definition is still the same---columns are variables and rows are observations. It is the form that data needs to be in to analyze it, whether that analysis is by graphing, modeling, or other means.

There are several methods that help create tidy data:

1. Piping
2. Selecting and Filtering
3. Mutate and Transmute
4. Grouping and Summarizing
5. Reshaping
6. Joining (merging)


> Heads up! Understanding these tools requires an understanding of what ways data can be moved around. For example, reshaping can refer to moving data into a more wide-format or long-format, can refer to summarizing or aggregating, and can refer to joining or binding. All of these are necessary to work with data flexibly. Because of this, we suggest taking your time to fully understand what each function is doing with the data.

Much of these may be things you have done in other tools such as spreadsheets. The copy-and-paste approach is seriously error prone and is not reproducible. Taking your time to learn these methods will be well worth it.

## Tidy (Long) Form {-}

Before diving in, I want to provide some examples of tidy data (also known as "long form") and how they are different from another form, often called "wide form."

Only when the data is cross-sectional and each individual is a row does this distinction not matter much. Otherwise, if there are multiple measurement periods per individual (longitudinal design), or there are multiple individuals per cluster (clustered design), the distinction between wide and long is very important for modeling and visualization.

### Wide Form {-}

Wide form generally has one unit (i.e. individual) per row. This generally looks like:

```{r, echo=FALSE}
data.frame("ID"=c(1:10), "Var_Time1"=rnorm(10), "Var_Time2"=runif(10))
```

Notice that each row has a unique ID, and some of the columns (all in this case) are variables at specific time points. This means that the variable is split up and is not a single column. This is a common way to collect and store data in our fields and so your data likely looks similar to this.

### Long Form {-}

In contrast, long format has the lowest nested unit as a single row. This means that a single ID can span multiple rows, usually with a unique time point for each row as so:

```{r, echo=FALSE}
data.frame("ID"=c(1,1,1,1,2,2,3,3,3), 
           "Time"=c(1,2,3,4,1,2,1,2,3), 
           "Var"=runif(9))
```

Notice that a single ID spans multiple columns and that each row has only one time point. Here, time is nested within individuals making it the lowest unit. Therefore, each row corresponds to a single time point. Generally, this is the format we want for most modeling techniques and most visualizations.

This is a very simple example of the differences between wide and long format. Although much more complex examples are probably available to you, we will start with this for now. Chapter 8 will delve into more complex data designs and how to reshape those. Toward the end of this chapter, we show reshaping with more simple data structures.


### Data Used for Examples {-}

To help illustrate each aspect of working with and cleaning your data, we are going to use real data from the National Health and Nutrition Examiniation Survey (NHANES). I've provided this data at [https://tysonstanley.github.io/assets/Data/NHANES.zip](https://tysonstanley.github.io/assets/Data/NHANES.zip). I've cleaned it up somewhat already.

Let's quickly read that data in so we can use it throughout the remainder of this chapter.

```{r, echo=FALSE}
## Run but not shown
library(rio)
dem_df <- import("~/Dropbox/GitHub/blog_rstats/assets/Data/NHANES_demographics_11.xpt")
med_df <- import("~/Dropbox/GitHub/blog_rstats/assets/Data/NHANES_MedHeath_11.xpt")
men_df <- import("~/Dropbox/GitHub/blog_rstats/assets/Data/NHANES_MentHealth_11.xpt")
act_df <- import("~/Dropbox/GitHub/blog_rstats/assets/Data/NHANES_PhysActivity_11.xpt")
```

```{r, eval=FALSE}
library(rio)
dem_df <- import("NHANES_demographics_11.xpt")
med_df <- import("NHANES_MedHeath_11.xpt")
men_df <- import("NHANES_MentHealth_11.xpt")
act_df <- import("NHANES_PhysActivity_11.xpt")
```

Now we have four separate, but related, data sets in memory:

1. `dem_df` containing demographic information
2. `med_df` containing medical health information
3. `men_df` containing mental health information
4. `act_df` containing activity level information

Since all of them have all-cap variable names, we are going to quickly change this with a little trick:

```{r}
names(dem_df) <- tolower(names(dem_df))
names(med_df) <- tolower(names(med_df))
names(men_df) <- tolower(names(men_df))
names(act_df) <- tolower(names(act_df))
```
This takes the names of the data frame (on the right hand side), changes them to lower case and then reassigns them to the names of the data frame.[^names]

We will now go through each aspect of the tidy way of working with data using these four data sets.

## Piping {-}

Let's introduce a few major themes in this tidyverse. First, the pipe operator -- `%>%`. It helps simplify the code and makes things more readable. It takes what is on the left hand side and puts it in the right hand side's function.

```{r, eval=FALSE}
dem_df %>% head()
```

So the above code takes the data frame `dem_df` and puts it into the `head` function. This does the same thing as `head(df)`. 

To illustrate its strength, consider the following example. We take `dem_df`, group it by gender, and then count.

```{r}
dem_df %>%
  group_by(riagendr) %>%
  summarize(mean_age = mean(ridageyr, na.rm = TRUE))
```

Without using the pipe, we would need to use something like:

```{r}
summarize(group_by(dem_df, riagendr), mean_age = mean(ridageyr, na.rm = TRUE))
```

Although it is fewer lines, this is notably harder to understand. Instead of reading from left to right, top to bottom, we must read inside out, which is unnatural for English reading individuals.



## Select Variables and Filter Observations {-}

We often want to subset our data in some way before we do many of our analyses. We may want to do this for a number of reasons (e.g., easier cognitively to think about the data, the analyses depend on the subsetting). The code below show the two main ways to subset your data: 

1. *selecting* variables and 
2. *filtering* observations.

To select three variables (i.e. gender ["riagendr"], age ["ridageyr"], and ethnicity ["ridreth1"]) we:
```{r, eval = FALSE}
selected_dem <- dem_df %>%
  select(riagendr, ridageyr, ridreth1)
```

Now, `selected_dem` has three variables and all the observations.

We can also filter (i.e. take out observations we don't want):
```{r, eval = FALSE}
filtered_dem <- dem_df %>%
  filter(riagendr == 1)
```

Since when `riagendr == 1` the individual is male, `filtered_dem` only has male participants. We can add multiple filtering options as well:
```{r, eval = FALSE}
filtered_dem <- dem_df %>%
  filter(riagendr == 1 & ridageyr > 16)
```

We now have only males that are older than 16 years old. We used `&` to say we want **both** conditions to be met. Alternatively, we could use:
```{r, eval = FALSE}
filtered_dem <- dem_df %>%
  filter(riagendr == 1 | ridageyr > 16)
```
By using `|` we are saying we want males **or** individuals older than 16. In other words, if either are met, that observation will be kept.

Finally, we can do all of these in one step:
```{r, eval = FALSE}
filtered_selected_dem <- dem_df %>%
  select(riagendr, ridageyr, ridreth1) %>%
  filter(riagendr == 1 & ridageyr > 16)
```
where we use two `%>%` operators to grab `dem_df`, select the three variables, and then filter the rows that we want.

## Mutate Variables {-}

Earlier we showed how to create a new variable or clean an existing one. We used the base `R` way for these. However, a great way to add and change variables is using `mutate()`. We grab `df`, and create a variable called `citizen` using the factored version of `dmdcitzn` and assign it back into `df`.

```{r}
## Our Grouping Variable as a factor
dem_df <- dem_df %>%
  mutate(citizen = factor(dmdcitzn))
```
The benefit is in the readability of the code. We repeat things like the name of the data frame much less. I highly recommend working this way.


## Grouping and Summarizing {-}

A major aspect of analysis is comparing groups. Lucky for us, this is very simple in `R`. I call it the three step summary:

1. Data
2. Group by
3. Summarize

```{r}
## Three step summary:
dem_df %>%                           ## 1. Data
  group_by(citizen) %>%              ## 2. Group by
  summarize(N = n())                 ## 3. Summarize
```

The output is very informative. The first column is the grouping variable and the second is the N (number of individuals) by group. We can quickly see that there are four levels, currently, to the citizen variable. After some reading of the documentation we see that `1 = Citizen` and `2 = Not a Citizen`. A value of `7` it turns out is a placeholder value for missing. And finally we have an NA category. It's unlikely that we want those to be included in any analyses, unless we are particularly interested in the missingness on this variable. So let's do some simple cleaning to get this where we want it. To do this, we will use the `furniture` package.

```{r, eval=FALSE}
install.packages("furniture")
```

```{r, message=FALSE, warning=FALSE}
dem_df <- dem_df %>%
  mutate(citizen = washer(citizen, 7),                ## Changes all 7's to NA's
         citizen = washer(citizen, 2, value = 0))     ## Changes all 2's to 0's
```

Now, our citizen variable is cleaned, with `0` meaning not a citizen and `1` meaning citizen. Let's rerun the code from above with the three step summary:

```{r}
## Three step summary:
dem_df %>%                           ## 1. Data
  group_by(citizen) %>%              ## 2. Group by
  summarize(N = n())                 ## 3. Summarize
```

Its clear that the majority of the subjects are citizens. We can also check multiple variables at the same time, just separating them with a comma in the `summarize` function.

```{r}
## Three step summary:
dem_df %>%                           ## 1. Data
  group_by(citizen) %>%              ## 2. Group by
  summarize(N = n(),                 ## 3. Summarize
            Age = mean(ridageyr, na.rm=TRUE))                 
```

We used the `n()` function (which gives us counts) and the `mean()` function which, shockingly, gives us the mean. Note that if there are `NA`'s in the variable, the mean (and most other functions like it) will give the result `NA`. To have `R` ignore these, we tell the `mean` function to remove the `NA`'s when you compute this using `na.rm=TRUE`.

We can also use `furniture::table1()` to do the three-step summary. By default, `table1()` will give you the mean and SD of numeric variables and counts and percentages for factors.

```{r}
## Three step summary:
dem_df %>%                           ## 1. Data
  group_by(citizen) %>%              ## 2. Group by
  table1(Age = ridageyr,             ## 3. Summarize
         Sex = factor(riagendr))                 
```

This pattern of grouping and summarizing is something that will follow us throughout the book. It's a great way to get to know your data well and to make decisions on what to do next with your data.

## Reshaping {-}

Earlier we described the differences between wide and long (or tidy) form. Now that you have an idea of the differences, we are going to introduce how to change from one to the other. Several functions exist for just this purpose, including `gather()` from the `tidyr` package and `reshape()` in the default `stats` package. Since these can be limited in certain situations, we are going to teach two functions that can be used in nearly any reshaping situation. For now, though, we will keep in simple.

First, we will go from wide to long form using `long()` from the `furniture` package.[^furniture1] We are using the ficticious data for the example of wide format from above.
```{r}
#library(furniture)
df_wide <- data.frame("ID"=c(1:10), 
                      "Var_Time1"=rnorm(10), 
                      "Var_Time2"=runif(10))
df_long <- long(df_wide, 
                c("Var_Time1", "Var_Time2"),
                v.names = "Var")
df_long
```
We provided the data (`df_wide`), the time varying variables (`c("Var_Time1", "Var_Time2")`), and told it what we should name the value variable (`v.names = "Var"`). Note that the function guessed, based on its name, that the variable `ID` was the identifying variable.

This function automatically assumes each observation is a time point, thus the `time` variable. We can easily change that by adding the argument `timevar` and giving it a name (e.g., `timevar = "cluster"`).

And now we will go from long to wide using `wide()` from the same package.
```{r}
df_long <- data.frame("ID"=c(1,1,1,1,2,2,3,3,3), 
                      "Time"=c(1,2,3,4,1,2,1,2,3), 
                      "Var"=runif(9))
df_wide <- wide(df_long, 
                v.names = c("Var"),
                timevar = "Time")
df_wide
```
Here, we provided the data `df_long` and the variable name (`Var`) that had the values and (`Time`) that contained the time labels (in this case, just numbers). With a little bit of code we can move data around without any copy-pasting that is so error-prone. Again, note that the function guessed, based on its name, that the variable `ID` was the identifying variable.


## Joining (merging) {-}

The final topic in the chapter is joining data sets. This is common in many situations including large surveys (e.g., a demographics set, physical activity set, family characteristics set), health records (financial data, doctors notes, diagnosis data), and longitudinal studies (data from wave I, data from wave II).

We currently have 4 data sets that have mostly the same people in them but with different variables. One tells us about the demographics; another gives us information on mental health. We may have questions that ask whether a demographic characteristics is related to a mental health factor. This means we need to merge, or join, our data sets.[^bind]

When we merge a data set, we combine them based on some ID variable(s). Here, this is simple since each individual is given a unique identifier in the variable `seqn`. Within the `dplyr` package (part of the tidyverse) there are four main joining functions: `inner_join`, `left_join`, `right_join` and `full_join`. Each join combines the data in slightly different ways. 

The figure below shows each type of join in Venn Diagram form. Each are discussed below.

![Joining](Joining.jpg)


### Inner Join {-}

Here, only those individuals that are in both data sets that you are combining will remain. So if person "A" is in data set 1 and not in data set 2 then he/she will not be included.

```{r, eval=FALSE}
inner_join(df1, df2, by="IDvariable")
```

### Left or Right Join {-}

This is similar to inner join but now if the individual is in data set 1 then `left_join` will keep them even if they aren't in data set 2. `right_join` means if they are in data set 2 then they will be kept whether or not they are in data set 1.

```{r, eval=FALSE}
left_join(df1, df2, by="IDvariable")   ## keeps all in df1
right_join(df1, df2, by="IDvariable")  ## keeps all in df2
```

### Full Join {-}

This one simply keeps all individuals that are in either data set 1 or data set 2. 

```{r, eval=FALSE}
full_join(df1, df2, by="IDvariable")
```

Each of the left, right and full joins will have missing values placed in the variables where that individual wasn't found. For example, if person "A" was not in df2, then in a full join they would have missing values in the df1 variables.

For our NHANES example, we will use `full_join` to get all the data sets together. Note that in the code below we do all the joining in the same overall step. 

```{r}
df <- dem_df %>%
  full_join(med_df, by="seqn") %>%
  full_join(men_df, by="seqn") %>%
  full_join(act_df, by="seqn")
```

So now `df` is the the joined data set of all four. We started with `dem_df` joined it with `med_df` by `seqn` then joined that joined data set with `men_df` by `seqn`, and so on.

For many analyses in later chapters, we will use this new `df` object that is the combination of all the data sets that we had before. Whenever this is the case, this data will be explicitly referenced. Note, that in addition to what was shown in this chapter, a few other cleaning tasks were done on the data. This final version of `df` can be found at [tysonstanley.github.io/R](http://www.tysonstanley.github.io/R).



## Wrap it up {-}

Let's put all the pieces together that we've learned in the chapter together on this new `df` data frame we just created. Below, we using piping, selecting, filtering, grouping, and summarizing.

First, let's create an overall depression variable that is the sum of all the depression items (we could do IRT or some other way of combining them but that is not the point here). Below, we do three things:

1. We clean up each depression item using `furniture::washer()` since both "7" and "9" are placeholders for missing values. We take override the original variable with the cleaned one.
2. Next, we sum all the items into a new variable called `dep`.
3. Finally, we create a dichotomized variable called `dep2` using nested `ifelse()` functions. The `ifelse()` statements read: if condition holds (`dep` is greater than or equal to `10`), then `1`, else if condition holds (`dep` is less than `10`), then `0`, else `NA`. The basic build of the function is: `ifelse(condition, value if true, value if false)`.

In the end, we are creating a new data frame called `df2` with the new and improved `df` with the items cleaned and depression summed and dichotomized.
```{r}
df2 <- df %>%
  mutate(dpq010 = washer(dpq010, 7,9),
         dpq020 = washer(dpq020, 7,9),
         dpq030 = washer(dpq030, 7,9),
         dpq040 = washer(dpq040, 7,9),
         dpq050 = washer(dpq050, 7,9),
         dpq060 = washer(dpq060, 7,9),
         dpq070 = washer(dpq070, 7,9),
         dpq080 = washer(dpq080, 7,9),
         dpq090 = washer(dpq090, 7,9),
         dpq100 = washer(dpq100, 7,9)) %>%
  mutate(dep = dpq010 + dpq020 + dpq030 + dpq040 + dpq050 +
               dpq060 + dpq070 + dpq080 + dpq090) %>%
  mutate(dep2 = factor(ifelse(dep >= 10, 1,
                       ifelse(dep < 10, 0, NA))))
```

After these adjustments, let's select, filter, group by, and summarize.

```{r}
df2 %>%
  select(ridageyr, riagendr, mcq010, dep) %>%
  filter(ridageyr > 10 & ridageyr < 40) %>%
  group_by(riagendr) %>%
  summarize(asthma = mean(mcq010, na.rm=TRUE),
            depr   = mean(dep, na.rm=TRUE))
```

We can see that males (`riagendr = 1`) have nearly identical asthma levels but lower depression levels than their female counterparts. 

## General Cleaning of the Data Set {-}

A package known as `janitor` provides some nice functions that add to the ideas presented in this chapter. Specifically, two functions are of note:

1. `clean_names()` -- cleans up the names of the data frame to make them easier to type and make sure it works well with `R`.
2. `remove_empty()` -- removes either empty columns or empty rows (columns or rows that have only missing values).

These can be used in tandem, like so:
```{r}
df3 <- df2 %>%
  janitor::clean_names() %>%
  janitor::remove_empty("cols") %>%
  janitor::remove_empty("rows")
```


Ultimately, I hope you see the benefit to using these methods. With these methods, you can clean, reshape, and summarize your data. Because these are foundational, we will apply these methods throughout the book. 



## Apply It {-}

[This link](http://tysonbarrett.com/DataR/Chapter2.zip) contains a folder complete with an Rstudio project file, an RMarkdown file, and a few data files. Download it and unzip it to do the following steps.


### Step 1 {-}

Open the `Chapter2.Rproj` file. This will open up RStudio for you.

### Step 2 {-}

Once RStudio has started, in the panel on the lower-right, there is a `Files` tab. Click on that to see the project folder. You should see the data files and the `Chapter2.Rmd` file. Click on the `Chapter2.Rmd` file to open it. In this file, import the data, create a new variable, select three varaibles, and summarize a variable using the three step summary.

Once that code is in the file, click the `knit` button. This will create an HTML file with the code and output knitted together into one nice document. This can be read into any browser and can be used to show your work in a clean document.




[^bind]: Note that this is different than adding new rows but not new variables. Merging requires that we have at least some overlap of individuals in both data sets.

[^names]: Note that these are not particularly helpful names, but they are the names provided in the original data source. If you have questions about the data, visit [http://wwwn.cdc.gov/Nchs/Nhanes/Search/Nhanes11_12.aspx](http://wwwn.cdc.gov/Nchs/Nhanes/Search/Nhanes11_12.aspx).

[^hadley]: Hadley Wickham (2016). tidyverse: Easily Install and Load 'Tidyverse' Packages. R package version 1.0.0. https://CRAN.R-project.org/package=tidyverse

[^pack]: Remember, a package is an extension to `R` that gives you more functions that you can easily load into `R`.

[^furniture1]: Although the furniture package isn't in the `tidyverse`, it is a valuable package to use with the other `tidyverse` packages.

<!--chapter:end:02-Tidyverse.Rmd-->



# Chapter 3: Understanding and Describing Your Data {-}

> "If you can’t explain it simply, you don’t understand it well enough." --- Albert Einstein

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Run but not shown
## Getting data ready for the examples
library(foreign)
library(furniture)
library(tidyverse)
load("~/Dropbox/GitHub/blog_rstats/assets/Data/NHANES_2012.rda")
```

We are going to take what we've learned from the previous two chapters and use them together to have simple but powerful ways to understand your data. This chapter will be split into two sections:

1. Descriptive Statistics
2. Visualizations

The two go hand-in-hand in understanding what is happening in your data before you attempt any modeling procedures. We are often most interested in three things when exploring our data: *understanding distributions*, *understanding relationships*, and *looking for outliers or errors*.

## Descriptive Statistics {-}

Several methods of discovering descriptives in a succinct way have been developed for `R`. My favorite (full disclosure: it is one that I made so I may be biased) is the `table1` function in the `furniture` package.

This function has been designed to be simple and complete. It produces a well-formatted table that you can easily export and use as a table in a report or article.[^tab1] We'll use the `df` object we created in Chapter 2.

Let's get descriptive statistics for five of the variables: asthma, race, depression, family size, and sedentary behavior.
```{r, message=FALSE, warning=FALSE}
#library(furniture)
df %>%
  table1(asthma, race, dep, famsize, sed)
```
This quickly gives you means and standard deviations or counts and percentages.

The code below shows the means/standard devaitions or counts/percentages by a grouping variable---in this case, `C`.
```{r, message=FALSE, warning=FALSE}
df %>%
  group_by(asthma) %>%
  table1(race, dep, famsize, sed)
```

We can also test for differences by group as well.
```{r, message=FALSE, warning=FALSE}
df %>%
  group_by(asthma) %>%
  table1(race, dep, famsize, sed,
         test = TRUE)
```
Several other options exist for you to play around with, including obtaining medians and ranges and removing a lot of the white space of the table.

With three or four short lines of code we can get a good idea about variables that may be related to the grouping variable and any missingness in the factor variables. There's much more you can do with `table1` and there are vignettes and tutorials available to learn more.[^tutorials]

Other quick descriptive functions exist; here are a few of them.
```{r, eval = FALSE}
## install with install.packages("psych")
psych::describe(df)  
```
```{r, eval = FALSE}
## install with install.packages("Hmisc")
Hmisc::describe(df) 
```
```{r, eval = FALSE}
## install with install.packages("janitor")
janitor::tabyl(df)
```
There is truly no shortage of descriptive information that you can obtain within `R`.


## Visualizations {-}

Understanding your data, in my experience, almost always requires visualizations[^viz]. If we are going to use a model of some sort, understanding the distributions and relationships beforehand are very helpful in interpreting the model and catching errors in the data. Also finding any outliers or errors that could be highly influencing the modeling should be understood beforehand.

For simple but appealing visualizations we are going to be using `ggplot2`. This package is used to produce professional level plots for many journalism organizations (e.g. five-thrity-eight). These plots are quickly presentation quality and can be used to impress your boss, your advisor, or your friends.

### Using `ggplot2` {-}

This package is included in the `tidyverse` and is built on the "Grammar of Graphics" principles of data visualization. In essence, it is built on adding layers to a plot. Below, we walk through quick ways to start to visualize our data.

First, we have a nice `qplot` function that is short for "quick plot." It quickly decides what kind of plot is useful given the data and variables you provide.
```{r}
qplot(df$sed)          ## Makes a simple histogram
qplot(df$sed, df$dep)  ## Makes a scatterplot
```

For a bit more control over the plot, you can use the `ggplot` function. The first piece is the `ggplot` piece. From there, we add layers. These layers generally start with `geom_` then have the type of plot. Below, we start with telling `ggplot` the basics of the plot and then build a boxplot. 

The key pieces of ggplot:

1. `aes()` is how we tell `ggplot()` to look at the variables in the data frame.
2. Within `aes()` we told it that the x-axis is the variable "C" and the y-axis is the variable "D" and then we color it by variable "C" as well (which we told specifically to the boxplot).
3. `geom_` functions are how we tell `ggplot` what to plot---in this case, a boxplot.

These same pieces will be found throughout `ggplot` plotting. In later chapters we will introduce much more in relation to these plots.
```{r}
ggplot(df, aes(x=sed, y=dep)) +
  geom_boxplot(aes(color = sed))
```

Here's a few more examples:
```{r}
ggplot(df, aes(x=sed)) +
  geom_bar(stat="count")
```

```{r}
ggplot(df, aes(x=sed, y=dep)) +
  geom_point(aes(color = famsize))
```

*Note that the warning that says it removed a row is because we had missing values in in these variables.*

```{r}
ggplot(df, aes(x=sed, 
               y=dep, 
               group = race,
               color = race)) +
  geom_point() +
  geom_smooth(method = "lm")
```

We are going to make the first one again but with some aesthetic adjustments. Notice that we just added two extra lines telling `ggplot2` how we want some things to look.[^look]
```{r}
ggplot(df, aes(x=famsize, y=dep, group = famsize)) +
  geom_boxplot(aes(color = riagendr)) +
  theme_bw() +
  scale_color_manual(values = c("dodgerblue4", "coral2"))
```

The `theme_bw()` makes the background white, the `scale_color_manual()` allows us to change the colors in the plot. You can get a good idea of how many types of plots you can do by going to [http://docs.ggplot2.org/current][ggplot2]. Almost any informative plot that you need to do as a researcher is possible with `ggplot2`.

We will be using `ggplot2` extensively in the book to help understand our data and our models as well as communicate our results.

## Apply It {-}

[This link](http://tysonbarrett.com/DataR/Chapter3.zip) contains a folder complete with an Rstudio project file, an RMarkdown file, and a few data files. Download it and unzip it to do the following steps.


### Step 1 {-}

Open the `Chapter3.Rproj` file. This will open up RStudio for you.

### Step 2 {-}

Once RStudio has started, in the panel on the lower-right, there is a `Files` tab. Click on that to see the project folder. You should see the data files and the `Chapter3.Rmd` file. Click on the `Chapter3.Rmd` file to open it. In this file, import the data, create a descriptive table with `table1()` and three different types of visualizations (e.g., boxplot, histogram, scatterplot).

Once that code is in the file, click the `knit` button. This will create an HTML file with the code and output knitted together into one nice document. This can be read into any browser and can be used to show your work in a clean document.



[^tab1]: It is called "table1" because a nice descriptive table is often found in the first table of many academic papers.

[^tutorials]: [tysonstanley.github.io](tysonstanley.github.io)

[ggplot2]: http://docs.ggplot2.org/current/

[^look]: This is just scratching the surface of what we can change in the plots.

[^viz]: If you'd like to learn more about data visualization, see Kieran Healy's *Data Visualization: A Practical Introduction* at [http://socviz.co](http://socviz.co).

<!--chapter:end:03-UnderstandData.Rmd-->



```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Run but not shown
## Getting data ready for the examples
library(foreign)
library(furniture)
library(tidyverse)
library(anteo)
load("~/Dropbox/GitHub/blog_rstats/assets/Data/NHANES_2012.rda")
```


# Chapter 4: Basic Analyses {-}

> "The goal is to turn data into information, and information into insight." --- Carly Fiorina

In this chapter we are going to demonstrate basic modeling in `R`. Lucky for us, `R` is built for these analyses. It is actually quite straight-forward to run these types of models and analyze the output. Not only that, but there are simple ways to compare models.

We will go through the **ANOVA** family of analyses, the **linear regression** models, and look at **diagnostics** of each.


## ANOVA {-}

ANOVA stands for **an**alysis **o**f **va**riance. It is a family of methods (e.g. ANCOVA, MANOVA) that all share the fact that they compare a continuous dependent variable by a grouping factor variable (and may have multiple outcomes or other covariates). 

$$
Y_i = \alpha_0 + \alpha_1 \text{Group}_i + e_i
$$
Since the groups are compared using "effect coding," the $\alpha_0$ is the grand mean and each of the group level means are compared to it.

To run an ANOVA model, you can simply use the `aov` function. In the example below, we are analyzing whether family size (although not fully continuous it is still useful for the example) differs by race.

```{r}
df$race <- factor(df$ridreth1, 
                  labels=c("MexicanAmerican", "OtherHispanic", "White", "Black", "Other"))
df$famsize <- as.numeric(df$dmdfmsiz)

fit <- aov(famsize ~ race, df)
anova(fit)
```

We make sure the variables are the right type, then we use the `aov` function. Inside of the function we have what is called a formula. It has the general structure: `leftside ~ rightside`. Generally, the left side is an outcome variable and the right side is the predictor (i.e. independent) variable. Here, we have `race` predicting `famsize`. We assign the model to the name `fit` which is a common way of denoting it is a model. Finally, we use the `anova` function to output a nice ANOVA table. 

In the output we see the normal ANOVA table and we can see the p-value (`Pr(>F)`) is very, very small and thus is quite significant. We can look at how the groups relate using a box plot. We will be using some of the practice you got in Chapter 3 using `ggplot2` for this.

```{r}
library(ggplot2)

ggplot(df, aes(x=race, y=famsize)) +
  geom_boxplot(aes(color=race)) +
  scale_color_manual(guide=FALSE, 
                     values=c("dodgerblue3", "coral2", "chartreuse4", 
                              "darkorchid", "firebrick2")) +
  theme_bw()
```

This immediately gives us an idea of where some differences may be occuring. It would appear that "White" and "MexicanAmerican" groups are different in family size.

## Assumptions {-}

We also would like to make sure the assumptions look like they are being met. In ANOVA, we want the residuals to be distributed normally, the variance of each group should be approximately the same, the groups are assumed to be randomly assigned, and the sample should be randomly selected as well. 

In `R` we can get some simple graphical checks using `plot`. All we provide is our ANOVA object (here it is `fit`). The line before it `par(mfrow=c(1,2))` tells `R` to have two plots per row (the 1 means one row, 2 means two columns).

```{r, fig.height=4, fig.width=5}
par(mfrow=c(1,2))
plot(fit)
```

Here, it looks like we have a problem with normality (see the Normal Q-Q plot). Those dots should approximately follow the dotted line, which is not the case. In the first plot (Residuals vs. Fitted) suggests we have approximate homoskedasticity.


## Linear Modeling {-}

Linear regression is nearly identical to ANOVA. In fact, a linear regression with a continuous outcome and categorical predictor is exactly the same (if we use effect coding). For example, if we run the same model but with the linear regression function `lm` we get the same ANOVA table.

```{r}
fit2 <- lm(famsize ~ race, data=df)
anova(fit2)
```

Surprise! It is the same as before. Here we can also use the `summary` function and we get the coefficients in the model as well (using dummy coding). The first level of the categorical variable is the reference group (the group that the others are compared to). We also get the intercept (in this case, the average value of the reference group).

```{r}
summary(fit2)
```


## Assumptions {-}

Linear regression has a few important assumptions, often called "Gauss-Markov Assumptions". These include:

1. The model is linear in parameters.
2. Homoskedasticity (i.e. the variance of the residual is roughly uniform across the values of the independents).
3. Normality of residuals.

Numbers 2 and 3 are fairly easy to assess using the `plot()` function on the model object as we did with the ANOVA model. The linear in parameters suggests that the relationship between the outcome and independents is linear.
```{r, fig.height=4, fig.width=5}
par(mfrow=c(1,2))
plot(fit2)
```


## Comparing Models {-}

Often when running linear regression, we want to compare models and see if one fits significantly better than another. We also often want to present all the models in a table to let our readers compare the models. We will demonstrate both.

### Compare Statistically {-}

Using the `anova()` function, we can compare models statistically.

```{r}
anova(fit, fit2)
```

The `anova()` function works with all sorts of modeling schemes and can help in model selection. Not surprisingly, when we compared the ANOVA and the simple linear model, they are *exactly* the same in overall model terms (the only difference is in how the cateogrical variable is coded---either effect coding in ANOVA or dummy coding in regression). For a more interesting comparison, lets run a new model with an additional variable and then make a comparison.

```{r}
fit3 = lm(famsize ~ race + marriage, data=df)
summary(fit3)
```

Notice that the variable is associated with the outcome according to the t-test seen in the summary. So we would expect that `fit3` is better than `fit2` at explaining the outcome, which we see in the output below.

```{r}
anova(fit2, fit3)
```


### Compare in a Table {-}

We can also compare the models in a well-formatted table that makes many aspects easy to compare.
Two main packages allow us to compare models:

1. `stargazer`
2. `texreg`

Both provide simple functions to compare multiple models. For example, `stargazer` provides:

```{r}
library(stargazer)
stargazer(fit2, fit3,
          type = "text")
```


## When Assumptions Fail {-}

There are many things we can try when our assumptions fail. In my opinion, the best and most interpretable way is to use a Generalized Linear Model (GLM) which is discussed in the next chapter. There are a few other things you can try which I'll show here. But, keep in mind that these things can cause other problems. For example, to fix normality we may accidentally cause heteroskedasticity. With that in mind, here are some common methods to help a model fit better.

### Log-Linear, Log-Log, Linear-Log, Other {-}

Sounds like a great tongue-twister? Well, it is but it's also three ways of specifying (i.e. deciding what is in) your model better.

**Log-Linear** is where we adjust the outcome variable by a natural log transformation. This is done easily in `R`:

```{r, eval=FALSE}
df$log_outcome <- log(df$outcome)

lm(log_outcome ~ var1, data=df)
```

**Log-Log** is where we adjust both the outcome and the predictor variable with a log transformation. This is also easily done:

```{r, eval=FALSE}
df$log_outcome <- log(df$outcome)
df$log_var1    <- log(df$var1)

lm(log_outcome ~ log_var1, data=df)
```


**Linear-Log** is where we adjsut just the predictor variable with a log transformation. And, you guessed it, this is easily done in `R`:

```{r, eval=FALSE}
df$log_var1 <- log(df$var1)

lm(outcome ~ log_var1 + var2, data=df)
```

**Other** methods such as square rooting the outcome or using some power function (e.g. square, cube) are also quite common. There are functions that look for the best transformation to use. However, I will not cover it here since I think GLM's are better. So if you want to learn about other ways to help your linear model go to the next chapter.


## Interactions {-}

Many times hypotheses dealing with human beings include interactions between effects. Interactions are when the effect of one variable depends on another variable. For example, the effect of marital status on family size may depend on whether the individual is a minority. In fact, this is the hypothesis we'll test below.

Including interactions in ANOVA and regression type models are very simple in `R`. Since interpretations of interaction effects are often best through plots, we will also show simple methods to visualize the interactions as well.

### Interactions in ANOVA {-}

In general, we refer to ANOVA's with interactions as "2-way Factorial ANOVA's". We interact race and marriage status in this ANOVA. For simplicity, we created a binary race variable called minority using the `ifelse()` function. We explain this in more depth in Chapter 5.
```{r}
df$minority <- factor(ifelse(df$race == "White", 0, 1), 
                      labels = c("White", "Minority"))
fit_anova <- aov(famsize ~ minority*marriage, df)
anova(fit_anova)
```
Notice two things: First, the interaction is significant (p = .003). This is important since we are going to try to interpret this interaction. Second, by including `minority*marriage` we get both the main effects and the interaction. This is very important for interpretation purposes so you can thank `R` for making it a bit more easy on you. 

We can check the assumptions the same way as before:
```{r, fig.height=4, fig.width=5}
par(mfrow=c(1,2))
plot(fit_anova)
```
Again, the assumptions are not met for this model. But, if we ignore that for now, we can quickly find a way to interpret the interaction.

We first create a new data set that is composed of every possible combination of the variables in the model. This allows us to get unbiased estimates for the plotting.
```{r}
newdata <- expand.grid(minority = levels(df$minority),
                       marriage = levels(df$marriage))
newdata$preds <- predict(fit_anova, newdata=newdata)
```

We now use `ggplot2` just as before.
```{r}
ggplot(newdata, aes(x = marriage, y = preds, group = minority)) +
  geom_line(aes(color = minority)) +
  geom_point(aes(color = minority)) +
  labs(y = "Predicted Family Size",
       x = "Marital Status") +
  scale_color_manual(name = "",
                     values = c("dodgerblue3", "chartreuse3")) +
  theme_anteo_wh()           ## from anteo package
```

The plot tells use a handful of things. For example, we see minorities generally have more children across marital statuses. However, the difference is smaller for married and divorced individuals compared to widowed, separated, never married, and living with a partner. There's certainly more to gleen from the plot, but we won't waste your time.


### Interactions in Linear Regression {-}

Interactions in linear regression is nearly identical as in ANOVA, except we use dummy coding. It provides a bit more information. For example, we get the coefficients from the linear regression whereas the ANOVA does not provide this. We can run a regression model via:
```{r}
fit_reg <- lm(famsize ~ minority*marriage, df)
summary(fit_reg)
```
We used `summary()` to see the coefficients. If we used `anova()` it would have been the same as the one for the ANOVA. 

We can use the exact same methods here as we did with the ANOVA, including checking assumptions, creating a new data set, and using `ggplot2` to check the interaction. We won't repeat it here so you can move on to Chapter 5.


## Apply It {-}

[This link](http://tysonbarrett.com/DataR/Chapter4.zip) contains a folder complete with an Rstudio project file, an RMarkdown file, and a few data files. Download it and unzip it to do the following steps.


### Step 1 {-}

Open the `Chapter4.Rproj` file. This will open up RStudio for you.

### Step 2 {-}

Once RStudio has started, in the panel on the lower-right, there is a `Files` tab. Click on that to see the project folder. You should see the data files and the `Chapter4.Rmd` file. Click on the `Chapter4.Rmd` file to open it. In this file, import the data and run each type of statistical analysis presented in this chapter (there are others that are presented in Chapters 5, 6, and 7 as well that you do not need to do yet).

Once that code is in the file, click the `knit` button. This will create an HTML file with the code and output knitted together into one nice document. This can be read into any browser and can be used to show your work in a clean document.




<!--chapter:end:04-BasicAnalyses.Rmd-->



```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Run but not shown
## Getting data ready for the examples
library(foreign)
library(furniture)
library(tidyverse)
library(anteo)
load("~/Dropbox/GitHub/blog_rstats/assets/Data/NHANES_2012.rda")
```


# Chapter 5: Generalized Linear Models {-}

> "You must stick to your conviction, but be ready to abandon your assumptions." --- Dennis Waitley

Generalized Linear Models (GLM's) are extensions of linear regression to areas where assumptions of normality and homoskedasticity do not hold. There are several versions of GLM's, each for different types and distributions of outcomes. We are going to go through several of the most common. 

This chapter is to introduce the method very briefly and demonstrate how to perform one in `R`. We do not delve into the details of each method much, but rather focus on showing the quirks of the coding.

We discuss:

1. Logistic Regression
2. Poisson Regression
3. GLM with Gamma distribution
4. Negative binomial
5. Beta Regression

## Logistic Regression {-}

For binary outcomes (e.g., yes or no, correct or incorrect, sick or healthy), logistic regression is a fantastic tool that provides useful and interpretable information. Much like simple and multiple linear regression, logistic regression[^logistic] uses dummy coding and provides coefficients that tell us the relationship between the outcome and the independent variables.

Since the outcome is binary, we use a statistical transformation to make things work well. This makes it so the outcome is in "log-odds." A simple exponentiation of the coefficients and we get very useful "odds ratios." These are very common in many fields using binary data.

Luckily, running a logistic regression is simple in `R`. We first create the binary outcome variable called `dep`. We use a new function called `mutate` to create a new variable (we could do this a number of ways but this is probably the cleanest way).

```{r}
## First creating binary depression variable
df <- df %>%
  mutate(dep = dpq010 + dpq020 + dpq030 + dpq040 + dpq050 +
               dpq060 + dpq070 + dpq080 + dpq090) %>%
  mutate(dep2 = ifelse(dep >= 10, 1,
                ifelse(dep < 10, 0, NA)))
```
Note that we added the values from the ten variables that give us an overall depression score (`dep`). We then use `ifelse()` to create a binary version of depression called `dep2` with a cutoff of $\geq 16$ meaning depressed. Because there are missing values denoted as "NA" in this variable, we use a "nested ifelse" to say:

1. IF depression $\geq 10$ then dep2 is 1, 
2. IF dpression $< 10$, then dep2 is 0, 
3. ELSE dep2 is NA.

Note that these nested `ifelse()` statements can be as long as you want. We further need to clean up the asthma and sedentary variables.
```{r, message=FALSE, warning=FALSE}
## Fix some placeholders
df <- df %>%
  mutate(asthma = washer(mcq010, 9),
         asthma = washer(asthma, 2, value = 0)) %>%
  mutate(sed = washer(pad680, 9999, 7777))
```

Now let's run the logistic regression:
```{r, message=FALSE, warning=FALSE}
l_fit <- glm(dep2 ~ asthma + sed + race + famsize,
             data = df,
             family = "binomial")
summary(l_fit)
```

We used `glm()` (stands for generalized linear model). The key to making it logistic, since you can use `glm()` for a linear model using maximum likelihood instead of `lm()` with least squares, is `family = "binomial"`. This tells `R` to do a logistic regression.


## Poisson Regression {-}

As we did in logistic regression, we will use the `glm()` function. The difference here is we will be using an outcome that is a count variable. For example, the sedentary variable (`sed`) that we have in `df` is a count of the minutes of sedentary activity.

```{r, message=FALSE, warning=FALSE}
p_fit <- glm(sed ~ asthma + race + famsize,
             data = df,
             family = "poisson")
summary(p_fit)
```

Sedentary may be over-dispersed (see plot)
```{r, echo=FALSE, message=FALSE, warning=FALSE}
qplot(df$sed, alpha = .75, binwidth = 75) + 
  theme_bw() +
  labs(x = "Minutes of Sedentary Behavior") +
  scale_alpha(guide=FALSE)
```
and so other methods related to poisson may be necessary. For this book, we are not going to be delving into these in depth but we will introduce some below.


### Gamma {-}

Regression with a gamma distribution are often found when analyzing costs in dollars. It is very similar to poisson but does not require integers and can handle more dispersion. However, the outcome must have values $> 0$. Just for demonstration:
```{r, message=FALSE, warning=FALSE}
## Adjust sed
df$sed_gamma <- df$sed + .01
g_fit <- glm(sed_gamma ~ asthma + race + famsize,
             data = df,
             family = "Gamma")
summary(g_fit)
```

### Two-Part or Hurdle Models {-}

We are going to use the `pscl` package to run a hurdle model. These models are built for situations where there is a count variable with many zeros ("zero-inflated"). The hurdle model makes slightly different assumptions regarding the zeros than the pure negative binomial that we present next. The hurdle consists of two models: one for whether the person had a zero or more (binomial) and if more than zero, how many (poisson).

To run a hurdle model, we are going to make a sedentary variable with many more zeros to illustrate and then we will run a hurdle model.
```{r, message=FALSE, warning=FALSE}
## Zero inflated sedentary (don't worry too much about the specifics)
df$sed_zero <- ifelse(sample(1:100, 
                             size = length(df$sed), 
                             replace=TRUE) %in% c(5,10,11,20:25), 0, 
                      df$sed)
## Hurdle model
library(pscl)
h_fit = hurdle(sed_zero ~ asthma + race + famsize,
               data = df)
summary(h_fit)
```
Notice that the output has two parts: "Count model coefficients (truncated poisson with log link):" and "Zero hurdle model coefficients (binomial with logit link):". Together they tell us about the relationship between the predictors and a count variable with many zeros.


### Negative Binomial {-}

Similar to that above, negative binomial is for zero-inflated count variables. It makes slightly different assumptions than the hurdle and doesn't use a two-part approach. In order to run a negative binomial model we'll use the `MASS` package and the `glm.nb()` function.
```{r, eval=FALSE, warning=FALSE, message=FALSE}
library(MASS)
fit_nb <- glm.nb(sed_zero ~ asthma + race + famsize,
                 data = df)
summary(fit_nb)
```
Note that this model is not really appropriate because our data is somewhat contrived. 


## Beta Regression {-}

For outcomes that are bound between a lower and upper bound, Beta Regression is a great method. For example, if we are looking at test scores that are bound between 0 and 100. It is a very flexible method and allows for some extra analysis regarding the variation. 

For this, we are going to use the `betareg` package. But first, we are going to reach a little and create a ficticiously bound variable in the data set.
```{r, message=FALSE, warning=FALSE}
## Variable bound between 0 and 1
df$beta_var <- sample(seq(.05, .99, by = .01), 
                      size = length(df$asthma),
                      replace = TRUE)
library(betareg)
fit_beta <- betareg(beta_var ~ asthma + race + famsize,
                    data = df)
summary(fit_beta)
```

The output provides coefficients and the "Phi" coefficients. Both are important parts of using beta regression but we are not going to discuss it here. 

There are many resources available to learn more about beta regression and each of these GLM's. As for now, we are going to move on to more complex modeling where there are clustering or repeated measures in the data. 

## Apply It {-}

[This link](http://tysonbarrett.com/DataR/Chapter4.zip) contains a folder complete with an Rstudio project file, an RMarkdown file, and a few data files. Download it and unzip it to do the following steps.


### Step 1 {-}

Open the `Chapter4.Rproj` file. This will open up RStudio for you.

### Step 2 {-}

Once RStudio has started, in the panel on the lower-right, there is a `Files` tab. Click on that to see the project folder. You should see the data files and the `Chapter4.Rmd` file. Click on the `Chapter4.Rmd` file to open it. In this file, import the data and run each type of statistical analysis presented in this chapter (there are others that are presented in Chapter 4---that you may have done already---and Chapters 6 and 7 that you do not need to do yet).

Once that code is in the file, click the `knit` button. This will create an HTML file with the code and output knitted together into one nice document. This can be read into any browser and can be used to show your work in a clean document.


## Conclusions {-}

One of the great things about `R` is that most modeling is very similar to the basic `lm()` function. In all of these GLM's the arguments are nearly all the same: a formula, the data, and family of model. As you'll see for Multilevel and Other Models chapters, this does not change much. Having a good start with basic models and GLM's gets you ready for nearly every other modeling type in `R`.


[^logistic]: Technically, logistic regression is a linear regression model.

<!--chapter:end:05-GLMs.Rmd-->



```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Run but not shown
## Getting data ready for the examples
library(foreign)
library(furniture)
library(tidyverse)
library(anteo)
load("~/Dropbox/GitHub/blog_rstats/assets/Data/NHANES_2012.rda")
```


# Chapter 6: Multilevel Modeling {-}

> "Simplicity does not precede complexity, but follows it." --- Alan Perlis

Multilevel data are more complex and don't meet the assumptions of regular linear or generalized linear models. But with the right modeling schemes, the results can be very interpretable and actionable. Two powerful forms of multilevel modeling are:

1. Generalized Estimating Equations (GEE)
2. Mixed effects (ME; i.e., hierarchical linear modeling, multilevel modeling)

Several similarities and differences should be noted briefly. As for similarities, they both attempt to control for the lack of independence within clusters, although they do it in different ways. Also, they are both built on linear regression which makes them flexible and powerful at finding relationships in the data.

The differences are subtle but important. First, the interpretation is somewhat different between the two. GEE is a population-averaged (e.g., marginal) model whereas ME is subject specific. In other words, *GEE is the average effect* while *ME is the effect found in the average person*. In a linear model, these coefficients are the same but when we use different forms such as logistic or poisson, these can be quite different (although in my experience they generally tell a similar story). Second, ME models are much more complex than the GEE models and can struggle with convergence compared to the GEE. This also means that GEE's are generally fitted much more quickly. Still the choice of the modeling technique should be driven by your hypotheses and not totally dependent on speed of the computation.

First, if we needed to, we'd reshape our data so that it is ready for the analyses (see Chapter 8 for more on reshaping). For both modeling techniques we want our data in long form[^longform]. What this implies is that each row is an observation. What this actually means about the data depends on the data. For example, if you have repeated measures, then often data is stored in wide form---a row is an individual. To make this long, we want each time point within a person to be a row---a single individual can have multiple rows but each row is a unique observation.

Currently, our data is in long form since we are working within community clusters within this data. So, each row is an observation and each cluster has multiple rows. Note that although these analyses will be within community clusters instead of within subjects (i.e. repeated measures), the overall steps will be the exact same.

This chapter certainly does not cover all of multilevel modeling in `R`. Entire books are dedicated to that single subject. Rather, we are introducing the methods and the packages that can be used to start using these methods. 

## GEE {-}

There are two packages, intimately related, that allow us to perform GEE modeling---`gee` and `geepack`. These have some great features and make running a fairly complex model pretty simple. However, as great as they are, there are some annoying shortcomings. We'll get to a few of them throughout this section.

GEE's, in general, want a few pieces of information from you. First, the outcome and predictors. This is just as in linear regression and GLM's. Second, we need to provide a correlation structure. This tells the model the approximate pattern of correlations between the time points or clusters. It also wants a variable that tells the cluster ID's. Finally, it also wants the family (i.e. the type of distribution).

Since this is not longitudinal, but rather clustered within communities, we'll assume for this analysis an unstructured correlation structure. It is the most flexible and we have enough power for it here.

For `geepack` to work, we need to filter out the missing values for the variables that will be in the model.
```{r, message=FALSE, warning=FALSE}
df2 <- df %>%
  filter(complete.cases(dep, famsize, sed, race, asthma))
```

Now, we'll build the model with both packages (just for demonstration). We predict depression with asthma, family size, minutes of sedentary behavior, and the subject's race.
```{r, message=FALSE, warning=FALSE}
library(gee)
fit_gee <- gee(dep ~ asthma + famsize + sed + race,
               data = df2,
               id = df2$sdmvstra,
               corstr = "unstructured")
summary(fit_gee)$coef

library(geepack)
fit_geeglm <- geeglm(dep ~ asthma + famsize + sed + race,
                     data = df2,
                     id = df2$sdmvstra,
                     corstr = "unstructured")
summary(fit_geeglm)
```

The `gee` package doesn't directly provide p-values but provides the z-scores, which can be used to find the p-values. The `geepack` provides the p-values in the way you'll see in the `lm()` and `glm()` functions.

These models are interpreted just as the regular GLM. It has adjusted for the correlations within the clusters and provides valid standard errors and p-values.


## Mixed Effects {-}

Mixed effects models require a bit more thinking about the effects. It is called "mixed effects" because we include both fixed and random effects into the model simultaneously. The random effects are those that we don't necessarily care about the specific values but want to control for it and/or estimate the variance. The fixed effects are those we are used to estimating in linear models and GLM's. 

These are a bit more clear with an example. We will do the same overall model as we did with the GEE but we'll use ME. To do so, we'll use the `lme4` package. In the model below, we predict depression with asthma, family size, minutes of sedentary behavior, and the subject's race. We have a random intercept (which allows the intercept to vary across clusters).

```{r, message=FALSE, warning=FALSE}
library(lme4)
fit_me <- lmer(dep ~ asthma + famsize + sed + race + (1 | cluster),
               data = df2,
               REML = FALSE)
summary(fit_me)
```

You'll see that there are no p-values provided here. This is because p-values are not well-defined in the ME framework. A good way to test it can be through the `anova()` function, comparing models. Let's compare a model with and without `asthma` to see if the model is significantly better with it in.

```{r, message=FALSE, warning=FALSE}
fit_me1 <- lmer(dep ~ famsize + sed + race + (1 | cluster),
               data = df2,
               REML = FALSE)

anova(fit_me, fit_me1)
```

This comparison strongly suggests that `asthma` is a significant predictor ($\chi^2 = 50.5$, p < .001). We can do this with both fixed and random effects, as below:

```{r, message=FALSE, warning=FALSE}
fit_me2 <- lmer(dep ~ famsize + sed + race + (1 | cluster),
               data = df2,
               REML = TRUE)
fit_me3 <- lmer(dep ~ famsize + sed + race + (1 + asthma | cluster),
               data = df2,
               REML = TRUE)
anova(fit_me2, fit_me3, refit = FALSE)
```

Here, including random slopes for asthma appears to be significant ($\chi^2 = 36.9$, p < .001).

Linear mixed effects models converge pretty well. You'll see that the conclusions and estimates are very similar to that of the GEE. For generalized versions of ME, the convergence can be harder and more picky. As we'll see below, it complains about large eigenvalues and tells us to rescale some of the variables.

```{r, message=FALSE}
library(lme4)
fit_gme <- glmer(dep2 ~ asthma + famsize + sed + race + (1 | cluster),
                 data = df2,
                 family = "binomial")
```

After a quick check, we can see that `sed` is huge compared to the other variables. If we simply rescale it, using the `I()` function within the model formula, we can rescale it by 1,000. Here, that is all it needed to converge.

```{r, message=FALSE, warning=FALSE}
library(lme4)
fit_gme <- glmer(dep2 ~ asthma + famsize + I(sed/1000) + race + (1 | cluster),
                 data = df2,
                 family = "binomial")
summary(fit_gme)
```


## Apply It {-}

[This link](http://tysonbarrett.com/DataR/Chapter4.zip) contains a folder complete with an Rstudio project file, an RMarkdown file, and a few data files. Download it and unzip it to do the following steps.


### Step 1 {-}

Open the `Chapter4.Rproj` file. This will open up RStudio for you.

### Step 2 {-}

Once RStudio has started, in the panel on the lower-right, there is a `Files` tab. Click on that to see the project folder. You should see the data files and the `Chapter4.Rmd` file. Click on the `Chapter4.Rmd` file to open it. In this file, import the data and run each type of statistical analysis presented in this chapter (there are others that are presented in Chapters 4 and 5 that you may have done already and methods from Chapter 7 that you have not done yet).

Once that code is in the file, click the `knit` button. This will create an HTML file with the code and output knitted together into one nice document. This can be read into any browser and can be used to show your work in a clean document.


## Conclusions {-}

This has been a really brief introduction into a thriving, large field of statistical analyses. These are the general methods for using `R` to analyze multilevel data. Our next chapter will discuss more modeling techniques in `R`, including mediation, mixture, and structural equation modeling.


[^longform]: We discuss what this means in much more depth and demonstrate reshaping of data in Chapter 8. It is an important tool to understand if you are working with data in various forms. Although many reshape their data by copying-and-pasting in a spreadsheet, what we present in Chapter 8 is much more efficient, cleaner, less error-prone, and replicatable. 


<!--chapter:end:06-MultilevelModels.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---



```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Run but not shown
## Getting data ready for the examples
library(foreign)
library(furniture)
library(tidyverse)
library(anteo)
load("~/Dropbox/GitHub/blog_rstats/assets/Data/NHANES_2012.rda")
```


# Chapter 7: Other Modeling Techniques {-}

> "Simplicity is the ultimate sophistication." --- Leonardo da Vinci

In this chapter we cover, however briefly, modeling techniques that are especially useful to make complex relationships easier to interpret. We will focus on mediation and moderation modeling, methods relating to structural equation modeling (SEM), and methods applicable to our field from machine learning. Although these machine learning may appear very different than mediation and SEM, they each have advantages that can help in different situations. For example, SEM is useful when we know there is a high degree of measurement error or our data has multiple indicators for each construct. On the other hand, regularized regression and random forests--two popular forms of machine learning--are great to explore patterns and relationships there are hundreds or thousands of variables that may predict an outcome. 

Mediation modeling, although often used within SEM, can help us understand pathways of effect from one variable to another. It is especially useful with moderating variables (i.e., variables that interact with another).

So we'll start with discussing mediation, then we'll move on to SEM, followed by machine learning techniques.


## Mediation Modeling {-}

Mediation modeling can be done via several packages. For now, we recommend using either `lavaan` (stands for "latent variable analysis")[^lavaa] or `MarginalMediation` (written by the author of this book). Although both are technically still "beta" versions, they both perform very well especially for more simple models. It makes mediation modeling straightforward.

Below, we model the following mediation model:
$$
depression = \beta_0 + \beta_1 asthma + \epsilon_1
$$

$$
time_{Sedentary} = \lambda_0 + \lambda_1 asthma + \lambda_2 depression + \epsilon_2
$$

In essence, we believe that asthma increases depression which in turn increases the amount of time spent being sedentary. To run this with `MarginalMediation`, we will use two distinct regression models (see Chapter 4) and combine them with `mma()`. The object `pathbc` is the model with sedentary behavior as the outcome and `patha` is the path leading to the mediator.

```{r, message=FALSE, warning=FALSE}
library(MarginalMediation)

df$sed_hr = df$sed/60  ## in hours instead of minutes

pathbc <- glm(sed_hr ~ dep + asthma, data = df)
patha  <- glm(dep ~ asthma, data = df)

mma(pathbc, patha,
    ind_effects = c("asthmaAsthma-dep"))
```

This gives us the estimates of the individual regression models and the estimates of the indirect and direct effects.

To do the exact same model with `lavaan`, we can do the following:

```{r, message=FALSE, warning=FALSE}
library(lavaan)

## Our model
model1 <- '
  dep ~ asthma
  sed_hr ~ dep + asthma
'
## sem function to run the model
fit <- sem(model1, data = df)
summary(fit)
```

From the output we see asthma does predict depression and depression does predict time being sedentary. There is also a direct effect of asthma on sedentary behavior even after controlling for depression. We can further specify the model to have it give us the indirect effect and direct effects tested.

```{r, message=FALSE, warning=FALSE}
## Our model
model2 <- '
  dep ~ a*asthma
  sed_hr ~ b*dep + c*asthma
 
  indirect := a*b
  total := c + a*b
'
## sem function to run the model
fit2 <- sem(model2, data = df)
summary(fit2)
```

We defined a few things in the model. First, we gave the coefficients labels of `a`, `b`, and `c`. Doing so allows us to define the `indirect` and `total` effects. Here we see the indirect effect, although small, is significant at $p < .001$. The total effect is larger (not surprising) and is also significant.

Also note that we can make the regression equations have other covariates as well if we needed to (i.e. control for age or gender) just as we do in regular regression. 

```{r, message=FALSE, warning=FALSE}
## Our model
model2.1 <- '
  dep ~ asthma + ridageyr
  sed_hr ~ dep + asthma + ridageyr
'
## sem function to run the model
fit2.1 <- sem(model2.1, data = df)
summary(fit2.1)
```

Although we don't show it here, we can also do moderation ("interactions") as part of the mediation model (especially using the `MarginalMediation` package).



## Structural Equation Modeling {-}

Instead of summing our depression variable, we can use SEM to run the mediation model from above but use the latent variable of depression instead.

```{r, message=FALSE, warning=FALSE}
## Our model
model3 <- '
  dep1 =~ dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090
  dep1 ~ a*asthma
  sed_hr ~ b*dep1 + c*asthma

  indirect := a*b
  total := c + a*b
'
## sem function to run the model
fit3 <- sem(model3, data = df)
summary(fit3)
```

We defined `dep1` as a latent variable using `=~`. Although the model does not fit the data well--"`P-value (Chi-square) = 0.000`"--it is informative for demonstration. We would likely need to find out how the measurement model (`dep1 =~ dpq010 + dpq020 + dpq030 +`) actually fits before throwing it into a mediation model. We can do that via:

```{r}
model4 <- '
  dep1 =~ dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090
'
fit4 <- cfa(model4, data=df)
summary(fit4)
```

As we can see, there is a lack of fit in the measurement model. It is possible that these depression questions could be measuring more than one factor. We could explore this using exploratory factor analysis. We don't demonstrate that here, but know that it is possible to do in `R` with a few other packages.


## Machine Learning Techniques {-}

We are briefly going to introduce some machine learning techniques that may be of interest to researchers. We will quickly introduce and demonstrate:

1. Ridge, Lasso and Elastic Net
2. Random Forests

In order to use these methods, we can use the fantastic `caret` package. It allows us to do nearly any type of machine learning technique. It is a type of package that takes many other packages and gives us a simple syntax across all the methods.

### Ridge, Lasso and Elastic Net {-}

Lasso and elastic net can do variable selection in addition to estimation. Ridge is great at handling correlated predictors. Each of them are better than conventional methods at prediction and each of them can handle large numbers of predictors. To learn more see "Introduction to Statistical Learning" by Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie. A free PDF is available on their website.

To use the package, it wants the data in a very specific form. First, we need to remove any missingness. We use `na.omit()` to do this. We take all the predictors (without the outcome) and put it in a data matrix object. We only include a few for the demonstration but you can include *many* predictors. We name ours `X`. `Y` is our outcome.

```{r, message=FALSE, warning=FALSE}
df2 <- df %>%
  dplyr::select(riagendr, ridageyr, ridreth3, race, famsize, dep, asthma, sed_hr) %>%
  na.omit
```

Then we use the `train()` function to fit the different models. This function, by default, uses cross-validation[^crossval], which we don't discuss here, but it an important topic to become familiar with. Below we fit a model that is either a ridge, a lasso, or an elastic net model depending on the `alpha` level. This is done using the `method = "glmnet"` argument. We specify the model by the formula `sed_hr ~ .` which means we want `sed_hr` to be the outcome and all the rest of the variables to be predictors.

```{r, message=FALSE, warning=FALSE}
library(caret)

## Use 10-fold cross validation
fitControl <- trainControl(method = "cv",
                           number = 10)

## Run the model
fit <- train(sed_hr ~ ., 
             method = "glmnet",
             data = df2,
             trControl = fitControl)
fit
```

With this model, we can assess the most important predictors of sedentary behavior. We can do that with:

```{r}
varImp(fit)
```

This shows us that, of these variables, `race` was most important, followed by `asthma`. Importantly, though, this model did not predict the outcome very well so these are likely not very important predictors overall.

### Random Forests {-}

Random forests is another machine learning method that can do fantastic prediction. It is built in a very different way than the methods we have discussed up to this point. It is not built on a linear modeling scheme; rather, it is built on classification and regression trees (CART). Again, "Introduction to Statistical Learning" is a great resource to learn more.

Conveniently, we can use the `randomForest` package. (We can also use the `caret` package here.) We specify the model by the formula `sed_hr ~ .` just like before, which means we want `sed_hr` to be the outcome and all the rest of the variables to be predictors.

```{r}
library(randomForest)

fit_rf <- randomForest(sed_hr ~ ., data = df2)
fit_rf
```

We can find out which variables were important in the model via:

```{r}
par(mfrow=c(1,1))  ## back to one plot per page
varImpPlot(fit_rf)
```

We can see that age (`ridageyr`) is the most important variable, depression (`dep`) follows, with the family size (`famsize`) the third most important in the random forests model.

## Apply It {-}

[This link](http://tysonbarrett.com/DataR/Chapter4.zip) contains a folder complete with an Rstudio project file, an RMarkdown file, and a few data files. Download it and unzip it to do the following steps.


### Step 1 {-}

Open the `Chapter4.Rproj` file. This will open up RStudio for you.

### Step 2 {-}

Once RStudio has started, in the panel on the lower-right, there is a `Files` tab. Click on that to see the project folder. You should see the data files and the `Chapter4.Rmd` file. Click on the `Chapter4.Rmd` file to open it. In this file, import the data and run each type of statistical analysis presented in this chapter (there are others that are presented in Chapters 4, 5, and 6 that you may have done already).

Once that code is in the file, click the `knit` button. This will create an HTML file with the code and output knitted together into one nice document. This can be read into any browser and can be used to show your work in a clean document.



## Conclusions {-}

Although we only discussed these methods briefly, that does not mean they are less important. On the contrary, they are essential upper level statistical methods. This brief introduction hopefully helped you know what `R` is capable of across a wide range of methods.

The next chapter begins our "advanced" topics, starting with "Advanced Data Manipulation".

[^lavaa]: The `lavaan` package has some great vignettes at [http://lavaan.ugent.be/](http://lavaan.ugent.be/) to help with the other types of models it can handle.

[^crossval]: Cross-validation is a common way to reduce over-fitting and make sure your model is generalizable. Generally, you split your data into training and testing sets. It is very common in machine learning and is beginning to be practiced in academic fields as well. We recommend using it as often as you can, especially with these methods but also to make sure your other models are accurate on new data as well.

<!--chapter:end:07-OtherModels.Rmd-->



# Chapter 8: Advanced Data Manipulation {-}

> "Every new thing creates two new questions and two new opportunities." --- Jeff Bezos

There's so much more we can do with data in `R` than what we've presented. Two main topics we need to clarify here are:

1. How do you reshape your data from wide to long form or vice versa in more complex data structures?
2. How do can we automate tasks that we need done many times?

We will introduce both ideas to you in this chapter. To discuss the first, show the use of `long()` and `wide()` from the `furniture` package. For the second, we need to talk about loops. Looping, for our purposes, refers to the ability to repeat something across many variables or data sets. There's many ways of doing this but some are better than others. For looping, we'll talk about:

1. vectorized functions,
2. `for` loops, and
3. the `apply` family of functions.


## Reshaping Your Data {-}

We introduced you to wide form and long form of your data in Chapter 2. In reality, data can take on nearly infinite forms but for most data in health, behavioral, and social science, these two forms are sufficient to know. 

In some situations, your data may have multiple variables with multiple time points (known as time-variant variables) and other variables that are not (known as time-invariant variables) as shown:

```{r, echo=FALSE}
d1 <- data.frame("ID"=c(1:10), 
                 "Var_Time1"=rnorm(10), "Var_Time2"=runif(10), 
                 "Var2_Time1"=rnorm(10), "Var2_Time2"=rnorm(10),
                 "Var3"=rnorm(10))
d1
```

Notice that this data frame is in wide format (each ID is one row and there are multiple times or measurements per person for two of the variables). To change this to wide format, we'll use `long()`. The first argument is the data.frame, followed by two variable names (names that we go into the new long form), and then the numbers of the columns that are the measures (e.g., `Var_Time1` and `Var_Time2`).
```{r}
long_form <- furniture::long(d1, 
                             c("Var_Time1", "Var_Time2"), 
                             c("Var2_Time1", "Var2_Time2"),
                             v.names = c("Var", "Var2"))
long_form
```

As you can see, it took the variable names and put that in our first variable that we called "measures". The actual values of the variables are now in the variable we called "values". Finally, notice that each ID now has two rows (one for each measure).

To go in the opposite direction (long to wide) we can use the `wide()` function. All we do is provide the long formed data frame, variables that are time-varying (`Var1` and `Var2`) and the variable showing the time points (`time`).
```{r}
wide_form <- furniture::wide(long_form, 
                             v.names = c("Var", "Var2"),
                             timevar = "time")
wide_form
```
And we are back to the wide form.

These steps can be followed for situations where there are many measures per person, many people per cluster, etc. In most cases, this is the way multilevel data analysis occurs (as we discussed in Chapter 6) and is a nice way to get our data ready for plotting.

The following figure shows the features of both `long()` and `wide()`.

![](DataCleaning_Handout.jpg)




## Repeating Actions (Looping) {-}

To fully go into looping, understanding how to write your own functions is needed.

### Your Own Functions {-}

Let's create a function that estimates the mean (although it is completely unnecessary since there is already a perfectly good `mean()` function). 
```{r}
mean2 <- function(x){
  n <- length(x)
  m <- (1/n) * sum(x)
  return(m)
}
```

We create a function using the `function()` function.[^functions] Within the `function()` we put an `x`. This is the argument that the function will ask for. Here, it is a numeric vector that we want to take the mean of. We then provide the meat of the function between the `{}`. Here, we did a simple mean calculation using the `length(x)` which gives us the number of observations, and `sum()` which sums the numbers in `x`.

Let's give it a try:
```{r}
v1 <- c(1,3,2,4,2,1,2,1,1,1)   ## vector to try
mean2(v1)                      ## our function
mean(v1)                       ## the base R function
```

Looks good! These functions that you create can do whatever you need them to (within the bounds that `R` can do). I recommend by starting outside of a function that then put it into a function. For example, we would start with:
```{r}
n <- length(v1)
m <- (1/n) * sum(v1)
m
```
and once things look good, we would put it into a function like we had before with `mean2`. It is an easy way to develop a good function and test it while developing it.

By creating your own function, you can simplify your workflow and can use them in loops, the `apply` functions and the `purrr` package.

For practice, we will write one more function. Let's make a function that takes a vector and gives us the N, the mean, and the standard deviation.
```{r}
important_statistics <- function(x, na.rm=FALSE){
  N  <- length(x)
  M  <- mean(x, na.rm=na.rm)
  SD <- sd(x, na.rm=na.rm)
  
  final <- c(N, M, SD)
  return(final)
}
```
One of the first things you should note is that we included a second argument in the function seen as `na.rm=FALSE` (you can have as many arguments as you want within reason). This argument has a default that we provide as `FALSE` as it is in most functions that use the `na.rm` argument. We take what is provided in the `na.rm` and give that to both the `mean()` and `sd()` functions. Finally, you should notice that we took several pieces of information and combined them into the `final` object and returned that.

Let's try it out with the vector we created earlier.
```{r}
important_statistics(v1)
```
Looks good but we may want to change a few aesthetics. In the following code, we adjust it so we have each one labeled.
```{r}
important_statistics2 <- function(x, na.rm=FALSE){
  N  <- length(x)
  M  <- mean(x, na.rm=na.rm)
  SD <- sd(x, na.rm=na.rm)
  
  final <- data.frame(N, "Mean"=M, "SD"=SD)
  return(final)
}
important_statistics2(v1)
```
We will come back to this function and use it in some loops and see what else we can do with it.


### Vectorized {-}

By construction, `R` is the fastest when we use the vectorized form of doing things. For example, when we want to add two variables together, we can use the `+` operator. Like most functions in `R`, it is vectorized and so it is fast. Below we create a new vector using the `rnorm()` function that produces normally distributed random variables. First argument in the function is the length of the vector, followed by the mean and SD.
```{r}
v2 <- rnorm(10, mean=5, sd=2)
add1 <- v1 + v2
round(add1, 3)
```
We will compare the speed of this to other ways of adding two variables together and see it is the simplest and quickest.

### For Loops {-}

For loops have a bad reputation in the `R` world. This is because, in general, they are slow. It is among the slowest of ways to iterate (i.e., repeat) functions. We start here to show you, in essence, what the `apply` family of functions are doing, often, in a faster way.

At times, it is easiest to develop a for loop and then take it and use it within the `apply` or `purrr` functions. It can help you think through the pieces that need to be done in order to get your desired result.

For demonstration, we are using the `for` loop to add two variables together. The code between the `()`'s tells `R` information about how many loops it should do. Here, we are looping through `1:10` since there are ten observations in each vector. We could also specify this as `1:length(v1)`. When using `for` loops, we need to keep in mind that we need to initialize a variable in order to use it within the loop. That's precisely what we do with the `add2`, making it a numberic vector with 10 observations.
```{r}
add2 <- vector("numeric", 10)   ## Initialize
for (i in 1:10){
  add2[i] <- v1[i] + v2[i]
}
round(add2, 3)
```
Same results! But, we'll see later that the speed is much than the vectorized function.


### The `apply` family {-}

The `apply` family of functions that we'll introduce are:

1. `apply()`
2. `lapply()`
3. `sapply()`
4. `tapply()`

Each essentially do a loop over the data you provide using a function (either one you created or another). The different versions are extremely similar with some minor differences. For `apply()` you tell it if you want to iterative over the columns or rows; `lapply()` assumes you want to iterate over the columns and outputs a list (hence the `l`); `sapply()` is similar to `lapply()` but outputs vectors and data frames. `tapply()` has the most differences because it can iterative over columns by a grouping variable. We'll show `apply()`, `lapply()` and `tapply()` below.

For example, we can add two variables together here. We provide it the `data.frame` that has the variables we want to add together.
```{r}
df <- data.frame(v1, v2)
add3 <- apply(df, 1, sum)
round(add3, 3)
```
The function `apply()` has three main arguments: a) the `data.frame` or list of data, b) 1 meaning to apply the function for each row or 2 to the columns, and c) the function to use.

We can also use one of our own functions such as `important_statistics2()` within the `apply` family.
```{r}
lapply(df, important_statistics2)
```
This gives us a list of two elements, one for each variable, with the statistics that our function provides. With a little adjustment, we can make this into a `data.frame` using the `do.call()` function with `"rbind"`.
```{r}
do.call("rbind", lapply(df, important_statistics2))
```

`tapply()` allows us to get information by a grouping factor. We are going to add a factor variable to the data frame we are using `df` and then get the mean of the variables by group.
```{r}
group1 <- factor(sample(c(0,1), 10, replace=TRUE))
tapply(df$v1, group1, mean)
```
We now have the means by each group. This, however, is probably replaced by the 3 step summary that we learned earlier in `dplyr` using `group_by()` and `summarize()`.

These functions are useful in many situations, especially where there are no vectorized functions. You can always get an idea of whether to use a `for` loop or an `apply` function by giving it a try on a small subset of data to see if one is better and/or faster.

#### Speed Comparison {-}

We can test to see how fast functions are with the `microbenchmark` package. Since it wants functions, we will create a function that uses the `for` looop.
```{r}
forloop <- function(var1, var2){
  add2 <- vector("numeric", length(var1))
  for (i in 1:10){
    add2[i] <- var1[i] + var2[i]
  }
  return(add2)
}
```

Below, we can see that the vectorized version is nearly 50 times faster than the `for` loop and 300 times faster than the `apply`. Although the `for` loop was faster here, sometimes it can be slower than the `apply` functions--it just depends on the situation. But, the vectorized functions will almost always be *much* faster than anything else. It's important to note that the `+` is also a function that can be used as we do below, highlighting the fact that anything that does something to an object in `R` is a function.
```{r}
library(microbenchmark)
microbenchmark(forloop(v1, v2),
               apply(df, 1, sum),
               `+`(v1, v2))
```
Of course, as it says the units are in nanoseconds. Whether a function takes 200 or 200,000 nanoseconds probably won't change your life. However, if the function is being used repeatedly or on on large data sets, this can make a difference.

### Using "Anonymous Functions" in Apply {-}

Last thing to know here is that you don't need to create a named function everytime you want to use `apply`. We can use what is called "Anonymous" functions. Below, we use one to get at the N and mean of the data.
```{r}
lapply(df, function(x) rbind(length(x), mean(x, na.rm=TRUE)))
```
So we don't name the function but we design it like we would a named function, just minus the `return()`. We take `x` (which is a column of `df`) and do `length()` and `mean()` and bind them by rows. The first argument in the anonymous function will be the column or variable of the data you provide.

Here's another example:
```{r}
lapply(df, function(y) y * 2 / sd(y))
```
We take `y` (again, the column of `df`), times it by two and divide by the standard deviation of `y`. Note that this is gibberish and is not some special formula, but again, we can see how flexible it is.

The last two examples also show something important regarding the output:

1. The output will be at the level of the anonymous function. The first had two numbers per variable because the function produced two summary statistics for each variable. The second we multiplied `y` by 2 (so it is still at the individual observation level) and then divide by the SD. This keeps it at the observation level so we get ten values for every variable.
2. We can name the argument anything we want (as long as it is one word). We used `x` in the first and `y` in the second but as long as it is the same within the function, it doesn't matter what you use.

Finally, we may not want our variables to be in the list format. We may want to control more tightly what is outputted from the looping. For that, we can thank the `purrr` package (part of the `tidyverse`; note the three r's in purrr). This package provides many valuable functions that you can explore. Of particular mention here, though, are some of the `map*()` functions that work just like `lapply()`.

1. `map()` -- outputs a list
2. `map_df()` -- outputs a data frame
3. `map_if()` -- outputs a list but only makes any changes to the variables that meet a condition (e.g., `is.numeric()`).

```{r}
purrr::map(df, function(y) y * 2 / sd(y))
```

```{r}
purrr::map_df(df, function(y) y * 2 / sd(y))
```

```{r}
purrr::map_if(df, is.numeric, function(y) y * 2 / sd(y))
```


## Apply It {-}

[This link](http://tysonbarrett.com/DataR/Chapter8.zip) contains a folder complete with an Rstudio project file, an RMarkdown file, and a few data files. Download it and unzip it to do the following steps.


### Step 1 {-}

Open the `Chapter8.Rproj` file. This will open up RStudio for you.

### Step 2 {-}

Once RStudio has started, in the panel on the lower-right, there is a `Files` tab. Click on that to see the project folder. You should see the data files and the `Chapter8.Rmd` file. Click on the `Chapter8.Rmd` file to open it. In this file, import the data, reshape it to long form, create your own function to do something for you, and apply the function in a loop over some of the variables of the data set.

Once that code is in the file, click the `knit` button. This will create an HTML file with the code and output knitted together into one nice document. This can be read into any browser and can be used to show your work in a clean document.



## Conclusions {-}

These are useful tools to use in your own data manipulation beyond that what we discussed with `dplyr`. It takes time to get used to making your own functions so be patient with yourself as you learn how to get `R` to do exactly what you want in a condensed, replicable format. 

With these new tricks up your sleeve, we can move on to more advanced plotting using `ggplot2`.


[^functions]: That seemed like excessive use of the word function... It is important though. So, get used to it!


<!--chapter:end:08-AdvancedDataManipulation.Rmd-->



```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Run but not shown
## Getting data ready for the examples
library(foreign)
library(furniture)
library(tidyverse)
library(anteo)
load("~/Dropbox/GitHub/blog_rstats/assets/Data/NHANES_2012.rda")
```


# Chapter 9: Advanced Plotting {-}

> "The commonality between science and art is in trying to see profoundly - to develop strategies of seeing and showing." --- Edward Tufte


Once again, we will turn to our friend `ggplot2` to plot; but now, we are going to take it to another level. We will use many of the options that this powerful package provides and discuss briefly some important aspects of a good plot.

We will go through several aspects of the code that makes plotting in `R` flexible and beautiful.

1. Types of plots
2. Color schemes
3. Themes
4. Labels and titles
5. Facetting

To highlight these features we'll be using our NHANES data again; specifically, sedentary behavior, depression, asthma, family size, and race. As this is only an introduction, refer to [http://docs.ggplot2.org/current/](http://docs.ggplot2.org/current/) for more information on `ggplot2`.

To begin, it needs to be understood that the first line where we actually use the `ggplot` function, will then apply to all subsequent laters (e.g., `geom_point()`). For example,
```{r, eval=FALSE}
ggplot(df, aes(x = dep, y = sed, group = asthma))
```
means for the rest of the layers, unless we over-ride it, each will use `df` with `dep` as the x variable, `sed` as the y, and a grouping on `asthma`. So when many layers are going to use the same command put it in this so you don't have to write the same argument many times. A common one here could be:
```{r, eval=FALSE}
ggplot(df, aes(x = dep, y = sed, group = asthma, color = asthma))
```
since we often want to color by our grouping variable.

Before going forward, a nice feature of `ggplot2` allows us to use an "incomplete" plot to add on to. For example, if we have a good idea of the main structure of the plot but want to explore some changes, we can do the following:
```{r}
p1 <- ggplot(df, aes(x = dep, y = sed, group = asthma)) +
  geom_point()
p1
```
So now `p1` has the information for this basic, and honestly fairly uninformative, plot. We'll use this feature to build on plots that we like. 

Some of our figures will also need summary data so we'll start that here as well:
```{r}
summed_data <- df %>%
  group_by(asthma, dep2) %>%
  summarize(s_se = sd(sed, na.rm=TRUE)/sqrt(n()),
            sed  = mean(sed, na.rm=TRUE),
            N    = n())
```

As you hopefully recognize a bit, we are summarizing the time spent being sedentary by both asthma and the dichotomous depression variables. If it doesn't make sense at first, read it line by line to see what I did. This will be useful for several types of plots.


## Types of Plots {-}
### Scatterplots {-}
We'll start with a scatterplot--one of the most simple yet informative plots.
```{r, message=FALSE, warning=FALSE}
ggplot(df, aes(x = dep, y = sed, group = asthma)) +
  geom_point(aes(color = asthma))
```

It's not amazing. There looks to be a lot of overlap of the points. Also, it would be nice to know general trend lines for each group. Below, `alpha` refers to how transparent the points are, `method = "lm"` refers to how the line should be fit, and `se=FALSE` tells it not to include error ribbons.
```{r, message=FALSE, warning=FALSE}
ggplot(df, aes(x = dep, y = sed, group = asthma)) +
  geom_jitter(aes(color = asthma), alpha = .5) +
  geom_smooth(aes(color = asthma), method = "lm", se=FALSE)
```

It's getting better but we could still use some more features. We'll come back to this in the next sections.

### Boxplots {-}
Box plots are great ways to assess the variability in your data. Below, we create a boxplot but change `p1`'s x variable so that it is the factor version of depression.
```{r, message=FALSE, warning=FALSE}
ggplot(df, aes(x = factor(dep2), y = sed)) +
  geom_boxplot()
```

This plot is, at best, mediocre. But there's more we can do.
```{r, message=FALSE, warning=FALSE}
ggplot(df, aes(x = factor(dep2), y = jitter(sed, 100))) +
  geom_jitter(alpha = .1, color = "chartreuse4") +
  geom_boxplot(alpha = .75, color = "dodgerblue4") 
```

This now provides the (jittered) raw data points as well to hightlight the noise and the number of observations in each group.


### Bar Plots {-}
Bar plots are great ways to look at means and standard deviations for groups.
```{r, message=FALSE, warning=FALSE}
ggplot(summed_data, aes(x = dep2, y = sed, group = asthma)) +
  geom_bar(aes(fill = asthma), stat = "identity", position = "dodge")
```

We used `stat = "identity"` to make it based on the mean (default is `count`), and `position = "dodge"` makes it so the bars are next to each other as opposed to stacked. Let's also add error bars.

```{r, message=FALSE, warning=FALSE}
p = position_dodge(width = .9)
ggplot(summed_data, aes(x = dep2, y = sed, group = asthma)) +
  geom_bar(aes(fill = asthma), 
           stat = "identity", 
           position = p,
           alpha = .8) +
  geom_errorbar(aes(ymin = sed - s_se, ymax = sed + s_se,
                    color = asthma), 
                position = p,
                width = .3)
```

There's a lot in there but much of it is what you've seen before. For example, we use `alpha` in the `geom_bar()` to tell it to be slightly transparent so we can see the error bars better. We used the `position_dodge()` function to specify exactly how much dodge we wanted. In this way, we are able to line up the error bars and the bars. If we just use `position = "dodge"` we have less flexibility and control. 

Much more can be done to clean this up, which we'll show in later sections.

### Line Plots {-}
Line plots are particularly good at showing trends and relationships. Below we we use it to highlight the relationship between depression, sedentary behavior, and asthma.
```{r, message=FALSE, warning=FALSE}
ggplot(summed_data, aes(x = dep2, y = sed, group = asthma)) +
  geom_line(aes(color = asthma))
```

Good start, but let's add some features.
```{r, message=FALSE, warning=FALSE}
pos = position_dodge(width = .1)
ggplot(summed_data, aes(x = dep2, y = sed, group = asthma, color = asthma)) +
  geom_line(position = pos) +
  geom_point(position = pos) +
  geom_errorbar(aes(ymin = sed - s_se, ymax = sed + s_se), 
                width = .1, 
                position = pos)
```

That looks a bit better. From here, let's go on to color schemes to make the plots a bit better.

## Color Schemes {-}

We'll start by using the scatterplot we made above but we will change the colors a bit using `scale_color_manual()`.
```{r, message=FALSE, warning=FALSE}
ggplot(df, aes(x = dep, y = sed, group = asthma)) +
  geom_jitter(aes(color = asthma), alpha = .5) +
  geom_smooth(aes(color = asthma), method = "lm", se=FALSE) +
  scale_color_manual(values = c("dodgerblue4", "chartreuse4"))
```
Depending on your personal taste, you can adjust it with any color. On my blog, I've [posted](https:\\tysonstanley.github.io) the colors available in R (there are many). 

> Advice: Don't get too lost in selecting colors but it can add a nice touch to any plot. The nuances of plot design can be invigorating but also time consuming to be smart about how long you spend using it.

Next, let's adjust the bar plot. We will also add some colors here, but we will differentiate between "color" and "fill". 

1. Fill fills in the object with color. This is useful for things that are more than simply a line or a dot.
2. Color colors the object. This outlines those items that can also be filled and colors lines and dots.
```{r, message=FALSE, warning=FALSE}
p = position_dodge(width = .9)
ggplot(summed_data, aes(x = dep2, y = sed, group = asthma)) +
  geom_bar(aes(fill = asthma, color = asthma), 
           stat = "identity", 
           position = p,
           alpha = .8) +
  geom_errorbar(aes(ymin = sed - s_se, ymax = sed + s_se,
                    color = asthma), 
                position = p,
                width = .3) +
  scale_color_manual(values = c("dodgerblue4", "chartreuse4")) +   ## controls the color of the error bars
  scale_fill_manual(values = c("aliceblue", "beige"))
```

Just so you are aware:

- aliceblue is a lightblue
- beige is a light green
- dodgerblue4 is a dark blue
- chartreuse4 is a dark green

So the `fill` colors are light and the `color` colors are dark in this example. You, of course, can do whatever you want color-wise. I'm a fan of this style though so we will keep it for now.

These same functions can be used on the other plots as well. Feel free to give them a try. As for the book, we'll move on to the next section: Themes.

## Themes {-}

Using the plot we just made--the bar plot--we will show how theme options work. There are several built in themes that change many aspects of the plot (e.g., `theme_bw()`, `theme_classic()`, `theme_minimal()`). There are many more if you download the `ggthemes` package. Fairly simply you can create plots similar to those in newspapers and magazines.

First, we are going to save the plot to simply show the different theming options.

```{r, message=FALSE, warning=FALSE}
p = position_dodge(width = .9)
p1 = ggplot(summed_data, aes(x = dep2, y = sed, group = asthma)) +
  geom_bar(aes(fill = asthma, color = asthma), 
           stat = "identity", 
           position = p,
           alpha = .8) +
  geom_errorbar(aes(ymin = sed - s_se, ymax = sed + s_se,
                    color = asthma), 
                position = p,
                width = .3) +
  scale_color_manual(values = c("dodgerblue4", "chartreuse4")) +   ## controls the color of the error bars
  scale_fill_manual(values = c("aliceblue", "beige"))
```

#### Theme Black and White {-}

```{r, message=FALSE, warning=FALSE}
p1 + 
  theme_bw()
```

#### Theme Classic {-}

```{r, message=FALSE, warning=FALSE}
p1 + 
  theme_classic()
```

#### Theme Minimal {-}

```{r, message=FALSE, warning=FALSE}
p1 + 
  theme_minimal()
```


#### Theme Economist (from `ggthemes`) {-}

```{r, message=FALSE, warning=FALSE}
library(ggthemes)
p1 + 
  theme_economist()
```

#### Theme FiveThirtyEight (from `ggthemes`) {-}

```{r, message=FALSE, warning=FALSE}
p1 + 
  theme_fivethirtyeight()
```

#### Theme Tufte (from `ggthemes`) {-}

```{r, message=FALSE, warning=FALSE}
p1 + 
  theme_tufte()
```

#### Theme Stata (from `ggthemes`) {-}

```{r, message=FALSE, warning=FALSE}
p1 + 
  theme_stata()
```

#### Your Own Theme {-}

There are many more but you get the idea. In addition to the built in themes, you can use the `theme()` function and make your own adjustments. There are *many* options so we will just introduce the idea.

```{r, message=FALSE, warning=FALSE}
p1 + 
  theme(legend.position = "bottom",  ## puts legend at the bottom of figure
        legend.background = element_rect(color = "lightgrey"),  ## outlines legend
        panel.background = element_rect(fill = "grey99",   ## fills the plot with a very light grey
                                        color = "grey70"),  ## light border around plot
        text = element_text(family = "Times"))     ## all text in plot is now Times
```

There are many more options but essentially if there is something you want to change, you probably can.

## Labels and Titles {-}

Using our last plot, we will also want to add good labels and/or titles.

```{r, message=FALSE, warning=FALSE}
p1 + 
  theme(legend.position = "bottom",  
        legend.background = element_rect(color = "lightgrey"),
        panel.background = element_rect(fill = "grey99",
                                        color = "grey70"),
        text = element_text(family = "Times")) +
  labs(y = "Sedentary Behavior (Minutes)",
       x = "Depression (1 = Depressed)",
       title = "Comparison of Sedentary Behavior",
       subtitle = "across Depression and Asthma")
```

## Facetting {-}

Facetting is very useful when trying to compare more than three variables at a time or you cannot use color or shading. It is often useful and beautiful. Facetting splits the data based on some grouping variable (e.g., asthma) to highlight differences in the relationship.
```{r, message=FALSE, warning=FALSE}
p1 + 
  theme(legend.position = "bottom",  
        legend.background = element_rect(color = "lightgrey"),
        panel.background = element_rect(fill = "grey99",
                                        color = "grey70"),
        text = element_text(family = "Times")) +
  labs(y = "Sedentary Behavior (Minutes)",
       x = "Depression (1 = Depressed)",
       title = "Comparison of Sedentary Behavior",
       subtitle = "across Depression and Asthma") +
  facet_grid(~asthma)
```

You can facet by more than one variable and it will create separate panels for each combination of the facetting variables.


## Apply It {-}

[This link](http://tysonbarrett.com/DataR/Chapter9.zip) contains a folder complete with an Rstudio project file, an RMarkdown file, and a few data files. Download it and unzip it to do the following steps.


### Step 1 {-}

Open the `Chapter9.Rproj` file. This will open up RStudio for you.

### Step 2 {-}

Once RStudio has started, in the panel on the lower-right, there is a `Files` tab. Click on that to see the project folder. You should see the data files and the `Chapter9.Rmd` file. Click on the `Chapter9.Rmd` file to open it. In this file, import the data and run each type of statistical analysis presented in this chapter.

Once that code is in the file, click the `knit` button. This will create an HTML file with the code and output knitted together into one nice document. This can be read into any browser and can be used to show your work in a clean document.



## Conclusions {-}

This was a quick demonstration of plotting with `ggplot2`. There is so much more you can do. However, in the end, exploring and communicating the data through plots is simply something you need to practice. With time, you can *a priori* picture the types of plots that will highlight things in your data, the ways you can adjust it, and how you need to manipulate your data to make it plot ready. Be patient and have fun trying things. In my experience, almost anytime I think, "Can R do this?", it can, so try to do cool stuff and you'll probably find that you can.

<!--chapter:end:09-AdvancedPlotting.Rmd-->



# Chapter 10: Where to Go from Here and Common Pitfalls {-}

> "The journey of a thousand miles begins with one step." --- Lao Tzu

There are many resources that can aid in developing your `R` skills from here. We have introduced the basics of `R`, helping you take a few steps on your journey of understanding `R`. We have focused on the ones that are most important for researchers in the health, behavioral, and social sciences. 

Since this has been a primer, we hope that you will continue your learning of `R` via the various sources available at little to no cost. Just like this book, many `R` books are available online as well as in print. This allows you to explore and learn online at your own pace without having to buy a bunch of books or other resources.

Below, we list a few `R` books that we have found to be useful. Most are available free in some form.

1. [R for Data Science by Hadley Wickham and Garrett Grolemund](http://r4ds.had.co.nz/)
2. [Efficient R Programming by Colin gillespie and Robin Lovelace](https://csgillespie.github.io/efficientR/)
3. [The R Cookbook](http://www.cookbook-r.com/)
4. [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)

There are *many, many* books that talk about `R` in various forms so by no means is this a complete list.

## Common Pitfalls {-}

To end, we wanted to highlight some pitfalls that can plague any beginner to `R`. We list a few that we've encountered, although others surely exist.

1. Document your work.
2. Avoid overriding objects unless it is on purpose. Changing objects can be hard to keep track of in bigger projects.
3. Ask questions. `R` is very flexible; this can make it overwhelming to learn since there are many ways to perform the same task. However, there are people who have figured out easy ways to do complex stuff and most are willing to answer an email.
4. Plan out the steps of your data manipulation and analyses. A few minutes of planning can help you not get lost in the technology and lose sight of the goal.
5. Understand the statistics before throwing data in a model. This can lead to major problems in science. At the very least, understand the assumptions of the modeling type and when it can and should be used.
6. Do exploratory data analysis (EDA) to understand your data. `R` is made for this--so use it. Otherwise, your model may be completely wrong and have many violated assumptions.
7. Be transparent in your writing. If you use the `R` scripts correctly, you can provide your code as part of any publication. This will greatly increase replicability of our important research findings.


## Quiz {-}

As a final note, we thought we would give you a quiz to test your memory of the topics we've covered. Don't worry; no pressure to get them all. We've included some tougher ones. Regardless of how well you do, we hope you'll continue improving in your `R` programming skills.

#### Question 1 {-}
What kind of vector is this?
```{r, eval=FALSE}
x <- c(10.1, 2.1, 4.6, 2.3, 8.9)
```

#### Question 2 {-}
What does this line of code do?
```{r, eval=FALSE}
df[c(1,5), c("B", "C")]
```

#### Question 3 {-}
In the `tidyverse` there are four join functions. What are they?

#### Question 4 {-}
What functions are used in the "three step summary" as described in Chapter 2?

#### Question 5 {-}
What does the following code do?
```{r, eval=FALSE}
ggplot(df, aes(x=C, y=D)) +
  geom_boxplot(aes(color = C)) +
  theme_bw() +
  scale_color_manual(values = c("dodgerblue4", "coral2"))
```

#### Question 6 {-}
Name three functions you can use to summarize your data in an informative way.

#### Question 7 {-}
What type of model does `aov()` perform?

#### Question 8 {-}
What are the differences between `aov()` and `lm()`?

#### Question 9 {-}
What assumptions of normality and heteroskedasticity fail, what function can be used to fit logistic and poisson regressions?

#### Question 10 {-}
If you were trying to perform logistic regression, what arguments are necessary?

#### Question 11 {-}
In multilevel modeling, which functions can be used to fit a Generalized Estimating Equations model?

#### Question 12 {-}
When comparing mixed effects models, what does `anova()` do?

#### Question 13 {-}
Can `R` do structural equation modeling? If so, what package(s) are useful?

#### Question 14 {-}
What types of models can `glmnet()` perform? How can you do a cross-validated "glmnet" model?

#### Question 15 {-}
Is the following data in wide or long form? How do you know? 
```{r, echo=FALSE}
data.frame("ID"=c(1:10), "Var_Time1"=rnorm(10), "Var_Time2"=runif(10)) %>%
  gather("measures", "values", 2:3)
```
To make your data long form but it is currently in wide form, what function(s) can you use?

#### Question 16 {-}
What form of looping is the fastest? What does `apply()` do? Can you do for loops in `R`?

#### Question 17 {-}
What is the following code doing?
```{r, eval=FALSE}
sandwhich <- function(pb, jam){
  s <- pb + jam
  return(s)
}
```

#### Question 18 {-}
What is wrong with this chunk of code?
```{r, eval=FALSE}
df <- df +
  mutate(newvar = ifelse(oldvar == 1, 1, 0))
```

#### Question 19 {-}
What is your favorite built in theme or how would you make your favorite custom theme?

#### Question 20 {-}
What kind of plot does the following make?
```{r, message=FALSE, warning=FALSE, eval=FALSE}
pos = position_dodge(width = .1)
ggplot(summed_data, aes(x = dep2, y = sed, group = asthma, color = asthma)) +
  geom_line(position = pos) +
  geom_errorbar(aes(ymin = sed - s_se, ymax = sed + s_se), 
                width = .1, 
                position = pos)
```


## Goodbye and Good Luck {-}

I hope this has been a useful primer to get you into `R`. If you still feel rusty, feel free to go through the book again or look at other online resources. `R` is very flexible and can ease the data and analysis burden of research. Implement good practices and your work will become easier to track, easier to document, and easier to communicate. Good luck on your journey using `R` in your research!

```{r, eval=FALSE}
Step1 <- of_a_journey(you) %>%
  has(begun)
You <- now_have_seen(aspects, of, R) %>%
  that_can(increase) %>%
  productivity(your)
GoodLuck <- journey(on, your)
```


<!--chapter:end:10-LearnMore.Rmd-->

---
title: "The Prospectus"
author: "R for Researchers: An Introduction"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!--
RESEARCH METHODS, STATISTICS & EVALUATION

Helen Salmon
helen.salmon@sagepub.com
(805) 410-7342
Twitter@HelenSSalmon

Helen is senior acquisitions editor for research methods, statistics, and evaluation and works across all areas of the list. Helen has more than 20 years of publishing experience, and has worked in both the U.K. and the U.S.  She previously held senior positions in marketing at SAGE. Helen brings her international experience and marketing know-how to help authors shape their books to best meet the needs of the market.

Leah Fargotstein
Leah.fargotstein@sagepub.com
(805) 410-7316

Leah is acquisitions editor for books in research methods and statistics, working with Helen Salmon across all areas of the list. She joined SAGE in 2007 as a member of the editorial acquisitions team in journals, lately responsible for managing and acquiring journals in research methods, political science, urban studies, and social work. In 2015, she joined the College Editorial team as acquisitions editor. Leah is eager to continue SAGE's strong tradition of high-quality, innovative research methods and statistics publishing. Contact Leah to talk with her about ideas that may be a good fit for an interdisciplinary project in research methods or statistics.
-->


### Project Description
<!--
Project Description: Describe the text and the topics you will cover. Do you have a special point of view? What approach will the book have, by way of a theme, analytic framework, or theoretical grounding? How will the content be organized? How will your text differ from its major competitors? How will it be similar? Please discuss both the print text and any digital assets—including instructor or student resources—that you feel should accompany the text.
-->

Given the rise of the R programming language---including its many benefits to reproducible research---many researchers can benefit from adopting its use in their analysis workflow. In this book, R will be introduced to individuals not assumed to have any coding experience, approaching R as a valuable tool that anyone can learn. It will focus on the approaches that provide the most intuitive inputs and outputs, relevant to researchers in Health, Behavioral, Educational, and Psychological Sciences. Although there are many R books, I have seen most of my students are intimidated by three aspects that nearly all R books have:

1. Most R books are lengthy, providing in-depth discussion about the topics ranging a broad range of approaches. If students are trying to understand R while also learning statistics, a 500 page book is overwhelming.
2. Very few R books are oriented towards the Health, Behavioral, Educational, and Psychological Sciences. This generally means that these others books discuss approaches not relevant to the student and lack discussion on appropriate methods that are relevent to their research.
3. The most intuitive approaches are of fairly recent development. Thus, many R books are outdated and do not provide this intuition that can reduce the cognitive burden of learning the language. 

This book will address each aspect. First, this book will *not* be a lengthy user's manual. Instead, it will introduce and demonstrate the important concepts in R necessary for the student to start using it right away in the course. Second, this book specifically addresses the use of R in Health, Behavioral, Educational, and Psychological Sciences. Although different in substantive research, the data situations encountered in these fields are very similar.\footnote{I've seen this firsthand as a data science and statistics consultant at Utah State University. Overall, the data collected is very similar across the fields.} Third, this book uses the intuitive grammar of the `tidyverse` group of packages.\footnote{More information at tidyverse.org.} This makes the code more readable, as each line of code has a subject, a verb, and possibly adjectives that make it more like reading regular English.

Each chapter will take on a general topic, demonstrate the use of the approaches relevant to the topic, and will provide a zip file that can be downloaded from an online source that provides practice data and code. 


### Unique Value Proposition
<!--
Unique Value Proposition: What is your rationale for writing this text? What need or problem does your audience have AND how does your proposed book meet that need? How will your book improve student outcomes in this course? How will this book improve instructors’ teaching in the classroom?
-->

The R programming environment has become one of the most widely used statistical tools, in both academia and industry. Because it is a programming language, many capable researchers in the Health, Behavioral, Educational, and Psychological Sciences avoid it altogether. However, recent developments in R have made the tool far more intuitive. But this information is not clear to researchers because many of the available resources to learn R come from a computer science, data science, or biological science background. Further, many of these resources are quite broad with several hundreds of pages. Many of the items discussed are unnecessary, and are potentially overwhelming. This book, instead, addresses the up-to-date uses to get started enough to become literate in the R language without discussing all possible uses of R across all quantitative fields. 

This style of book is important as many programs do not specifically teach R as part of their graduate studies. Instead, many classes introduce it as part of other statistical courses. As such, the book will be a great tool to use in conjunction with other statistical texts. It will not be overwhelming but, rather, will provide the background in R necessary to work with their own data and do the analyses needed.


### Length
<!--
Length: What is the projected length of the book (in word count, as well as manuscript or book pages)?
-->

The projected length is roughly 45,000 words and 120 book pages; long enough for the information to be taught, but not too much to overwhelm the student.

### Pedagogy
<!--
Pedagogy: What illustrations (photos, tables, figures, charts, and maps) will the book employ and for what purpose? How many and to what effect? What plans do you have for special features (boxed inserts, glossaries, appendices, questions, key terms, etc.)? What student or instructor goals will these features address? What
competitive advantages will they offer?
-->

Two major approaches to pedagogy will be emphasized: 1) students need to see lots of code examples to become literate in R, and 2) visuals help make the abstract data work more concrete. 

First, this book contains many code input and output examples. It walks through the pieces of the input that lead to the output using real-life examples from one of the NHANES (National Health and Nutrition Examination Survey) data sets.

Second, many visuals are used to demonstrate what various approaches are actually doing to the data. This is essential because many data transformations and manipulations are quite abstract and can otherwise become confusing. I intend on making several more for the published version of the book than is currently in the draft version.

### Market
<!--
Market: In which course(s) will your work be adopted? What kind of schools offers this course? What prerequisites, if any, are there? What sort of student takes this course? Are you seeing any meaningful changes in the way this course is offered or taught? Are there any trends in this course in terms of content or approach?
-->

This book is useful in statistics courses for researchers in the Health, Behavioral, Educational, and Psychological Sciences. The draft version of the book has been used to teach undergraduate, graduate, post-doctoral, and senior researchers. A general introduction to quantitative data (in spreadsheets or other forms) is helpful before beginning the book but is not absolutely essential. Also, a general introduction to statistics (t-tests, ANOVA, regression, etc.) can help make some of the middle chapters more meaningful. 

I've found that the students most commonly in these courses are those that have little or no experience coding, know some statistical approaches at an applied level, have little interest in general data science, and usually want to be more independent and reliable in their quantitative research. These span undergraduates, graduates, post-doctoral fellows, and faculty members.


### Competition
<!--
Competition: What books are professors who are likely to adopt your book currently assigning? What are the strengths and weaknesses of the books against which yours will compete? Please be specific in your response, and include an assessment of at least 2-3 competing textbooks or other works.
-->

Several R books exist:

- *Discovering Statistics with R*. This is a full text introduction to statistics while showing R examples. It is a great resource but is not directly tied to teaching how to do data work with R. Rather, its focus is on the statistics. This can be useful if an instructor wants to teach from this specific book but can be problematic otherwise.
- *R for Data Science*. This book addresses the use of R in data science. It is another great resource but its audience is too broad for it to discuss examples and approaches specific to public health, behavioral and psychological sciences, or education. 
- *Modern Dive*. Much like *R for Data Science*, this book provides an introduction to working with data in R. It, however, instructs on approaches less commonly used in the public health, behavioral and psychological sciences, and education. Although great approaches, this can be distracting to students. In my experience, very few courses teach using the approaches taught in *Modern Dive*.
- *R for SPSS and SAS Users*. This manual thoroughly explains R code in reference to SPSS and SAS. It is, in my experience, overwhelming for students to actually use. It can sometimes be a good resource to look up a specific analysis that the student has done in SPSS or SAS. However, it is outdated and not visually appealing.

These four are commonly used in applied statistics courses wherein the instructor wants to either emphasize the use of R or, at the very least, allow its use.


### Schedule
<!--
Schedule: What is your schedule for writing the book? How much have you already written? What dates do you estimate for completion of a first draft?
-->

I have a first draft prepared, and is available online at: [tysonbarrett.com/Rstats](http://tysonbarrett.com/Rstats). It still needs some things adjusted and added but the framework is there. I'd like to get it ready for publication in the next year or so.

### Credentials
<!--
Credentials: What are your credentials; teaching, writing, research experience; degrees and affiliations; special
qualifications? Do you teach this course regularly?
-->

I have a PhD in Quantitative Psychology, do research regarding methodological advancement, develop R-based research tools, and teach statistics courses each semester for graduate students in public health, behavioral and psychological sciences, and education. I also teach an introductory R course each Fall and Spring semester, which currently uses the draft of this book.


## Annotated Table of Contents
<!--
Please include a table of contents that describes, in as much detail as possible, the contents and approach of each chapter. Please include first- and second-level headings for each chapter. The more information you can provide here, the more substantive the feedback reviewers can provide.
-->

The book is divided into 3 parts: 1) Introduction to the basics of working and understanding data, 2) Analyses with data, and 3) More advanced data cleaning and understanding. Each chapter ends with a link to practice data and code that walks the reader through using the approaches discussed.

##### Part I

- **Chapter 1: The Basics**. This chapter introduces the core concepts of R. This includes the discussion of the various types of objects in R that will be important early on. Because data import is often difficult at first, the chapter also shows basic ways to read in data from different sources. The importing is shown by using the NHANES data set.
- **Chapter 2: Working with and Cleaning Your Data**. This chapter introduces the data manipulation verbs that help data cleaning to be more intuitive. These include *selecting* variables, *filtering* observations, *mutating* variables, among others. These are shown on the NHANES data set imported in Chapter 1.
- **Chapter 3: Understanding Your Data**. This chapter then shows approaches that help the students better understand our cleaned data via summary tables and data visualizations. For the tables, I use the `furniture`, `psych`, and `stargazer` packages. For the visuals, I use `ggplot2` and its extensions.

##### Part II

The chapters in part 2 do not describe any of the analyses in depth. Instead, they show how each method can be done and how assumptions can be checked.

- **Chapter 4: Basic Statistical Analyses**. Linear models are foundational to statistics in the Health, Behavioral, Educational, and Psychological Sciences. I start with t-tests, ANOVAs, and linear regression. This chapter also shows how to use summary tables to present the results of these models.
- **Chapter 5: Generalized Linear Models**. Another major area of statistics that is used in the fields is Generalized Linear Models, particularly logistic regression. I show how several of these models can be run and checked. Again, I show how the results can be presented reproducibly and succinctly.
- **Chapter 6: Multilevel Modeling**. This chapter shows how multilevel models can be used in R, showing its use on nested data and generalized to longitudinal data. Both Generalized Estimating Equations and Linear Mixed Effects models are demonstrated.
- **Chapter 7: Other Modeling Techniques**. The other modeling techniques include mediation analysis, structural equation models, and some basic machine learning techniques. This chapter provides some foundations on which a student can start using R to assess these types of models. 

##### Part III

- **Chapter 8: Advanced data manipulation**. This chapter extends chapter 2 by discussing reshaping data in more depth, using loops to automate data cleaning and communicating (for loops, apply family, `purrr` package), and writing custom functions. 
- **Chapter 9: Advanced plotting**. This chapter extends chapter 3 by showing the flexibility of `ggplot2` by demonstrating the use of different themes, the ability to control font and colors, and the ability to combine plots.
- **Chapter 10: Where to go from here**. This chapter discusses additional resources that a student can use to continue their learning of R. Given this book is an introduction, it provides resources that can extend their data cleaning abilities, their `ggplot2` abilities, among others. It also includes a practice test that allows the student to review each chapter of the book. The conclusion of this chapter ends the book.




<!--chapter:end:Prospectus.Rmd-->

