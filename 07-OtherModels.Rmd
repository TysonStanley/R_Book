---
title: "R for Health, Behavioral, and Social Scientists"
author: "Tyson S. Barrett"
date: "`r Sys.Date()`"
site: "bookdown::bookdown_site"
output:
  bookdown::gitbook: default
documentclass: book
link-citations: yes
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Run but not shown
## Getting data ready for the examples
library(foreign)
library(furniture)
library(tidyverse)
dem_df <- read.xport("~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_demographics_11.xpt")
med_df <- read.xport("~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_MedHeath_11.xpt")
men_df <- read.xport("~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_MentHealth_11.xpt")
act_df <- read.xport("~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_PhysActivity_11.xpt")
names(dem_df) <- tolower(names(dem_df))
names(med_df) <- tolower(names(med_df))
names(men_df) <- tolower(names(men_df))
names(act_df) <- tolower(names(act_df))
df <- dem_df %>%
  full_join(med_df, by="seqn") %>%
  full_join(men_df, by="seqn") %>%
  full_join(act_df, by="seqn") %>%
  mutate(marriage = factor(washer(dmdmartl, 77, 99))) %>%
  filter(complete.cases(marriage)) %>%
  mutate(race = factor(ridreth1, 
                       labels=c("MexicanAmerican", "OtherHispanic", 
                                "White", "Black", "Other"))) %>%
  mutate(famsize = as.numeric(washer(dmdfmsiz, 7,9))) %>%
  mutate(dpq010 = washer(dpq010, 7,9),
         dpq020 = washer(dpq020, 7,9),
         dpq030 = washer(dpq030, 7,9),
         dpq040 = washer(dpq040, 7,9),
         dpq050 = washer(dpq050, 7,9),
         dpq060 = washer(dpq060, 7,9),
         dpq070 = washer(dpq070, 7,9),
         dpq080 = washer(dpq080, 7,9),
         dpq090 = washer(dpq090, 7,9),
         dpq100 = washer(dpq100, 7,9)) %>%
  mutate(dep = dpq010 + dpq020 + dpq030 + dpq040 + dpq050 +
               dpq060 + dpq070 + dpq080 + dpq090) %>%
  mutate(dep2 = factor(ifelse(dep >= 10, 1,
                       ifelse(dep < 10, 0, NA))))
df <- df %>%
  mutate(asthma = washer(mcq010, 9, 7),
         asthma = factor(washer(asthma, 2, value = 0),
                         labels = c("No Asthma", "Asthma"))) %>%
  mutate(sed = washer(pad680, 9999, 7777)) %>%
  mutate(cluster = sdmvstra - 89) %>%
  filter(complete.cases(asthma) & complete.cases(dep2)) 
```


# Chapter 7: Other Modeling Techniques {-}

> "Simplicity is the ultimate sophistication."
>
> --- Leonardo da Vinci

In this chapter we cover, however briefly, modeling techniques that are especially useful to make complex relationships easier to interpret. We will focus on mediation and moderation modeling, methods relating to structural equation modeling (SEM), and methods applicable to our field from machine learning. Although these machine learning may appear very different than mediation and SEM, they each have advantages that can help in different situations. For example, SEM is useful when we know there is a high degree of measurement error or our data has multiple indicators for each construct. On the other hand, regularized regression and random forests--two popular forms of machine learning--are great to explore patterns and relationships there are hundreds or thousands of variables that may predict an outcome. 

Mediation modeling, although often used within SEM, can help us understand pathways of effect from one variable to another. It is especially useful with moderating variables (i.e., variables that interact with another).

So we'll start with discussing mediation, then we'll move on to SEM, followed by machine learning.


## Mediation Modeling {-}

Mediation modeling can be done via several packages. For now, we recommend using `lavaan` (stands for "latent variable analysis")[^lavaa]. Although it is technically still a "beta" version, it performs very well especially for more simple models. It makes mediation modeling straightforward.

Below, we model the following mediation model:
$$
depression = \beta_0 + \beta_1 asthma + \epsilon_1
$$

$$
time_{Sedentary} = \lambda_0 + \lambda_1 asthma + \lambda_2 depression + \epsilon_2
$$

In essence, we believe that asthma increases depression which in turn increases the amount of time spent being sedentary. 

```{r, message=FALSE, warning=FALSE}
library(lavaan)
df$sed_hr = df$sed/60  ## in hours instead of minutes

## Our model
model1 <- '
  dep ~ asthma
  sed_hr ~ dep + asthma
'
## sem function to run the model
fit <- sem(model1, data = df)
summary(fit)
```

From the output we see asthma does predict depression and depression does predict time being sedentary. There is also a direct effect of asthma on sedentary behavior even after controlling for depression. We can further specify the model to have it give us the indirect effect and direct effects tested.

```{r, message=FALSE, warning=FALSE}
## Our model
model2 <- '
  dep ~ a*asthma
  sed_hr ~ b*dep + c*asthma
 
  indirect := a*b
  total := c + a*b
'
## sem function to run the model
fit2 <- sem(model2, data = df)
summary(fit2)
```

We defined a few things in the model. First, we gave the coefficients labels of `a`, `b`, and `c`. Doing so allows us to define the `indirect` and `total` effects. Here we see the indirect effect, although small, is significant at $p < .001$. The total effect is larger (not surprising) and is also significant.

Also note that we can make the regression equations have other covariates as well if we needed to (i.e. control for age or gender) just as we do in regular regression. 

```{r, message=FALSE, warning=FALSE}
## Our model
model2.1 <- '
  dep ~ asthma + ridageyr
  sed_hr ~ dep + asthma + ridageyr
'
## sem function to run the model
fit2.1 <- sem(model2.1, data = df)
summary(fit2.1)
```

Although we don't show it here, we can also do moderation ("interactions") as part of the mediation model.

## Structural Equation Modeling {-}

Instead of summing our depression variable, we can use SEM to run the mediation model from above but use the latent variable of depression instead.

```{r, message=FALSE, warning=FALSE}
## Our model
model3 <- '
  dep1 =~ dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090
  dep1 ~ a*asthma
  sed_hr ~ b*dep1 + c*asthma

  indirect := a*b
  total := c + a*b
'
## sem function to run the model
fit3 <- sem(model3, data = df)
summary(fit3)
```

We defined `dep1` as a latent variable using `=~`. The regression, indirect, and total results are very similar the summed score version. Although the model does not fit the data well--"`P-value (Chi-square) = 0.000`"--it is informative for demonstration. We would likely need to find out how the measurement model (`dep1 =~ dpq010 + dpq020 + dpq030 +`) actually fits before throwing it into a mediation model. We can do that via:

```{r}
model4 <- '
  dep1 =~ dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090
'
fit4 <- cfa(model4, data=df)
summary(fit4)
```

As we can see, there is a lack of fit in the measurement model. It is possible that these depression questions could be measuring more than one factor. We could explore this using exploratory factor analysis. We don't demonstrate that here, but know that it is possible to do in `R` with a few other packages.


## Machine Learning Techniques {-}

We are briefly going to introduce some machine learning techniques that may be of interest to researchers. We will quickly introduce and demonstrate:

1. Ridge, Lasso and Elastic Net
2. Random Forests

### Ridge, Lasso and Elastic Net {-}

In order to do either ridge, lasso, or elastic net regression, we can use the fantastic `glmnet` package. Using the `cv.glmnet()` function we can run the ridge ($alpha = 0$), lasso ($alpha = 1$ which is default), and elastic net ($0 \leq alpha \leq 1$). It turns out that elastic net is the combination of the ridge and lasso methods and the closer `alpha` is to 1 the more it acts like lasso and the closer it is to 0 the more it acts like ridge.

Lasso and elastic net can do variable selection in addition to estimation. Ridge is great at handling correlated predictors. Each of them are better than conventional methods at prediction and each of them can handle large numbers of predictors. To learn more see "Introduction to Statistical Learning" by Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie. A feww PDF is available on their website.

To use the package, it wants the data in a very specific form. First, we need to remove any missingness. We use `na.omit()` to do this. We take all the predictors (without the outcome) and put it in a data matrix object. We only include a few for the demonstration but you can include *many* predictors. We name ours `X`. `Y` is our outcome.

```{r, message=FALSE, warning=FALSE}
df2 <- df %>%
  dplyr::select(riagendr, ridageyr, ridreth3, race, famsize, dep, asthma, sed_hr) %>%
  na.omit
X <- df2 %>%
  dplyr::select(-sed_hr) %>%
  data.matrix
Y <- df2$sed_hr
```

Then we use the `cv.glmnet()` function to fit the different models. The "cv" refers to cross-validation[^crossval], which we don't discuss here, but it an important topic to become familiar with. Below we fit a ridge, a lasso, and an elastic net model. The elastic net model uses more of the lasso penalty because the `alpha` is closer to 1 than 0.

```{r, message=FALSE, warning=FALSE}
library(glmnet)

fit_ridge <- cv.glmnet(X, Y, alpha = 0)
fit_lasso <- cv.glmnet(X, Y, alpha = 1)
fit_enet  <- cv.glmnet(X, Y, alpha = .8)
```

The plots below show where appropriate `lambda` values are based on the mean squared error of the cross-validated prediction. The vertical dashed lines show a reasonable range of lambda values that can be used.

```{r}
par(mfrow = c(2,2))  ## put plots together on 2 x 2 grid
plot(fit_ridge)
plot(fit_lasso)
plot(fit_enet)
```

We can get the coefficients at a reasonable `lambda`. Specifically, we use the "1-SE rule" (near the right hand side vertical dashed lines in the above plots) by `s = "lambda.1se"`. You can directly tell it what `lambda` value you'd like but this is a simple rule of thumb.

```{r}
coef(fit_ridge, s = "lambda.1se")
coef(fit_lasso, s = "lambda.1se")
coef(fit_enet, s = "lambda.1se")
```

Although we briefly introduce these regression methods, they are indeed very important. We highly recommend learning more about them.


### Random Forests {-}

Random forests is another machine learning method that can do fantastic prediction. It is built in a very different way than the methods we have discussed up to this point. It is not built on a linear modeling scheme; rather, it is built on classification and regression trees (CART). Again, "Introduction to Statistical Learning" is a great resource to learn more.

Conveniently, we can use the `randomForest` package. We specify the model by the formula `sed_hr ~ .` which means we want `sed_hr` to be the outcome and all the rest of the variables to be predictors.

```{r}
library(randomForest)

fit_rf <- randomForest(sed_hr ~ ., data = df2)
fit_rf
```

We can find out which variables were important in the model via:

```{r}
par(mfrow=c(1,1))  ## back to one plot per page
varImpPlot(fit_rf)
```

We can see that age (`ridageyr`) is the most important variable, depression (`dep`) follows, with the family size (`famsize`) the third most important in the random forests model.


## Conclusions {-}

Although we only discussed these methods briefly, that does not mean they are less important. On the contrary, they are essential upper level statistical methods. This brief introduction hopefully helped you know what `R` is capable of across a wide range of methods.

The next chapter begins our "advanced" topics, starting with "Advanced Data Manipulation".

[^lavaa]: The `lavaan` package has some great vignettes at [http://lavaan.ugent.be/](http://lavaan.ugent.be/) to help with the other types of models it can handle.

[^crossval]: Cross-validation is a common way to reduce over-fitting and make sure your model is generalizable. Generally, you split your data into training and testing sets. It is very common in machine learning and is beginning to be practiced in academic fields as well. We recommend using it as often as you can, especially with these methods but also to make sure your other models are accurate on new data as well.
