[
["index.html", "Rstats for Researchers Preface Part I Part II Part III Download R and RStudio", " Rstats for Researchers Tyson S. Barrett 2017-01-12 Preface “If I have seen further, it is by standing on the shoulders of giants.” — Isaac Newton, circa 1676 Science. It is built on replication and construction. Once an idea is established and well replicated, the next steps are to build on to it. We try to build from a foundation established by others. It should also be how we analyze our data, not only in methodology but also in software. Lucky for us, R is how we do just that. This book is for beginners in R; especially those in health, behavioral and social sciences. R is wonderfully unique. It is an open source project and improvements—known as packages—are being made by smart individuals across the globe. We’ll introduce you to many ways in which R can be used in your work. You’ll find that it can help in all facets of your data analysis and communication, while improving your replicability. In the long run, taking some time to learn a knew tool will save you time, energy, and probably most importantly, frustration. When a researcher is frustrated, it becomes so easy to overlook important features. We will quickly, and succinctly, introduce the newest, easiest, and most understandable ways of working with your data. To do this, we will have three main parts: 1) working with and simple analyses of your data, 2) modeling your data, and 3) more advanced techniques to help your workflow. Part I Chapter 1: Introduces the basics of the language Chapter 2: Manipulate Your Data (reshape, subset, join) Chapter 3: Understanding your data (summary statistics, ggplot2) Part II Chapter 4: Basic Analyses (ANOVA, Linear Regression) Chapter 5: Generalized Linear Models Chapter 6: Mulilevel Modeling Chapter 7: Other Modeling Techniques Part III Chapter 8: Advanced data manipulation Chapter 9: Advanced plotting Chapter 10: Where to go from here At the end of the book, you should be able to: 1) use R to perform your data manipulation and data analyses and 2) understand online helps (e.g. www.stackoverflow.com, www.r-bloggers.com) so your potential in R becomes nearly limitless. Download R and RStudio To begin, you will need to download the R software www.r-project.org and then the RStudio software www.rstudio.com. R is the brains and RStudio1 is an “IDE” (something that helps us work with R much more easily). Once both are installed (helps on installing the software can be found on www.rstudio.com, www.r-bloggers.com, and www.statmethods.net) you are good to go. The remainder of the book will be about actually using it. Enjoy!2 Get the free version of Rstudio. Believe me, it doesn’t feel like it should be free software.↩ Note that to return to Tyson’s blog, you can click here↩ "],
["chapter-1-the-basics.html", "Chapter 1: The Basics Objects, Data Types and Functions Importing Data Saving Data Conclusions", " Chapter 1: The Basics Let’s jump right into it. R is an open source statistical software made by statisticians. This means it generally speaks the language of statistics. This is very helpful when it comes running analyses but can be confusing when starting to understand the code. Early Advice: Don’t get overwhelmed. It may feel like there is a lot to learn, but taking things one at a time will work surprisingly quickly. I’ve designed this book to discuss what you need to know from the beginning. Other topics that are not discussed are things you can learn later and do not need to be of your immediate concern. Because learning code is best through experience, we’ll start with a simple walk-through of data types, functions, and objects. Objects, Data Types and Functions Objects R uses objects and operators. An object, just like in the physical world, is something you can do things with. In the real world, we have objects that we use regularly. For example, we have chairs. Chairs are great for some things (sitting, sleeping) and horrible at others (driving, flying). Similarly, in R each type of object is useful for certain things. The data types we just discussed are certain types of objects. The data frames objects are a very good way to store data in an easily analyzable format. Because this is so analogous to the real world, it becomes quite natural to work with. You can have many objects in memory, which allows flexbility in analyzing many different things simply within a single R session. For your work, the first thing you work with will be data in various forms. Below, we explain the different data types and how they can combine into what is known as a data.frame. Data Types In R there are three main data types that you’ll work with in research: numeric factor character The first, numeric, is just that: numbers. In R, you can make a numeric variable with the code below: x &lt;- c(10.1, 2.1, 4.6, 2.3, 8.9) The c() is a function 3 that stands for “concatenate” which basically glues the values inside the paratheses together into one. We use &lt;- to put it into x. So in this case, x (which we could have named anything) is saving those values so we can work with them4. A factor variable is a categorical variable (i.e., only a limited number of options exist). For example, race/ethnicity is a factor variable. race &lt;- c(1, 3, 2, 1, 1, 2, 1, 3, 4, 2) The code above actually produces a numeric vector (since it was only provided numbers). We can quickly tell R that it is indeed supposed to be a factor. race &lt;- factor(race, labels = c(&quot;white&quot;, &quot;black&quot;, &quot;hispanic&quot;, &quot;asian&quot;)) The factor() function tells R that the first thing race is actually a factor. The additional argument labels tells R what each of the values means. If we print out race we see that R has replaced the numeric values with the labels. race ## [1] white hispanic black white white black white ## [8] hispanic asian black ## Levels: white black hispanic asian Finally, and maybe less relevantly, there are character variables. These are words (known as strings). In research this is often where subjects give open responses to a question. ch &lt;- c(&quot;I think this is great.&quot;, &quot;I would suggest you learn R.&quot;, &quot;You seem quite smart.&quot;) When we combine multiple variables into one, we create a data.frame. A data frame is like a spreadsheet table, like the ones you have probably seen in excel and SPSS. Here’s a simple example: df &lt;- data.frame(&quot;A&quot;=c(1,2,1,4,3), &quot;B&quot;=c(1.4,2.1,4.6,2.0,8.2), &quot;C&quot;=c(0,0,1,1,1)) df ## A B C ## 1 1 1.4 0 ## 2 2 2.1 0 ## 3 1 4.6 1 ## 4 4 2.0 1 ## 5 3 8.2 1 We can do quite a bit with the data.frame that we called df5. Once again, we could have called this data frame anything, although I recommend short names. If “A” and “C” are factors we can tell R by: df$A &lt;- factor(df$A, labels = c(&quot;level1&quot;, &quot;level2&quot;, &quot;level3&quot;, &quot;level4&quot;)) df$C &lt;- factor(df$C, labels = c(&quot;Male&quot;, &quot;Female&quot;)) In the above code, the $ reaches into df to grab a variable (or column). The following code does the exact same thing: df[[&quot;A&quot;]] &lt;- factor(df$A, labels = c(&quot;level1&quot;, &quot;level2&quot;, &quot;level3&quot;, &quot;level4&quot;)) df[[&quot;C&quot;]] &lt;- factor(df$C, labels = c(&quot;Male&quot;, &quot;Female&quot;)) and so is the following: df[, &quot;A&quot;] &lt;- factor(df$A, labels = c(&quot;level1&quot;, &quot;level2&quot;, &quot;level3&quot;, &quot;level4&quot;)) df[, &quot;C&quot;] &lt;- factor(df$C, labels = c(&quot;Male&quot;, &quot;Female&quot;)) df[[&quot;A&quot;]] grabs the A variable just like df$A. The last example shows that we can grab both columns and rows. In df[, &quot;C&quot;] we have a spot just a head of the comma. It works like this: df[rows, columns]. So we could specifically grab certain rows and certain columns. df[1:3, &quot;A&quot;] df[1:3, 1] Both lines of the above code grabs rows 1 thorugh 3 and column “A”. Finally, we can combine the c() function to grab different rows and columns. To grab rows 1 and 5 and columns “B” and “C” you can do the following: df[c(1,5), c(&quot;B&quot;, &quot;C&quot;)] We may also want to get more information about the data frame before we do any subsetting. There are a few nice functions to get information that can help us know what we should do next with our data. ## Get the variable names names(df) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; ## Know what type of variable it is class(df$A) ## [1] &quot;factor&quot; ## Get quick summary statistics for each variable summary(df) ## A B C ## level1:2 Min. :1.40 Male :2 ## level2:1 1st Qu.:2.00 Female:3 ## level3:1 Median :2.10 ## level4:1 Mean :3.66 ## 3rd Qu.:4.60 ## Max. :8.20 ## Get the first 10 columns of your data head(df, n=10) ## A B C ## 1 level1 1.4 Male ## 2 level2 2.1 Male ## 3 level1 4.6 Female ## 4 level4 2.0 Female ## 5 level3 8.2 Female We admit that the last one is a bit pointless since our data frame is only a few lines long. However, these functions can give you quick information about your data with hardly any effort on your part. Functions Earlier we mentioned that c() was a “function.” Functions are how we do things we our data. There are probably hundreds of thousands of functions at your reach. In fact, you can create your own! We’ll discuss that more later. For now, know that each function has a name (the function name of c() is “c”), arguments, and output of some sort. Arguments are the information that you provide the function (e.g. we gave c() a bunch of numbers). Output from a function varies to a massive degree but, in general, the output is what you are using the function for (e.g., for c() we wanted to create a vector—a variable—of data). At any point, by typing: ?functionname we get information in the “Help” window providing the arguments and output. Below we show you several functions for both importing and saving data. Note that each have a name, arguments, and output. Importing Data Most of the time you’ll want to import data into R rather than manually entering it line by line, variable by variable. There are some built in ways to import many delimited6 data types (e.g. comma delimited–also called a CSV, tab delimited, space delimited). Other packages7 have been developed to help with this as well. The first, if it is an R data file in the form .rda or .RData simply use: load(&quot;file.rda&quot;) Note that you don’t assign this to a name such as df. Instead, it loads whatever R objects were saved to it. Delimited Files Most delimited files are saved as .csv, .txt, or .dat. As long as you know the delimiter, this process is easy. df &lt;- read.table(&quot;file.csv&quot;, sep = &quot;,&quot;, header=TRUE) ## for csv df &lt;- read.table(&quot;file.txt&quot;, sep = &quot;\\t&quot;, header=TRUE) ## for tab delimited df &lt;- read.table(&quot;file.txt&quot;, sep = &quot; &quot;, header=TRUE) ## for space delimited The argument sep tells the function what kind of delimiter the data has and header tells R if the first row contains the variable names (you can change it to FALSE if the first row isn’t). Note that at the end of the lines you see that I left a comment using #. I used two for stylistic purposes but only one is necessary. Anything after a # is not read by the computer; it’s just for us humans. Heads up! Note that unless you are using the load function, you need to assign what is being read in to a name. In the examples, all were called df. In real life, you won’t run a bunch of different read functions to the same name because only the last one run would be saved (the others would be written over). However, if you have multiple data files to import you can assign them to different names and later merge them. Merging, also called joining, is something we’ll discuss in the next chapter. Other Data Formats Data from other statistical software such as SAS, SPSS, or Stata are also easy to get into R. We will use two powerful packages: haven foreign To install, simply run: install.packages(&quot;packagename&quot;) This only needs to be run once on a computer. Then, to use it in a single R session (i.e. from when you open R to when you close it) run: library(packagename) Using these packages, I will show you simple ways to bring your data in from other formats. library(haven) df &lt;- read_dta(&quot;file.dta&quot;) ## for Stata data df &lt;- read_spss(&quot;file.sav&quot;) ## for SPSS data df &lt;- read_sas(&quot;file.sas7bdat&quot;) ## for this type of SAS file library(foreign) df &lt;- read.xport(&quot;file.xpt&quot;) ## for export SAS files If you have another type of data file to import, online helps found on sites like www.stackoverflow.com and www.r-bloggers.com often have the solution. Saving Data Finally, there are many ways to save data. Most of the read... functions have a corresponding write... function. write.table(df, file=&quot;file.csv&quot;, sep = &quot;,&quot;) ## to create a CSV data file R automatically saves missing data as NA since that is what it is in R. But often when we write a CSV file, we might want it as blank or some other value. If that’s the case, we can add another argument na = &quot; &quot; after the sep argument. If you ever have questions about the specific arguments that a certain function has, you can simply run: ?functionname So, if you were curious about the different arguments in write.table simply run: ?write.table. In the pane with the files, plots, packages, etc. a document will show up to give you more informaton. Conclusions R is designed to be flexible and do just about anything with data that you’ll need to do as a researcher. With this chapter under your belt, you can now read basic R code, import and save your data. The next chapter will introduce the “tidyverse” of methods that can help you join, reshape, summarize, group, and much more. R is all about functions. Functions tell R what to do with the data. You’ll see many more examples throughout the book.↩ This is a great feature of R. It is called “object oriented” which basically means R creates objects to work with. I discuss this more in 1.2.↩ I used this name since df is common in online helps and other resources.↩ The delimiter is what separates the pieces of data.↩ A package is an extension to R that gives you more functions–abilities–to work with data. Anyone can write a package, although to get it on the Comprehensive R Archive Network (CRAN) it needs to be vetted to a large degree. In fact, after some practice, you could write a package to help you more easily do your work.↩ "],
["chapter-2-manipulating-your-data.html", "Chapter 2: Manipulating Your Data Tidy Methods Piping Select and Filter Grouping and Summarizing Reshaping Joining (merging)", " Chapter 2: Manipulating Your Data In order to manipulate your data in the cleanest, most up-to-date manner, we are going to be using the “tidyverse” group of methods. The tidyverse8 is a group of packages9 that provide a simple syntax that can do many basic (and complex) data manipulating. The group of packages can be downloaded via: install.packages(&quot;tidyverse&quot;) After downloading it, simply use: library(tidyverse) Note that when we loaded tidyverse it loaded 6 packages and told you of “conflicts”. These conflicts are where two or more loaded packages have the same function in them. The last loaded package is the one that R will use by default. For example, if we loaded two packages–awesome and amazing–and both had the function–make_really_great and we loaded awesome and then amazing as so: library(awesome) library(amazing) R will automatically use the function from amazing. We can still access the awesome version of the function (because even though the name is the same, they won’t necessarily do the same things for you). We can do this by: awesome::make_really_great(arg) That’s a bit of an aside, but know that you can always get at a function even if it is “masked” from your current session. Tidy Methods I’m introducing this to you for a couple reasons. It simplifies the code and makes the code more readable. As the saying goes, there are always at least two collaborators on any project: you and future you. It is the cutting edge. The most influential individuals in the R world, including the makers and maintainers of RStudio, use these methods and syntax. The majority of what you’ll need to do with data as a researcher will be covered by these functions. The goal of these functions is to help tidy up your data. Tidy data is based on columns being variables and rows being observations. It is the form that data needs to be in to analyze it, whether that analysis is by graphing, modeling, or other means. There are several methods that help create tidy data: Piping Selecting and Filtering Grouping and Summarizing Reshaping Joining (merging) To help illustrate each aspect, we are going to use real data from the National Health and Nutrition Examiniation Survey (NHANES). I’ve provided this data at https://tysonstanley.github.io/assets/Data/NHANES.zip. I’ve cleaned it up somewhat already. Let’s quickly read that data in so we can use it throughout the remainder of this chapter. First, we will set our working directory with setwd. This tells R where to look for files, including your data files. My specific file location is below so you will need to adjust it to wherever you saved the NHANES data. setwd(&quot;~/Dropbox/GitHub/blog_rstats/assets/Data/&quot;) library(foreign) dem_df &lt;- read.xport(&quot;NHANES_demographics_11.xpt&quot;) med_df &lt;- read.xport(&quot;NHANES_MedHeath_11.xpt&quot;) men_df &lt;- read.xport(&quot;NHANES_MentHealth_11.xpt&quot;) act_df &lt;- read.xport(&quot;NHANES_PhysActivity_11.xpt&quot;) Now we have four separate, but related, data sets in memory: dem_df containing demographic information med_df containing medical health information men_df containing mental health information act_df containing activity level information Since all of them have all-cap variable names, we are going to quickly change this with a little trick: names(dem_df) &lt;- tolower(names(dem_df)) names(med_df) &lt;- tolower(names(med_df)) names(men_df) &lt;- tolower(names(men_df)) names(act_df) &lt;- tolower(names(act_df)) This takes the names of the data frame (on the right hand side), changes them to lower case and then reassigns them to the names of the data frame.10 We will now go through each aspect of the tidy way of working with data using these four data sets. Piping Let’s introduce a few major themes in this tidyverse. First, the pipe operator – %&gt;%. It helps simplify the code and makes things more readable. It takes what is on the left hand side and puts it in the right hand side’s function. dem_df %&gt;% summary So the above code takes the data frame df and puts it into the summary function. This does the same thing as summary(df). In this simple case, it doesn’t really make the code more readable, but in more complex situations it can really help. In the following sections you will see how and where this type of coding is helpful. Select and Filter We often want to subset our data in some way before we do many of our analyses. This can make our cognitive load of the data much lighter but it can also be very important in the actual analyses. The code below show the two main ways to subset your data: 1) selecting variables and 2) filtering observations. To select three variables (i.e. gender [“riagendr”], age [“ridageyr”], and ethnicity [“ridreth1”]) we: selected_dem &lt;- dem_df %&gt;% select(riagendr, ridageyr, ridreth1) Now, selected_dem has three variables and all the observations. We can also filter (i.e. take out observations we don’t want): filtered_dem &lt;- dem_df %&gt;% filter(riagendr == 1) Since when riagendr == 1 the individual is male, filtered_dem only has male participants. We can add multiple filtering options as well: filtered_dem &lt;- dem_df %&gt;% filter(riagendr == 1 &amp; ridageyr &gt; 16) We now have only males that are older than 16 years old. We used &amp; to say we want both conditions to be met. Alternatively, we could: filtered_dem &lt;- dem_df %&gt;% filter(riagendr == 1 | ridageyr &gt; 16) which, using | we are saying we want males or individuals older than 16. In other words, if either are met, that observation will be kept. Finally, we can do all of these in one step: filtered_dem &lt;- dem_df %&gt;% select(riagendr, ridageyr, ridreth1) %&gt;% filter(riagendr == 1 &amp; ridageyr &gt; 16) where we use two %&gt;% operators to grab dem_df, select the three variables, and then filter the rows that we want. Grouping and Summarizing A major aspect of analysis is comparing groups. Lucky for us, this is very simple in R. I call it the three step summary: Data Group by Summarize ## Our Grouping Variable as a factor dem_df$citizen &lt;- factor(dem_df$dmdcitzn) ## Three step summary: dem_df %&gt;% ## 1. Data group_by(citizen) %&gt;% ## 2. Group by summarize(N = n()) ## 3. Summarize ## # A tibble: 4 × 2 ## citizen N ## &lt;fctr&gt; &lt;int&gt; ## 1 1 8685 ## 2 2 1040 ## 3 7 26 ## 4 NA 5 The output is very informative. The first column is the grouping variable and the second is the N (number of individuals) by group. We can quickly see that there are four levels, currently, to the citizen variable. After some reading of the documentation we see that 1 = Citizen and 2 = Not a Citizen. A value of 7 it turns out is a placeholder value for missing. And finally we have an NA category. It’s unlikely that we want those to be included in any analyses, unless we are particularly interested in the missingness on this variable. So let’s do some simple cleaning to get this where we want it. To do this, we will use the furniture package. install.packages(&quot;furniture&quot;) library(furniture) dem_df$citizen &lt;- washer(dem_df$citizen, 7) ## Changes all 7&#39;s to NA&#39;s dem_df$citizen &lt;- washer(dem_df$citizen, 2, value=0) ## Changes all 2&#39;s to 0&#39;s Now, our citizen variable is cleaned, with 0 meaning not a citizen and 1 meaning citizen. Let’s rerun the code from above with the three step summary: ## Three step summary: dem_df %&gt;% ## 1. Data group_by(citizen) %&gt;% ## 2. Group by summarize(N = n()) ## 3. Summarize ## # A tibble: 3 × 2 ## citizen N ## &lt;chr&gt; &lt;int&gt; ## 1 0 1040 ## 2 1 8685 ## 3 &lt;NA&gt; 31 Its clear that the majority of the subjects are citizens. We can also check multiple variables at the same time, just separating them with a comma in the summarize function. ## Three step summary: dem_df %&gt;% ## 1. Data group_by(citizen) %&gt;% ## 2. Group by summarize(N = n(), ## 3. Summarize Age = mean(ridageyr, na.rm=TRUE)) ## # A tibble: 3 × 3 ## citizen N Age ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1040 37.3 ## 2 1 8685 30.7 ## 3 &lt;NA&gt; 31 40.4 We used the n() function (which gives us counts) and the mean() function which, shockingly, gives us the mean. Note that if there are NA’s in the variable, the mean (and most other functions like it) will give the result NA. To have R ignore these, we tell the mean function to remove the NA’s when you compute this using na.rm=TRUE. This pattern of grouping and summarizing is something that will follow us throughout the book. It’s a great way to get to know your data well and to make decisions on what to do next with your data. Reshaping This is a big part of working with data. Unfortunately, it is also a difficult topic to understand without much practice at it. In general, two data formats exist: Wide form Long form Only when the data is cross-sectional and each individual is a row does this distinction not matter much. Otherwise, if there are multiple measures per individual, or there are multiple individuals per cluster, the distinction between wide and long is very important for modeling and visualization. Wide Form Wide form generally has one unit (i.e. individual) per row. This generally looks like: ## ID Var_Time1 Var_Time2 ## 1 1 -0.09358 0.70695 ## 2 2 0.55145 0.88045 ## 3 3 -0.06696 0.85022 ## 4 4 0.00807 0.13881 ## 5 5 0.20698 0.97789 ## 6 6 -0.90115 0.33348 ## 7 7 -2.80222 0.30193 ## 8 8 -0.22526 0.63240 ## 9 9 0.29547 0.00429 ## 10 10 -1.43070 0.02104 Notice that each row has a unique ID. This format is common in the ANOVA family of analyses. These can be used when there are set time points that the individual is measured and each individual has the same number of time points. This is not always reasonable, but when it is, wide format works just fine. However, when these conditions don’t hold, mixed effects modeling (i.e. multilevel modeling, hierarchical linear modeling) is particularly useful. In order to do mixed effects modeling, long format is required. Long Form In contrast, long format has the lowest nested unit as a single row. This means that a single ID can span multiple rows, usually with a unique time point for each row as so: ## ID Time Var ## 1 1 1 0.6854 ## 2 1 2 0.0559 ## 3 1 3 0.7443 ## 4 1 4 0.3197 ## 5 2 1 0.1757 ## 6 2 2 0.2728 ## 7 3 1 0.6390 ## 8 3 2 0.7865 ## 9 3 3 0.3724 Notice that a single ID spans multiple columns and that each row has only one time point. Here, time is nested within individuals making it the lowest unit. Therefore, each row corresponds to a single time point. Generally, this is the format we want for most modeling techniques and most visualizations. Joining (merging) The final topic in the chapter is joining data sets This is common when we are using bigger We currently have 4 data sets that have mostly the same people in them but with different variables. One tells us about the demographics; another gives us information on mental health. We may have questions that ask whether a demographic characteristics is related to a mental health factor. This means we need to merge, or join, our data sets.11 When we merge a data set, we combine them based on some ID variable(s). Here, this is simple since each individual is given a unique identifier in the variable seqn. Within the dplyr package there are four main joining functions: inner_join, left_join, right_join and full_join. Each join combines the data in slightly different ways. Let’s first load dplyr: library(dplyr) Inner Join Here, only those individuals that are in both data sets that you are combining will remain. So if person “A” is in data set 1 and not in data set 2 then he/she will not be included. inner_join(df1, df2, by=&quot;IDvariable&quot;) Left or Right Join This is similar to inner join but now if the individual is in data set 1 then left_join will keep them even if they aren’t in data set 2. right_join means if they are in data set 2 then they will be kept whether or not they are in data set 1. left_join(df1, df2, by=&quot;IDvariable&quot;) ## keeps all in df1 right_join(df1, df2, by=&quot;IDvariable&quot;) ## keeps all in df2 Full Join This one simply keeps all individuals that are in either data set 1 or data set 2. full_join(df1, df2, by=&quot;IDvariable&quot;) Each of the left, right and full joins will have missing values placed in the variables where that individual wasn’t found. For example, if person “A” was not in df2, then in a full join they would have missing values in the df1 variables. For our NHANES example, we will use full_join to get all the data sets together. Note that in the code below we do all the joining in the same overall step. df &lt;- dem_df %&gt;% full_join(med_df, by=&quot;seqn&quot;) %&gt;% full_join(men_df, by=&quot;seqn&quot;) %&gt;% full_join(act_df, by=&quot;seqn&quot;) So now df is the the joined data set of all four. We started with dem_df joined it with med_df by seqn then joined that joined data set with men_df by seqn, and so on. For analyses in the next chapter, we will use this new df object that is the combination of all the data sets that we had before. Hadley Wickham (2016). tidyverse: Easily Install and Load ‘Tidyverse’ Packages. R package version 1.0.0. https://CRAN.R-project.org/package=tidyverse↩ Remember, a package is an extension to R that gives you more functions that you can easily load into R.↩ Note that these are not particularly helpful names, but they are the names provided in the original data source. If you have questions about the data, visit http://wwwn.cdc.gov/Nchs/Nhanes/Search/Nhanes11_12.aspx.↩ Note that this is different than adding new rows but not new variables. Merging requires that we have at least some overlap of individuals in both data sets.↩ "],
["chapter-3-understanding-and-describing-your-data.html", "Chapter 3: Understanding and Describing Your Data Descriptive Statistics Visualizations", " Chapter 3: Understanding and Describing Your Data We are going to take what we’ve learned from the previous two chapters and use them together to have simple but powerful ways to understand your data. This chapter will be broken down into: Descriptive Statistics Visualizations The two go hand-in-hand in understanding what is happening in your data. We are often most interested in three things when exploring our data: understanding distributions, understanding relationships, and looking for outliers or errors. Descriptive Statistics Several methods of discovering descriptives in a succinct way have been developed for R. My favorite (full disclosure: it is one that I made so I may be biased) is the table1 function in the furniture package. This function has been designed to be simple and complete. It produces a well-formatted table that you can easily export and use as a table in a report or article.12 We’ll first create a ficticious data set and we’ll show the basic build of table1. library(furniture) df &lt;- data.frame(&quot;A&quot;=c(1,2,1,4,3,NA), &quot;B&quot;=c(1.4,2.1,4.6,2.0,NA,3.4), &quot;C&quot;=c(0,0,1,1,1,1), &quot;D&quot;=rnorm(6)) table1(df, A, B, C, D) ## ## |==============================| ## Mean/Count (SD/%) ## Observations 6 ## A ## 2.20 (1.30) ## B ## 2.70 (1.29) ## C ## 0.67 (0.52) ## D ## -0.00 (0.77) ## |==============================| This quickly gives you means and standard deviations (or counts and percentages if there were categorical variables). We could have also used the pipe operator here if we wanted via: df %&gt;% table1(A, B, C, D) ## ## |==============================| ## Mean/Count (SD/%) ## Observations 6 ## A ## 2.20 (1.30) ## B ## 2.70 (1.29) ## C ## 0.67 (0.52) ## D ## -0.00 (0.77) ## |==============================| It turns out, for we want “A” and “C” to be factors. df$A &lt;- factor(df$A, labels=c(&quot;cat1&quot;, &quot;cat2&quot;, &quot;cat3&quot;, &quot;cat4&quot;)) df$C &lt;- factor(df$C, labels=c(&quot;male&quot;, &quot;female&quot;)) table1(df, A, B, C, D) ## ## |==============================| ## Mean/Count (SD/%) ## Observations 6 ## A ## cat1 2 (40%) ## cat2 1 (20%) ## cat3 1 (20%) ## cat4 1 (20%) ## B ## 2.70 (1.29) ## C ## male 2 (33.3%) ## female 4 (66.7%) ## D ## -0.00 (0.77) ## |==============================| So now we see the counts and percentages for the factor variables. But now we can take a step further and look for relationships. The code below shows the means/standard devaitions or counts/percentages by a grouping variable–in this case, C. table1(df, A, B, D, splitby = ~C) ## ## |=====================================| ## male female ## Observations 2 4 ## A ## cat1 1 (50%) 1 (33.3%) ## cat2 1 (50%) 0 (0%) ## cat3 0 (0%) 1 (33.3%) ## cat4 0 (0%) 1 (33.3%) ## B ## 1.75 (0.49) 3.33 (1.30) ## D ## -0.08 (0.29) 0.04 (0.98) ## |=====================================| We can also test for differences by group as well (although this is not particularly good with a sample size of 5). It produces a warning since the \\(\\chi^2\\) approximation is not accurate with cells this small. table1(df, A, B, D, splitby = ~C, test=TRUE) ## ## |=============================================| ## male female P-Value ## Observations 2 4 ## A 0.405 ## cat1 1 (50%) 1 (33.3%) ## cat2 1 (50%) 0 (0%) ## cat3 0 (0%) 1 (33.3%) ## cat4 0 (0%) 1 (33.3%) ## B 0.162 ## 1.75 (0.49) 3.33 (1.30) ## D 0.842 ## -0.08 (0.29) 0.04 (0.98) ## |=============================================| Finally, we can include missingness in the table for factors. table1(df, A, B, D, splitby = ~C, test=TRUE, NAkeep = TRUE) ## ## |=============================================| ## male female P-Value ## Observations 2 4 ## A 0.405 ## cat1 1 (50%) 1 (25%) ## cat2 1 (50%) 0 (0%) ## cat3 0 (0%) 1 (25%) ## cat4 0 (0%) 1 (25%) ## NA 0 (0%) 1 (25%) ## B 0.162 ## 1.75 (0.49) 3.33 (1.30) ## D 0.842 ## -0.08 (0.29) 0.04 (0.98) ## |=============================================| So with three or four short lines of code we can get a good idea about variables that may be related to the grouping variable and any missingness in the factor variables. There’s much more you can do with table1 and there are vignettes and tutorials available to learn more.13 Other quick descriptive functions exist; here are a few of them. summary(df) ## descriptives for each variable in the data library(psych) ## install first describe(df) ## produces summary statistics for continuous variables library(Hmisc) ## install first Hmisc::describe(df) ## gives summary for each variable separately Visualizations Understanding your data, in my experience, generally requires visualizations. If we are going to use a model of some sort, understanding the distributions and relationships beforehand are very helpful in interpreting the model and catching errors in the data. Also finding any outliers or errors that could be highly influencing the modeling should be understood beforehand. For simple but appealing visualizations we are going to be using ggplot2. This package is used to produce professional level plots for many journalism organizations (e.g. five-thrity-eight). These plots are quickly presentation quality and can be used to impress your boss, your advisor, or your friends. 0.0.1 Using ggplot2 This package has a straight-forward syntax. It is built by adding layers to the plot. library(ggplot2) ## first install using install.packages(&quot;ggplot2&quot;) First, we have a nice qplot function that is short for “quick plot.” It quickly decides what kind of plot is useful given the data and variables you provide. qplot(df$A) ## Makes a simple histogram qplot(df$D, df$B) ## Makes a scatterplot ## Warning: Removed 1 rows containing missing values (geom_point). For a bit more control over the plot, you can use the ggplot function. The first piece is the ggplot piece. From there, we add layers. These layers generally start with geom_ then have the type of plot. Below, we start with telling ggplot the basics of the plot and then build a boxplot. The x-axis is the variable “C” and the y-axis is the variable “D” and then we color it by variable “C” as well. ggplot(df, aes(x=C, y=D)) + geom_boxplot(aes(color = C)) Here’s a few more examples: ggplot(df, aes(x=C)) + geom_bar(stat=&quot;count&quot;, aes(fill = C)) ggplot(df, aes(x=B, y=D)) + geom_point(aes(color = C)) ## Warning: Removed 1 rows containing missing values (geom_point). Note that the warning that says it removed a row is because we had a missing value in “C”. We are going to make the first one again but with some aesthetic adjustments. Notice that we just added two extra lines telling ggplot2 how we want some things to look.14 ggplot(df, aes(x=C, y=D)) + geom_boxplot(aes(color = C)) + theme_bw() + scale_color_manual(values = c(&quot;dodgerblue4&quot;, &quot;coral2&quot;)) The theme_bw() makes the background white, the scale_color_manual() allows us to change the colors in the plot. You can get a good idea of how many types of plots you can do by going to http://docs.ggplot2.org/current. Almost any informative plot that you need to do as a researcher is possible with ggplot2. We will be using ggplot2 extensively in the book to help understand our data and our models as well as communicate our results. We use this code to get the data ready for the rest of the chapter. We use this code for the data for each chapter hereafter. ## Run but not shown ## Getting data ready for the examples library(foreign) library(tidyverse) library(furniture) library(anteo) dem_df &lt;- read.xport(&quot;~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_demographics_11.xpt&quot;) med_df &lt;- read.xport(&quot;~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_MedHeath_11.xpt&quot;) men_df &lt;- read.xport(&quot;~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_MentHealth_11.xpt&quot;) act_df &lt;- read.xport(&quot;~/Box Sync/GitHub/blog_rstats/assets/Data/NHANES_PhysActivity_11.xpt&quot;) names(dem_df) &lt;- tolower(names(dem_df)) names(med_df) &lt;- tolower(names(med_df)) names(men_df) &lt;- tolower(names(men_df)) names(act_df) &lt;- tolower(names(act_df)) df &lt;- dem_df %&gt;% full_join(med_df, by=&quot;seqn&quot;) %&gt;% full_join(men_df, by=&quot;seqn&quot;) %&gt;% full_join(act_df, by=&quot;seqn&quot;) %&gt;% mutate(dmdmartl = washer(dmdmartl, 77,99)) %&gt;% mutate(dmdmartl = factor(dmdmartl, labels = c(&quot;Married&quot;, &quot;Widowed&quot;, &quot;Divorced&quot;, &quot;Separated&quot;, &quot;Never Married&quot;, &quot;Partner&quot;))) %&gt;% mutate(dmdfmsiz = washer(dmdfmsiz, 7, 9)) %&gt;% filter(complete.cases(dmdmartl)) It is called “table1” because a nice descriptive table is often found in the first table of many academic papers.↩ tysonstanley.github.io↩ This is just scratching the surface of what we can change in the plots.↩ "],
["chapter-4-basic-analyses.html", "Chapter 4: Basic Analyses ANOVA Linear Modeling When Assumptions Fail Interactions", " Chapter 4: Basic Analyses In this chapter we are going to demonstrate basic modeling in R. Lucky for us, R is built for these analyses. It is actually quite straight-forward to run these types of models and analyze the output. Not only that, but there are simple ways to compare models. We will go through the ANOVA family of analyses, the linear regression models, and look at diagnostics of each. ANOVA ANOVA stands for analysis of variance. It is a family of methods (e.g. ANCOVA, MANOVA) that all share the fact that they compare a continuous dependent variable by a grouping factor variable (and may have multiple outcomes or other covariates). \\[ Y_i = \\alpha_0 + \\alpha_1 \\text{Group}_i + e_i \\] Since the groups are compared using “effect coding,” the \\(\\alpha_0\\) is the grand mean and each of the group level means are compared to it. To run an ANOVA model, you can simply use the aov function. In the example below, we are analyzing whether family size (although not fully continuous it is still useful for the example) differs by race. df$race &lt;- factor(df$ridreth1, labels=c(&quot;MexicanAmerican&quot;, &quot;OtherHispanic&quot;, &quot;White&quot;, &quot;Black&quot;, &quot;Other&quot;)) df$famsize &lt;- as.numeric(df$dmdfmsiz) fit &lt;- aov(famsize ~ race, df) anova(fit) ## Analysis of Variance Table ## ## Response: famsize ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## race 4 417 104.3 49.8 &lt;2e-16 *** ## Residuals 5308 11121 2.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We make sure the variables are the right type, then we use the aov function. Inside of the function we have what is called a formula. It has the general structure: leftside ~ rightside. Generally, the left side is an outcome variable and the right side is the predictor (i.e. independent) variable. Here, we have race predicting famsize. We assign the model to the name fit which is a common way of denoting it is a model. Finally, we use the anova function to output a nice ANOVA table. In the output we see the normal ANOVA table and we can see the p-value (Pr(&gt;F)) is very, very small and thus is quite significant. We can look at how the groups relate using a box plot. We will be using some of the practice you got in Chapter 3 using ggplot2 for this. library(ggplot2) ggplot(df, aes(x=race, y=famsize)) + geom_boxplot(aes(color=race)) + scale_color_manual(guide=FALSE, values=c(&quot;dodgerblue3&quot;, &quot;coral2&quot;, &quot;chartreuse4&quot;, &quot;darkorchid&quot;, &quot;firebrick2&quot;)) + theme_bw() ## Warning: Removed 240 rows containing non-finite values (stat_boxplot). This immediately gives us an idea of where some differences may be occuring. It would appear that “White” and “MexicanAmerican” groups are different in family size. Assumptions We also would like to make sure the assumptions look like they are being met. In ANOVA, we want the residuals to be distributed normally, the variance of each group should be approximately the same, the groups are assumed to be randomly assigned, and the sample should be randomly selected as well. In R we can get some simple graphical checks using plot. All we provide is our ANOVA object (here it is fit). The line before it par(mfrow=c(1,2)) tells R to have two plots per row (the 1 means one row, 2 means two columns). par(mfrow=c(1,2)) plot(fit) Here, it looks like we have a problem with normality (see the Normal Q-Q plot). Those dots should approximately follow the dotted line, which is not the case. In the first plot (Residuals vs. Fitted) suggests we have approximate homoskedasticity. Linear Modeling Linear regression is nearly identical to ANOVA. In fact, a linear regression with a continuous outcome and categorical predictor is exactly the same (if we use effect coding). For example, if we run the same model but with the linear regression function lm we get the same ANOVA table. fit2 &lt;- lm(famsize ~ race, data=df) anova(fit2) ## Analysis of Variance Table ## ## Response: famsize ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## race 4 417 104.3 49.8 &lt;2e-16 *** ## Residuals 5308 11121 2.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Surprise! It is the same as before. Here we can also use the summary function and we get the coefficients in the model as well (using dummy coding). The first level of the categorical variable is the reference group (the group that the others are compared to). We also get the intercept (in this case, the average value of the reference group). summary(fit2) ## ## Call: ## lm(formula = famsize ~ race, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.353 -1.445 -0.445 1.071 3.555 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.3534 0.0660 50.81 &lt; 2e-16 *** ## raceOtherHispanic -0.4249 0.0905 -4.69 2.8e-06 *** ## raceWhite -0.9079 0.0735 -12.35 &lt; 2e-16 *** ## raceBlack -0.6880 0.0766 -8.98 &lt; 2e-16 *** ## raceOther -0.4079 0.0818 -4.99 6.3e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.45 on 5308 degrees of freedom ## (240 observations deleted due to missingness) ## Multiple R-squared: 0.0362, Adjusted R-squared: 0.0354 ## F-statistic: 49.8 on 4 and 5308 DF, p-value: &lt;2e-16 Assumptions Linear regression has a few important assumptions, often called “Gauss-Markov Assumptions”. These include: The model is linear in parameters. Homoskedasticity (i.e. the variance of the residual is roughly uniform across the values of the independents). Normality of residuals. Numbers 2 and 3 are fairly easy to assess using the plot() function on the model object as we did with the ANOVA model. The linear in parameters suggests that the relationship between the outcome and independents is linear. par(mfrow=c(1,2)) plot(fit2) Comparing Models Often when running linear regression, we want to compare models and see if one fits significantly better than another. We also often want to present all the models in a table to let our readers compare the models. We will demonstrate both. Compare Statistically Using the anova() function, we can compare models statistically. anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: famsize ~ race ## Model 2: famsize ~ race ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 5308 11121 ## 2 5308 11121 0 0 The anova() function works with all sorts of modeling schemes and can help in model selection. Not surprisingly, when we compared the ANOVA and the simple linear model, they are exactly the same in overall model terms (the only difference is in how the cateogrical variable is coded—either effect coding in ANOVA or dummy coding in regression). For a more interesting comparison, lets run a new model with an additional variable and then make a comparison. fit3 = lm(famsize ~ race + dmdmartl, data=df) summary(fit3) ## ## Call: ## lm(formula = famsize ~ race + dmdmartl, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.728 -1.081 -0.328 0.901 4.294 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.7277 0.0649 57.43 &lt; 2e-16 *** ## raceOtherHispanic -0.3264 0.0854 -3.82 0.00013 *** ## raceWhite -0.8191 0.0697 -11.75 &lt; 2e-16 *** ## raceBlack -0.4444 0.0731 -6.08 1.3e-09 *** ## raceOther -0.4001 0.0776 -5.15 2.7e-07 *** ## dmdmartlWidowed -1.2024 0.0700 -17.18 &lt; 2e-16 *** ## dmdmartlDivorced -1.1841 0.0643 -18.41 &lt; 2e-16 *** ## dmdmartlSeparated -0.6244 0.1043 -5.98 2.3e-09 *** ## dmdmartlNever Married -0.9042 0.0492 -18.36 &lt; 2e-16 *** ## dmdmartlPartner -0.5234 0.0727 -7.20 6.7e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.36 on 5303 degrees of freedom ## (240 observations deleted due to missingness) ## Multiple R-squared: 0.146, Adjusted R-squared: 0.145 ## F-statistic: 101 on 9 and 5303 DF, p-value: &lt;2e-16 Notice that the variable is associated with the outcome according to the t-test seen in the summary. So we would expect that fit3 is better than fit2 at explaining the outcome, which we see in the output below. anova(fit2, fit3) ## Analysis of Variance Table ## ## Model 1: famsize ~ race ## Model 2: famsize ~ race + dmdmartl ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 5308 11121 ## 2 5303 9850 5 1271 137 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Compare in a Table We can also compare the models in a well-formatted table that makes many aspects easy to compare. Two main packages allow us to compare models: stargazer texreg Both provide simple functions to compare multiple models. For example, stargazer provides: library(stargazer) stargazer(fit2, fit3, type = &quot;text&quot;) ## ## ======================================================================== ## Dependent variable: ## -------------------------------------------------- ## famsize ## (1) (2) ## ------------------------------------------------------------------------ ## raceOtherHispanic -0.425*** -0.326*** ## (0.091) (0.085) ## ## raceWhite -0.908*** -0.819*** ## (0.074) (0.070) ## ## raceBlack -0.688*** -0.444*** ## (0.077) (0.073) ## ## raceOther -0.408*** -0.400*** ## (0.082) (0.078) ## ## dmdmartlWidowed -1.200*** ## (0.070) ## ## dmdmartlDivorced -1.180*** ## (0.064) ## ## dmdmartlSeparated -0.624*** ## (0.104) ## ## dmdmartlNever Married -0.904*** ## (0.049) ## ## dmdmartlPartner -0.523*** ## (0.073) ## ## Constant 3.350*** 3.730*** ## (0.066) (0.065) ## ## ------------------------------------------------------------------------ ## Observations 5,313 5,313 ## R2 0.036 0.146 ## Adjusted R2 0.035 0.145 ## Residual Std. Error 1.450 (df = 5308) 1.360 (df = 5303) ## F Statistic 49.800*** (df = 4; 5308) 101.000*** (df = 9; 5303) ## ======================================================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 When Assumptions Fail There are many things we can try when our assumptions fail. In my opinion, the best and most interpretable way is to use a Generalized Linear Model (GLM) which is discussed in the next chapter. There are a few other things you can try which I’ll show here. But, keep in mind that these things can cause other problems. For example, to fix normality we may accidentally cause heteroskedasticity. With that in mind, here are some common methods to help a model fit better. Log-Linear, Log-Log, Linear-Log, Other Sounds like a great tongue-twister? Well, it is but it’s also three ways of specifying (i.e. deciding what is in) your model better. Log-Linear is where we adjust the outcome variable by a natural log transformation. This is done easily in R: df$log_outcome &lt;- log(df$outcome) lm(log_outcome ~ var1, data=df) Log-Log is where we adjust both the outcome and the predictor variable with a log transformation. This is also easily done: df$log_outcome &lt;- log(df$outcome) df$log_var1 &lt;- log(df$var1) lm(log_outcome ~ log_var1, data=df) Linear-Log is where we adjsut just the predictor variable with a log transformation. And, you guessed it, this is easily done in R: df$log_var1 &lt;- log(df$var1) lm(outcome ~ log_var1 + var2, data=df) Other methods such as square rooting the outcome or using some power function (e.g. square, cube) are also quite common. There are functions that look for the best transformation to use. However, I will not cover it here since I think GLM’s are better. So if you want to learn about other ways to help your linear model go to the next chapter. Interactions Many times hypotheses dealing with human beings include interactions between effects. Interactions are when the effect of one variable depends on another variable. For example, the effect of marital status on family size may depend on whether the individual is a minority. In fact, this is the hypothesis we’ll test below. Including interactions in ANOVA and regression type models are very simple in R. Since interpretations of interaction effects are often best through plots, we will also show simple methods to visualize the interactions as well. Interactions in ANOVA In general, we refer to ANOVA’s with interactions as “2-way Factorial ANOVA’s”. We interact race and marriage status in this ANOVA. For simplicity, we created a binary race variable called minority using the ifelse() function. We explain this in more depth in Chapter 5. df$minority &lt;- factor(ifelse(df$race == &quot;White&quot;, 0, 1), labels = c(&quot;White&quot;, &quot;Minority&quot;)) fit_anova &lt;- aov(famsize ~ minority*dmdmartl, df) anova(fit_anova) ## Analysis of Variance Table ## ## Response: famsize ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## minority 1 241 240.6 128.79 &lt;2e-16 *** ## dmdmartl 5 1376 275.1 147.30 &lt;2e-16 *** ## minority:dmdmartl 5 21 4.2 2.24 0.048 * ## Residuals 5301 9901 1.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice two things: First, the interaction is significant (p = .003). This is important since we are going to try to interpret this interaction. Second, by including minority*dmdmartl we get both the main effects and the interaction. This is very important for interpretation purposes so you can thank R for making it a bit more easy on you. We can check the assumptions the same way as before: par(mfrow=c(1,2)) plot(fit_anova) Again, the assumptions are not met for this model. But, if we ignore that for now, we can quickly find a way to interpret the interaction. We first create a new data set that is composed of every possible combination of the variables in the model. This allows us to get unbiased estimates for the plotting. newdata &lt;- expand.grid(minority = levels(df$minority), dmdmartl = levels(df$dmdmartl)) newdata$preds &lt;- predict(fit_anova, newdata=newdata) We now use ggplot2 just as before. ggplot(newdata, aes(x = dmdmartl, y = preds, group = minority)) + geom_line(aes(color = minority)) + geom_point(aes(color = minority)) + labs(y = &quot;Predicted Family Size&quot;, x = &quot;Marital Status&quot;) + scale_color_manual(name = &quot;&quot;, values = c(&quot;dodgerblue3&quot;, &quot;chartreuse3&quot;)) + theme_anteo_wh() ## from anteo package The plot tells use a handful of things. For example, we see minorities generally have more children across marital statuses. However, the difference is smaller for married and divorced individuals compared to widowed, separated, never married, and living with a partner. There’s certainly more to gleen from the plot, but we won’t waste your time. Interactions in Linear Regression Interactions in linear regression is nearly identical as in ANOVA, except we use dummy coding. It provides a bit more information. For example, we get the coefficients from the linear regression whereas the ANOVA does not provide this. We can run a regression model via: fit_reg &lt;- lm(famsize ~ minority*dmdmartl, df) summary(fit_reg) ## ## Call: ## lm(formula = famsize ~ minority * dmdmartl, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.347 -1.156 -0.347 0.844 4.431 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 2.9770 0.0423 70.31 ## minorityMinority 0.3704 0.0549 6.74 ## dmdmartlWidowed -1.4077 0.1066 -13.20 ## dmdmartlDivorced -1.2339 0.0958 -12.88 ## dmdmartlSeparated -0.6627 0.2349 -2.82 ## dmdmartlNever Married -1.1395 0.0873 -13.05 ## dmdmartlPartner -0.5705 0.1177 -4.85 ## minorityMinority:dmdmartlWidowed 0.3133 0.1409 2.22 ## minorityMinority:dmdmartlDivorced 0.0422 0.1288 0.33 ## minorityMinority:dmdmartlSeparated 0.0836 0.2622 0.32 ## minorityMinority:dmdmartlNever Married 0.2988 0.1055 2.83 ## minorityMinority:dmdmartlPartner 0.1305 0.1493 0.87 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## minorityMinority 1.7e-11 *** ## dmdmartlWidowed &lt; 2e-16 *** ## dmdmartlDivorced &lt; 2e-16 *** ## dmdmartlSeparated 0.0048 ** ## dmdmartlNever Married &lt; 2e-16 *** ## dmdmartlPartner 1.3e-06 *** ## minorityMinority:dmdmartlWidowed 0.0263 * ## minorityMinority:dmdmartlDivorced 0.7433 ## minorityMinority:dmdmartlSeparated 0.7500 ## minorityMinority:dmdmartlNever Married 0.0046 ** ## minorityMinority:dmdmartlPartner 0.3820 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.37 on 5301 degrees of freedom ## (240 observations deleted due to missingness) ## Multiple R-squared: 0.142, Adjusted R-squared: 0.14 ## F-statistic: 79.7 on 11 and 5301 DF, p-value: &lt;2e-16 We used summary() to see the coefficients. If we used anova() it would have been the same as the one for the ANOVA. We can use the exact same methods here as we did with the ANOVA, including checking assumptions, creating a new data set, and using ggplot2 to check the interaction. We won’t repeat it here so you can move on to Chapter 5. "],
["chapter-5-generalized-linear-models.html", "Chapter 5: Generalized Linear Models Logistic Regression Poisson Regression Beta Regression", " Chapter 5: Generalized Linear Models Generalized Linear Models (GLM’s) are extensions of linear regression to areas where assumptions of normality and homoskedasticity do not hold. There are several versions of GLM’s, each for different types and distributions of outcomes. We are going to go through several of the most common. This chapter is to introduce the method very briefly and demonstrate how to perform one in R. We do not delve into the details of each method much, but rather focus on showing the quirks of the coding. We discuss: Logistic Regression Poisson Regression GLM with Gamma distribution Negative binomial Beta Regression Logistic Regression For binary outcomes (e.g., yes or no, correct or incorrect, sick or healthy), logistic regression is a fantastic tool that provides useful and interpretable information. Much like simple and multiple linear regression, logistic regression15 uses dummy coding and provides coefficients that tell us the relationship between the outcome and the independent variables. Since the outcome is binary, we use a statistical transformation to make things work well. This makes it so the outcome is in “log-odds.” A simple exponentiation of the coefficients and we get very useful “odds ratios.” These are very common in many fields using binary data. Luckily, running a logistic regression is simple in R. We first create the binary outcome variable called dep. We use a new function called mutate to create a new variable (we could do this a number of ways but this is probably the cleanest way). ## First creating binary depression variable df &lt;- df %&gt;% mutate(dep = dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090) %&gt;% mutate(dep2 = ifelse(dep &gt;= 10, 1, ifelse(dep &lt; 10, 0, NA))) Note that we added the values from the ten variables that give us an overall depression score (dep). We then use ifelse() to create a binary version of depression called dep2 with a cutoff of \\(\\geq 16\\) meaning depressed. Because there are missing values denoted as “NA” in this variable, we use a “nested ifelse” to say: IF depression \\(\\geq 10\\) then dep2 is 1, IF dpression \\(&lt; 10\\), then dep2 is 0, ELSE dep2 is NA. Note that these nested ifelse() statements can be as long as you want. We further need to clean up the asthma and sedentary variables. ## Fix some placeholders df &lt;- df %&gt;% mutate(asthma = washer(mcq010, 9), asthma = washer(asthma, 2, value = 0)) %&gt;% mutate(sed = washer(pad680, 9999, 7777)) Now let’s run the logistic regression: l_fit &lt;- glm(dep2 ~ asthma + sed + race + famsize, data = df, family = &quot;binomial&quot;) summary(l_fit) ## ## Call: ## glm(formula = dep2 ~ asthma + sed + race + famsize, family = &quot;binomial&quot;, ## data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.781 -0.449 -0.408 -0.366 2.538 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.616404 0.237915 -11.00 &lt; 2e-16 *** ## asthma 0.564068 0.127550 4.42 9.8e-06 *** ## sed 0.000569 0.000261 2.18 0.0291 * ## raceOtherHispanic 0.711974 0.232854 3.06 0.0022 ** ## raceWhite 0.127317 0.211625 0.60 0.5474 ## raceBlack 0.016217 0.220530 0.07 0.9414 ## raceOther -0.464466 0.255621 -1.82 0.0692 . ## famsize -0.033267 0.037275 -0.89 0.3721 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2711.6 on 4440 degrees of freedom ## Residual deviance: 2654.9 on 4433 degrees of freedom ## (1119 observations deleted due to missingness) ## AIC: 2671 ## ## Number of Fisher Scoring iterations: 5 We used glm() (stands for generalized linear model). The key to making it logistic, since you can use glm() for a linear model using maximum likelihood instead of lm() with least squares, is family = &quot;binomial&quot;. This tells R to do a logistic regression. Poisson Regression As we did in logistic regression, we will use the glm() function. The difference here is we will be using an outcome that is a count variable. For example, the sedentary variable (sed) that we have in df is a count of the minutes of sedentary activity. p_fit &lt;- glm(sed ~ asthma + race + famsize, data = df, family = &quot;poisson&quot;) summary(p_fit) ## ## Call: ## glm(formula = sed ~ asthma + race + famsize, family = &quot;poisson&quot;, ## data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -27.40 -8.52 -1.54 6.01 36.95 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.663256 0.003250 1742.5 &lt;2e-16 *** ## asthma 0.065090 0.001984 32.8 &lt;2e-16 *** ## raceOtherHispanic 0.128639 0.003711 34.7 &lt;2e-16 *** ## raceWhite 0.349709 0.003048 114.8 &lt;2e-16 *** ## raceBlack 0.334284 0.003134 106.7 &lt;2e-16 *** ## raceOther 0.340376 0.003279 103.8 &lt;2e-16 *** ## famsize -0.023425 0.000501 -46.8 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 599886 on 5287 degrees of freedom ## Residual deviance: 573124 on 5281 degrees of freedom ## (272 observations deleted due to missingness) ## AIC: 613079 ## ## Number of Fisher Scoring iterations: 5 Sedentary may be over-dispersed (see plot) and so other methods related to poisson may be necessary. For this book, we are not going to be delving into these in depth but we will introduce some below. Gamma Regression with a gamma distribution are often found when analyzing costs in dollars. It is very similar to poisson but does not require integers and can handle more dispersion. However, the outcome must have values \\(&gt; 0\\). Just for demonstration: ## Adjust sed df$sed_gamma &lt;- df$sed + .01 g_fit &lt;- glm(sed_gamma ~ asthma + race + famsize, data = df, family = &quot;Gamma&quot;) summary(g_fit) ## ## Call: ## glm(formula = sed_gamma ~ asthma + race + famsize, family = &quot;Gamma&quot;, ## data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.355 -0.470 -0.078 0.304 1.683 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.53e-03 1.04e-04 33.88 &lt; 2e-16 *** ## asthma -1.70e-04 5.48e-05 -3.11 0.00188 ** ## raceOtherHispanic -4.57e-04 1.20e-04 -3.81 0.00014 *** ## raceWhite -1.10e-03 9.90e-05 -11.07 &lt; 2e-16 *** ## raceBlack -1.06e-03 1.01e-04 -10.44 &lt; 2e-16 *** ## raceOther -1.07e-03 1.05e-04 -10.25 &lt; 2e-16 *** ## famsize 6.38e-05 1.43e-05 4.45 8.8e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.298) ## ## Null deviance: 2030.7 on 5287 degrees of freedom ## Residual deviance: 1953.0 on 5281 degrees of freedom ## (272 observations deleted due to missingness) ## AIC: 70512 ## ## Number of Fisher Scoring iterations: 5 Two-Part or Hurdle Models We are going to use the pscl package to run a hurdle model. These models are built for situations where there is a count variable with many zeros (“zero-inflated”). The hurdle model makes slightly different assumptions regarding the zeros than the pure negative binomial that we present next. The hurdle consists of two models: one for whether the person had a zero or more (binomial) and if more than zero, how many (poisson). To run a hurdle model, we are going to make a sedentary variable with many more zeros to illustrate and then we will run a hurdle model. ## Zero inflated sedentary (don&#39;t worry too much about the specifics) df$sed_zero &lt;- ifelse(sample(1:100, size = length(df$sed), replace=TRUE) %in% c(5,10,11,20:25), 0, df$sed) ## Hurdle model library(pscl) h_fit = hurdle(sed_zero ~ asthma + race + famsize, data = df) summary(h_fit) ## ## Call: ## hurdle(formula = sed_zero ~ asthma + race + famsize, data = df) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -4.166 -1.478 -0.234 1.286 10.156 ## ## Count model coefficients (truncated poisson with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.643569 0.003427 1646.9 &lt;2e-16 *** ## asthma 0.073746 0.002065 35.7 &lt;2e-16 *** ## raceOtherHispanic 0.141212 0.003873 36.5 &lt;2e-16 *** ## raceWhite 0.361388 0.003211 112.5 &lt;2e-16 *** ## raceBlack 0.348239 0.003300 105.5 &lt;2e-16 *** ## raceOther 0.357565 0.003451 103.6 &lt;2e-16 *** ## famsize -0.020817 0.000525 -39.7 &lt;2e-16 *** ## Zero hurdle model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.24403 0.19394 11.57 &lt;2e-16 *** ## asthma 0.08254 0.14009 0.59 0.556 ## raceOtherHispanic 0.51784 0.24394 2.12 0.034 * ## raceWhite -0.02513 0.17855 -0.14 0.888 ## raceBlack -0.00138 0.18530 -0.01 0.994 ## raceOther -0.01556 0.19585 -0.08 0.937 ## famsize 0.01173 0.03338 0.35 0.725 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Number of iterations in BFGS optimization: 12 ## Log-likelihood: -2.81e+05 on 14 Df Notice that the output has two parts: “Count model coefficients (truncated poisson with log link):” and “Zero hurdle model coefficients (binomial with logit link):”. Together they tell us about the relationship between the predictors and a count variable with many zeros. Negative Binomial Similar to that above, negative binomial is for zero-inflated count variables. It makes slightly different assumptions than the hurdle and doesn’t use a two-part approach. In order to run a negative binomial model we’ll use the MASS package and the glm.nb() function. library(MASS) fit_nb &lt;- glm.nb(sed_zero ~ asthma + race + famsize, data = df) summary(fit_nb) Note that this model is not really appropriate because our data is somewhat contrived. Beta Regression For outcomes that are bound between a lower and upper bound, Beta Regression is a great method. For example, if we are looking at test scores that are bound between 0 and 100. It is a very flexible method and allows for some extra analysis regarding the variation. For this, we are going to use the betareg package. But first, we are going to reach a little and create a ficticiously bound variable in the data set. ## Variable bound between 0 and 1 df$beta_var &lt;- sample(seq(.05, .99, by = .01), size = length(df$asthma), replace = TRUE) library(betareg) fit_beta &lt;- betareg(beta_var ~ asthma + race + famsize, data = df) summary(fit_beta) ## ## Call: ## betareg(formula = beta_var ~ asthma + race + famsize, data = df) ## ## Standardized weighted residuals 2: ## Min 1Q Median 3Q Max ## -2.048 -0.703 -0.068 0.624 2.950 ## ## Coefficients (mean model with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.07627 0.05754 1.33 0.18 ## asthma 0.02492 0.03882 0.64 0.52 ## raceOtherHispanic 0.06812 0.06475 1.05 0.29 ## raceWhite 0.03067 0.05327 0.58 0.56 ## raceBlack 0.03647 0.05515 0.66 0.51 ## raceOther 0.09246 0.05849 1.58 0.11 ## famsize -0.00832 0.00978 -0.85 0.40 ## ## Phi coefficients (precision model with identity link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (phi) 2.513 0.042 59.9 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Type of estimator: ML (maximum likelihood) ## Log-likelihood: 108 on 8 Df ## Pseudo R-squared: 0.000886 ## Number of iterations: 17 (BFGS) + 2 (Fisher scoring) The output provides coefficients and the “Phi” coefficients. Both are important parts of using beta regression but we are not going to discuss it here. There are many resources available to learn more about beta regression and each of these GLM’s. As for now, we are going to move on to more complex modeling where there are clustering or repeated measures in the data. Technically, logistic regression is a linear regression model.↩ "],
["chapter-6-multilevel-modeling.html", "Chapter 6: Multilevel Modeling GEE Mixed Effects Conclusions", " Chapter 6: Multilevel Modeling There are two general forms of multilevel modeling: Generalized Estimating Equations (GEE) Mixed effects (ME; i.e., hierarchical linear modeling, multilevel modeling) Several similarities and differences should be noted briefly. As for similarities, they both attempt to control for the lack of independence within clusters, although they do it in different ways. Also, they are both built on linear regression which makes them flexible and powerful at finding relationships in the data. The differences are subtle but important. First, the interpretation is somewhat different between the two. GEE is a population-averaged (e.g., marginal) model whereas ME is subject specific. In other words, GEE is the average effect while ME is the effect found in the average person. In a linear model, these coefficients are the same but when we use different forms such as logistic or poisson, these can be quite different (although in my experience they generally tell a similar story). Second, ME models are much more complex than the GEE models and can struggle with convergence compared to the GEE. This also means that GEE’s are generally fitted much more quickly. Still the choice of the modeling technique should be driven by your hypotheses and not totally dependent on speed of the computation. First, if we needed to, we’d reshape our data so that it is ready for the analyses. For both modeling techniques we want our data in long form16. What this implies is that each row is an observation. What this actually means about the data depends on the data. For example, if you have repeated measures, then often data is stored in wide form—a row is an individual. To make this long, we want each time point within a person to be a row—a single individual can have multiple rows but each row is a unique observation. Currently, our data is in long form since we are working within community clusters within this data. So, each row is an observation and each cluster has multiple rows. Note that although these analyses will be within community clusters instead of within subjects (i.e. repeated measures), the overall steps will be the exact same. This chapter certainly does not cover all of multilevel modeling in R. Entire books are dedicated to that single subject. Rather, we are introducing the methods and the packages that can be used to start using these methods. GEE There are two packages, intimately related, that allow us to perform GEE modeling—gee and geepack. These have some great features and make running a fairly complex model pretty simple. However, as great as they are, there are some annoying shortcomings. We’ll get to a few of them throughout this section. GEE’s, in general, want a few pieces of information from you. First, the outcome and predictors. This is just as in linear regression and GLM’s. Second, we need to provide a correlation structure. This tells the model the approximate pattern of correlations between the time points or clusters. It also wants a variable that tells the cluster ID’s. Finally, it also wants the family (i.e. the type of distribution). Since this is not longitudinal, but rather clustered within communities, we’ll assume for this analysis an unstructured correlation structure. It is the most flexible and we have enough power for it here. For geepack to work, we need to filter out the missing values for the variables that will be in the model. df2 &lt;- df %&gt;% filter(complete.cases(dep, famsize, sed, race, asthma)) Now, we’ll build the model with both packages (just for demonstration). We predict depression with asthma, family size, minutes of sedentary behavior, and the subject’s race. library(gee) fit_gee &lt;- gee(dep ~ asthma + famsize + sed + race, data = df2, id = df2$sdmvstra, corstr = &quot;unstructured&quot;) ## (Intercept) asthma famsize sed ## 2.49981 1.35080 -0.04252 0.00137 ## raceOtherHispanic raceWhite raceBlack raceOther ## 1.17758 0.11300 0.09553 -0.54310 summary(fit_gee)$coef ## Estimate Naive S.E. Naive z Robust S.E. Robust z ## (Intercept) 2.49512 0.286751 8.701 0.268956 9.277 ## asthma 1.34822 0.186602 7.225 0.213515 6.314 ## famsize -0.03985 0.046169 -0.863 0.045731 -0.871 ## sed 0.00137 0.000336 4.062 0.000355 3.847 ## raceOtherHispanic 1.18503 0.307467 3.854 0.330601 3.584 ## raceWhite 0.11520 0.253204 0.455 0.227971 0.505 ## raceBlack 0.09216 0.262588 0.351 0.235979 0.391 ## raceOther -0.54271 0.280925 -1.932 0.240890 -2.253 library(geepack) fit_geeglm &lt;- geeglm(dep ~ asthma + famsize + sed + race, data = df2, id = df2$sdmvstra, corstr = &quot;unstructured&quot;) summary(fit_geeglm) ## ## Call: ## geeglm(formula = dep ~ asthma + famsize + sed + race, data = df2, ## id = df2$sdmvstra, corstr = &quot;unstructured&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) 2.557345 0.269972 89.73 &lt; 2e-16 *** ## asthma 1.344424 0.215328 38.98 4.3e-10 *** ## famsize -0.045007 0.045691 0.97 0.32461 ## sed 0.001309 0.000355 13.63 0.00022 *** ## raceOtherHispanic 1.167554 0.331537 12.40 0.00043 *** ## raceWhite 0.079791 0.229559 0.12 0.72815 ## raceBlack 0.059616 0.236249 0.06 0.80077 ## raceOther -0.577781 0.241566 5.72 0.01677 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Estimated Scale Parameters: ## Estimate Std.err ## (Intercept) 19.5 0.784 ## ## Correlation: Structure = unstructured Link = identity ## ## Estimated Correlation Parameters: ## Estimate Std.err ## alpha.1:2 0.1251 0.0165 ## alpha.1:3 0.4198 0.1032 ## alpha.1:4 2.8985 1.0676 ## alpha.1:5 -1.8480 0.2025 ## alpha.2:3 0.1222 0.0633 ## alpha.2:4 -0.0892 0.2021 ## alpha.2:5 0.2043 0.0371 ## alpha.3:4 -0.4963 0.1121 ## alpha.3:5 0.2493 0.0387 ## alpha.4:5 -0.6676 0.0875 ## Number of clusters: 4112 Maximum cluster size: 5 The gee package doesn’t directly provide p-values but provides the z-scores, which can be used to find the p-values. The geepack provides the p-values in the way you’ll see in the lm() and glm() functions. These models are interpreted just as the regular GLM. It has adjusted for the correlations within the clusters and provides valid standard errors and p-values. Mixed Effects Mixed effects models require a bit more thinking about the effects. It is called “mixed effects” because we include both fixed and random effects into the model simultaneously. The random effects are those that we don’t necessarily care about the specific values but want to control for it and/or estimate the variance. The fixed effects are those we are used to estimating in linear models and GLM’s. These are a bit more clear with an example. We will do the same overall model as we did with the GEE but we’ll use ME. To do so, we’ll use the lme4 package. In the model below, we predict depression with asthma, family size, minutes of sedentary behavior, and the subject’s race. We have a random intercept (which allows the intercept to vary across clusters). library(lme4) fit_me &lt;- lmer(dep ~ asthma + famsize + sed + race + (1 | cluster), data = df2, REML = FALSE) summary(fit_me) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: dep ~ asthma + famsize + sed + race + (1 | cluster) ## Data: df2 ## ## AIC BIC logLik deviance df.resid ## 25805 25869 -12892 25785 4431 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.323 -0.636 -0.355 0.271 5.436 ## ## Random effects: ## Groups Name Variance Std.Dev. ## cluster (Intercept) 0.104 0.322 ## Residual 19.398 4.404 ## Number of obs: 4441, groups: cluster, 14 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 2.489894 0.302638 8.23 ## asthma 1.329994 0.186533 7.13 ## famsize -0.043237 0.046320 -0.93 ## sed 0.001433 0.000337 4.26 ## raceOtherHispanic 1.283057 0.320393 4.00 ## raceWhite 0.009298 0.259480 0.04 ## raceBlack 0.168175 0.273351 0.62 ## raceOther -0.540119 0.285494 -1.89 ## ## Correlation of Fixed Effects: ## (Intr) asthma famsiz sed rcOthH racWht rcBlck ## asthma -0.042 ## famsize -0.510 -0.003 ## sed -0.324 -0.044 0.051 ## rcOthrHspnc -0.557 -0.034 0.052 -0.037 ## raceWhite -0.680 -0.038 0.135 -0.148 0.640 ## raceBlack -0.644 -0.057 0.094 -0.131 0.625 0.776 ## raceOther -0.580 0.000 0.049 -0.135 0.590 0.725 0.694 You’ll see that there are no p-values provided here. This is because p-values are not well-defined in the ME framework. A good way to test it can be through the anova() function, comparing models. Let’s compare a model with and without asthma to see if the model is significantly better with it in. fit_me1 &lt;- lmer(dep ~ famsize + sed + race + (1 | cluster), data = df2, REML = FALSE) anova(fit_me, fit_me1) ## Data: df2 ## Models: ## fit_me1: dep ~ famsize + sed + race + (1 | cluster) ## fit_me: dep ~ asthma + famsize + sed + race + (1 | cluster) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## fit_me1 9 25853 25911 -12918 25835 ## fit_me 10 25805 25869 -12892 25785 50.5 1 1.2e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This comparison strongly suggests that asthma is a significant predictor (\\(\\chi^2 = 50.5\\), p &lt; .001). We can do this with both fixed and random effects, as below: fit_me2 &lt;- lmer(dep ~ famsize + sed + race + (1 | cluster), data = df2, REML = TRUE) fit_me3 &lt;- lmer(dep ~ famsize + sed + race + (1 + asthma | cluster), data = df2, REML = TRUE) anova(fit_me2, fit_me3, refit = FALSE) ## Data: df2 ## Models: ## fit_me2: dep ~ famsize + sed + race + (1 | cluster) ## fit_me3: dep ~ famsize + sed + race + (1 + asthma | cluster) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## fit_me2 9 25879 25937 -12931 25861 ## fit_me3 11 25847 25917 -12912 25825 36.9 2 9.8e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here, including random slopes for asthma appears to be significant (\\(\\chi^2 = 36.9\\), p &lt; .001). Linear mixed effects models converge pretty well. You’ll see that the conclusions and estimates are very similar to that of the GEE. For generalized versions of ME, the convergence can be harder and more picky. As we’ll see below, it complains about large eigenvalues and tells us to rescale some of the variables. library(lme4) fit_gme &lt;- glmer(dep2 ~ asthma + famsize + sed + race + (1 | cluster), data = df2, family = &quot;binomial&quot;) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control ## $checkConv, : Model failed to converge with max|grad| = 0.0118968 (tol = ## 0.001, component 1) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue ## - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio ## - Rescale variables? After a quick check, we can see that sed is huge compared to the other variables. If we simply rescale it, using the I() function within the model formula, we can rescale it by 1,000. Here, that is all it needed to converge. library(lme4) fit_gme &lt;- glmer(dep2 ~ asthma + famsize + I(sed/1000) + race + (1 | cluster), data = df2, family = &quot;binomial&quot;) summary(fit_gme) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: dep2 ~ asthma + famsize + I(sed/1000) + race + (1 | cluster) ## Data: df2 ## ## AIC BIC logLik deviance df.resid ## 2672 2729 -1327 2654 4432 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.632 -0.329 -0.295 -0.258 4.971 ## ## Random effects: ## Groups Name Variance Std.Dev. ## cluster (Intercept) 0.022 0.148 ## Number of obs: 4441, groups: cluster, 14 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.6282 0.2431 -10.81 &lt; 2e-16 *** ## asthma 0.5570 0.1280 4.35 1.4e-05 *** ## famsize -0.0349 0.0374 -0.93 0.3501 ## I(sed/1000) 0.5877 0.2614 2.25 0.0246 * ## raceOtherHispanic 0.7517 0.2419 3.11 0.0019 ** ## raceWhite 0.0965 0.2157 0.45 0.6546 ## raceBlack 0.0505 0.2276 0.22 0.8245 ## raceOther -0.4689 0.2576 -1.82 0.0687 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) asthma famsiz I(/100 rcOthH racWht rcBlck ## asthma -0.057 ## famsize -0.491 -0.011 ## I(sed/1000) -0.324 -0.041 0.031 ## rcOthrHspnc -0.655 -0.032 0.045 -0.028 ## raceWhite -0.716 -0.036 0.132 -0.148 0.709 ## raceBlack -0.685 -0.064 0.089 -0.123 0.715 0.780 ## raceOther -0.575 -0.003 0.047 -0.123 0.608 0.691 0.656 Conclusions This has been a really brief introduction into a thriving, large field of statistical analyses. These are the general methods for using R to analyze multilevel data. Our next chapter will discuss more modeling techniques in R, including mediation, mixture, and structural equation modeling. We discuss what this means in much more depth and demonstrate reshaping of data in Chapter 8. It is an important tool to understand if you are working with data in various forms. Although many reshape their data by copying-and-pasting in a spreadsheet, what we present in Chapter 8 is much more efficient, cleaner, less error-prone, and replicatable.↩ "],
["chapter-7-other-modeling-techniques.html", "Chapter 7: Other Modeling Techniques Mediation Modeling Structural Equation Modeling Machine Learning Techniques 0.1 Conclusions", " Chapter 7: Other Modeling Techniques In this chapter we cover, however briefly, mediation and moderation modeling, methods relating to structural equation modeling (SEM), and methods applicable to our field from machine learning. Although these machine learning may appear very different than mediation and SEM, they each have advantages that can help in different situations. For example, SEM is useful when we know there is a high degree of measurement error or our data has multiple indicators for each construct. On the other hand, regularized regression and random forests–two popular forms of machine learning–are great to explore patterns and relationships there are hundreds or thousands of variables that may predict an outcome. Mediation modeling, although often used within SEM, can help us understand pathways of effect from one variable to another. It is especially useful with moderating variables (i.e., variables that interact with another). So we’ll start with discussing mediation, then we’ll move on to SEM, followed by machine learning. Mediation Modeling Mediation modeling can be done via several packages. For now, we recommend using lavaan (stands for “latent variable analysis”)17. Although it is technically still a “beta” version, it performs very well especially for more simple models. It makes mediation modeling straightforward. Below, we model the following mediation model: \\[ depression = \\beta_0 + \\beta_1 asthma + \\epsilon_1 \\] \\[ time_{Sedentary} = \\lambda_0 + \\lambda_1 asthma + \\lambda_2 depression + \\epsilon_2 \\] In essence, we believe that asthma increases depression which in turn increases the amount of time spent being sedentary. library(lavaan) df$sed_hr = df$sed/60 ## in hours instead of minutes ## Our model model1 &lt;- &#39; dep ~ asthma sed_hr ~ dep + asthma &#39; ## sem function to run the model fit &lt;- sem(model1, data = df) summary(fit) ## lavaan (0.5-22) converged normally after 30 iterations ## ## Used Total ## Number of observations 4618 5560 ## ## Estimator ML ## Minimum Function Test Statistic 0.000 ## Degrees of freedom 0 ## Minimum Function Value 0.0000000000000 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## dep ~ ## asthma 1.474 0.183 8.064 0.000 ## sed_hr ~ ## dep 0.044 0.011 3.951 0.000 ## asthma 0.406 0.139 2.922 0.003 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .dep 19.602 0.408 48.052 0.000 ## .sed_hr 11.167 0.232 48.052 0.000 From the output we see asthma does predict depression and depression does predict time being sedentary. There is also a direct effect of asthma on sedentary behavior even after controlling for depression. We can further specify the model to have it give us the indirect effect and direct effects tested. ## Our model model2 &lt;- &#39; dep ~ a*asthma sed_hr ~ b*dep + c*asthma indirect := a*b total := c + a*b &#39; ## sem function to run the model fit2 &lt;- sem(model2, data = df) summary(fit2) ## lavaan (0.5-22) converged normally after 30 iterations ## ## Used Total ## Number of observations 4618 5560 ## ## Estimator ML ## Minimum Function Test Statistic 0.000 ## Degrees of freedom 0 ## Minimum Function Value 0.0000000000000 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## dep ~ ## asthma (a) 1.474 0.183 8.064 0.000 ## sed_hr ~ ## dep (b) 0.044 0.011 3.951 0.000 ## asthma (c) 0.406 0.139 2.922 0.003 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .dep 19.602 0.408 48.052 0.000 ## .sed_hr 11.167 0.232 48.052 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## indirect 0.065 0.018 3.548 0.000 ## total 0.471 0.138 3.406 0.001 We defined a few things in the model. First, we gave the coefficients labels of a, b, and c. Doing so allows us to define the indirect and total effects. Here we see the indirect effect, although small, is significant at \\(p &lt; .001\\). The total effect is larger (not surprising) and is also significant. Also note that we can make the regression equations have other covariates as well if we needed to (i.e. control for age or gender) just as we do in regular regression. In this final example, we show that you can do moderation as part of the mediation model. ## Our model model2.1 &lt;- &#39; dep ~ asthma + ridageyr + asthma:ridageyr sed_hr ~ dep + asthma + ridageyr + asthma:ridageyr &#39; ## sem function to run the model fit2.1 &lt;- sem(model2.1, data = df) summary(fit2.1) ## lavaan (0.5-22) converged normally after 46 iterations ## ## Used Total ## Number of observations 4618 5560 ## ## Estimator ML ## Minimum Function Test Statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## dep ~ ## asthma 0.909 0.514 1.769 0.077 ## ridageyr -0.007 0.004 -1.643 0.100 ## asthma:ridagyr 0.012 0.010 1.142 0.254 ## sed_hr ~ ## dep 0.044 0.011 3.933 0.000 ## asthma 0.067 0.388 0.174 0.862 ## ridageyr -0.001 0.003 -0.436 0.663 ## asthma:ridagyr 0.007 0.008 0.932 0.351 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .dep 19.589 0.408 48.052 0.000 ## .sed_hr 11.165 0.232 48.052 0.000 Structural Equation Modeling Instead of summing our depression variable, we can use SEM to run the mediation model from above but use the latent variable of depression instead. ## Our model model3 &lt;- &#39; dep1 =~ dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090 dep1 ~ a*asthma sed_hr ~ b*dep1 + c*asthma indirect := a*b total := c + a*b &#39; ## sem function to run the model fit3 &lt;- sem(model3, data = df) summary(fit3) ## lavaan (0.5-22) converged normally after 47 iterations ## ## Used Total ## Number of observations 4618 5560 ## ## Estimator ML ## Minimum Function Test Statistic 1060.130 ## Degrees of freedom 43 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## dep1 =~ ## dpq010 1.000 ## dpq020 1.095 0.024 45.195 0.000 ## dpq030 1.130 0.031 36.891 0.000 ## dpq040 1.146 0.030 38.056 0.000 ## dpq050 0.934 0.025 36.837 0.000 ## dpq060 0.929 0.022 42.150 0.000 ## dpq070 0.869 0.022 39.766 0.000 ## dpq080 0.687 0.019 36.370 0.000 ## dpq090 0.307 0.011 28.546 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## dep1 ~ ## asthma (a) 0.172 0.023 7.636 0.000 ## sed_hr ~ ## dep1 (b) 0.344 0.104 3.296 0.001 ## asthma (c) 0.411 0.139 2.955 0.003 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .dpq010 0.306 0.007 42.005 0.000 ## .dpq020 0.212 0.006 37.552 0.000 ## .dpq030 0.560 0.013 43.845 0.000 ## .dpq040 0.515 0.012 43.339 0.000 ## .dpq050 0.384 0.009 43.867 0.000 ## .dpq060 0.221 0.005 40.824 0.000 ## .dpq070 0.249 0.006 42.455 0.000 ## .dpq080 0.217 0.005 44.050 0.000 ## .dpq090 0.090 0.002 46.128 0.000 ## .sed_hr 11.174 0.233 48.033 0.000 ## .dep1 0.256 0.010 24.692 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## indirect 0.059 0.020 3.034 0.002 ## total 0.471 0.138 3.406 0.001 We defined dep1 as a latent variable using =~. The regression, indirect, and total results are very similar the summed score version. Although the model does not fit the data well–“P-value (Chi-square) = 0.000”–it is informative for demonstration. We would likely need to find out how the measurement model (dep1 =~ dpq010 + dpq020 + dpq030 +) actually fits before throwing it into a mediation model. We can do that via: model4 &lt;- &#39; dep1 =~ dpq010 + dpq020 + dpq030 + dpq040 + dpq050 + dpq060 + dpq070 + dpq080 + dpq090 &#39; fit4 &lt;- cfa(model4, data=df) summary(fit4) ## lavaan (0.5-22) converged normally after 28 iterations ## ## Used Total ## Number of observations 4639 5560 ## ## Estimator ML ## Minimum Function Test Statistic 989.323 ## Degrees of freedom 27 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## dep1 =~ ## dpq010 1.000 ## dpq020 1.095 0.024 45.418 0.000 ## dpq030 1.126 0.030 36.926 0.000 ## dpq040 1.142 0.030 38.105 0.000 ## dpq050 0.928 0.025 36.677 0.000 ## dpq060 0.930 0.022 42.317 0.000 ## dpq070 0.868 0.022 39.931 0.000 ## dpq080 0.682 0.019 36.387 0.000 ## dpq090 0.306 0.011 28.610 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .dpq010 0.306 0.007 42.069 0.000 ## .dpq020 0.211 0.006 37.483 0.000 ## .dpq030 0.562 0.013 43.962 0.000 ## .dpq040 0.516 0.012 43.453 0.000 ## .dpq050 0.391 0.009 44.061 0.000 ## .dpq060 0.222 0.005 40.863 0.000 ## .dpq070 0.249 0.006 42.508 0.000 ## .dpq080 0.217 0.005 44.173 0.000 ## .dpq090 0.090 0.002 46.231 0.000 ## dep1 0.261 0.011 24.788 0.000 As we can see, there is a lack of fit in the measurement model. It is possible that these depression questions could be measuring more than one factor. We could explore this using exploratory factor analysis. We don’t demonstrate that here, but know that it is possible to do in R with a few other packages. Machine Learning Techniques We are briefly going to introduce some machine learning techniques that may be of interest to researchers. We will quickly introduce and demonstrate: Ridge, Lasso and Elastic Net Random Forests Ridge, Lasso and Elastic Net In order to do either ridge, lasso, or elastic net regression, we can use the fantastic glmnet package. Using the cv.glmnet() function we can run the ridge (\\(alpha = 0\\)), lasso (\\(alpha = 1\\) which is default), and elastic net (\\(0 \\leq alpha \\leq 1\\)). Lasso and elastic net can do variable selection in addition to the estimates. Ridge is great at handling correlated predictors. Each of them are better than conventional methods at prediction and each of them can handle large numbers of predictors. To learn more see “Introduction to Statistical Learning” by Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie. A feww PDF is available on their website. To use the package, it wants the data in a very specific form. First, we need to remove any missingness. We use na.omit() to do this. We take all the predictors (without the outcome) and put it in a data matrix object. We only include a few for the demonstration but you can include many predictors. We name ours X. Y is our outcome. df2 &lt;- df %&gt;% dplyr::select(riagendr, ridageyr, ridreth3, race, famsize, dep, asthma, sed_hr) %&gt;% na.omit X &lt;- df2 %&gt;% dplyr::select(-sed_hr) %&gt;% data.matrix Y &lt;- df2$sed_hr Then we use the cv.glmnet() function to fit the different models. The “cv” refers to cross-validation18, which we don’t discuss here, but it an important topic to become familiar with. Below we fit a ridge, a lasso, and an elastic net model. library(glmnet) fit_ridge &lt;- cv.glmnet(X, Y, alpha = 0) fit_lasso &lt;- cv.glmnet(X, Y, alpha = 1) fit_enet &lt;- cv.glmnet(X, Y, alpha = .8) The plots below show where appropriate lambda values are based on the mean squared error of the cross-validated prediction. The vertical dashed lines show a reasonable range of lambda values that can be used. par(mfrow = c(2,2)) ## put plots together on 2 x 2 grid plot(fit_ridge) plot(fit_lasso) plot(fit_enet) We can get the coefficients at a reasonable lambda. Specifically, we use the “1-SE rule” (near the right hand side vertical dashed lines in the above plots). coef(fit_ridge, s = &quot;lambda.1se&quot;) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 5.821033 ## riagendr 0.031077 ## ridageyr -0.000392 ## ridreth3 0.038893 ## race 0.063137 ## famsize -0.027690 ## dep 0.007346 ## asthma 0.070334 coef(fit_lasso, s = &quot;lambda.1se&quot;) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 5.35046 ## riagendr . ## ridageyr . ## ridreth3 . ## race 0.26366 ## famsize -0.03152 ## dep 0.00473 ## asthma . coef(fit_enet, s = &quot;lambda.1se&quot;) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 5.32733 ## riagendr . ## ridageyr . ## ridreth3 . ## race 0.27554 ## famsize -0.04213 ## dep 0.00854 ## asthma . Although we briefly introduce these regression methods, they are indeed very important. We highly recommend learning more about them. Random Forests Random forests is another machine learning method that can do fantastic prediction. It is built in a very different way than the methods we have discussed up to this point. It is not built on a linear modeling scheme; rather, it is built on classification and regression trees. Again, “Introduction to Statistical Learning” is a great resource to learn more. Conveniently, we can use the randomForest package. We specify the model by the formula sed_hr ~ . which means we want sed_hr to be the outcome and all the rest of the variables to be predictors. library(randomForest) fit_rf &lt;- randomForest(sed_hr ~ ., data = df2) fit_rf ## ## Call: ## randomForest(formula = sed_hr ~ ., data = df2) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## Mean of squared residuals: 10.8 ## % Var explained: 3.78 We can find out which variables were important in the model via: par(mfrow=c(1,1)) ## back to one plot per page varImpPlot(fit_rf) We can see that age (ridageyr) is the most important variable, depression (dep) follows, with the family size (famsize) the third most important in the random forests model. 0.1 Conclusions Although we only discussed these methods briefly, that does not mean they are less important. On the contrary, they are essential upper level statistical methods. This brief introduction hopefully helped you know what R is capable of across a wide range of methods. The next chapter begins our “advanced” topics, starting with “Advanced Data Manipulation”. The lavaan package has some great vignettes at http://lavaan.ugent.be/ to help with the other types of models it can handle.↩ Cross-validation is a common way to reduce over-fitting and make sure your model is generalizable.↩ "],
["chapter-8-advanced-data-manipulation.html", "Chapter 8: Advanced Data Manipulation Your Own Functions Vectorized For Loops The apply family The purrr package", " Chapter 8: Advanced Data Manipulation There’s so much more we can do with data in R than what we’ve presented. One of the main ways that R can help you in your research is by looping. Looping, for our purposes, refers to the ability to repeat something across many variables or data sets. There’s many ways of doing this but some are better than others. We will introduce: Vectorized for loops, The apply family of functions, and The purrr package. But first, we need to discuss how to create your own functions. This is very useful for things you do a lot and reduces errors and redundancies in your code. Your Own Functions Let’s create a function that estimates the mean (although it is completely unnecessary since there is already a perfectly good mean() function). mean2 &lt;- function(x){ n &lt;- length(x) m &lt;- (1/n) * sum(x) return(m) } We create a function using the function() function.19 Within the function() we put an x. This is the argument that the function will ask for. Here, it is a numeric vector that we want to take the mean of. We then provide the meat of the function between the {}. Here, we did a simple mean calculation using the length(x) which gives us the number of observations, and sum() which sums the numbers in x. Let’s give it a try: v1 &lt;- c(1,3,2,4,2,1,2,1,1,1) mean2(v1) ## [1] 1.8 mean(v1) ## [1] 1.8 Looks good! These functions that you create can do whatever you need them to (within the bounds that R can do). I recommend by starting outside of a function that then put it into a function. For example, we would start with: n &lt;- length(v1) m &lt;- (1/n) * sum(v1) m ## [1] 1.8 and once things look good, we would put it into a function like we had before with mean2. It is an easy way to develop a good function and test it while developing it. By creating your own function, you can simplify your workflow and can use them in loops. Vectorized By construction, R is the fastest when we use the vectorized form of doing things. For Loops For loops have a bad reputation in the R world. This is because, in general, they are slow. It is among the slowest of ways to iterate (i.e., repeat) functions. We start here to show you, in essence, what the apply family of functions and the purrr package are doing in a much faster way. The apply family The purrr package That seemed like excessive use of the word function… It is important though. So, get used to it!↩ "],
["chapter-9-advanced-plotting.html", "Chapter 9: Advanced Plotting Types of Plots Color Schemes Themes Labels and Titles Facetting", " Chapter 9: Advanced Plotting Once again, we will turn to our friend ggplot2 to plot; but now, we are going to take it to another level. We will use many of the options that this powerful package provides and discuss briefly some important aspects of a good plot. We will go through several aspects of the code that makes plotting in R flexible and beautiful. Types of plots Color schemes Themes Labels and titles Facetting Types of Plots Color Schemes Themes Labels and Titles Facetting "],
["chapter-10-where-to-go-from-here-and-common-pitfalls.html", "Chapter 10: Where to Go from Here and Common Pitfalls Common Pitfalls Quiz", " Chapter 10: Where to Go from Here and Common Pitfalls There are many resources that can aid in developing your R skills from here. We have introduced the basics of R, focusing on the ones that are most important for researchers in the health, social, and behavioral sciences. Since this has been a primer, we hope that you will continue your learning of R via the various sources available at little to no cost. Just like this book, many R books are available online as well as in print. This allows you to explore and learn online at your own pace without having to buy a bunch of books or other resources. Below, we list a handful of R references that we have found to be useful: R for Data Science by Hadley Wickham … Common Pitfalls To end, we wanted to highlight some pitfalls that can plague any beginner to R. We list a few that we’ve encountered, although others surely exist. Document your work. Quiz As a final note, we thought we would give you a quiz to test your memory of the topics we’ve covered. Don’t worry; no pressure to get them all. We’ve included some tougher ones. Regardless of how well you do, we hope you’ll continue improving in your R programming skills. Question 1 If the following code were part of your script would you: scream in fear be as proud as could be neither; I would fix it neither; it works but it isn’t pretty df &lt;- df + mutate(newvar = ifelse(oldvar == 1, 1, 0)) If it is broken, how would you fix it? Question 2 Thanks! "]
]
