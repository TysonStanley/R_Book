[
["chapter-4-basic-analyses.html", "Chapter 4: Basic Analyses ANOVA Linear Modeling When Assumptions Fail", " Chapter 4: Basic Analyses In this chapter we are going to demonstrate basic modeling in R. Lucky for us, R is built for these analyses. It is actually quite straight-forward to run these types of models and analyze the output. Not only that, but there are simple ways to compare models. We will go through the ANOVA family of analyses, the linear regression framework, and look at diagnostics of each. ANOVA ANOVA stands for analysis of variance. It is a family of methods (e.g. ANCOVA, MANOVA) that all share the fact that they compare a continuous dependent variable by a grouping factor variable (and may have multiple outcomes or other covariates). \\[ Y_i = \\alpha_0 + \\alpha_1 \\text{Group}_i + e_i \\] Since the groups are compared using “effect coding,” the \\(\\alpha_0\\) is the grand mean and each of the group level means are compared to it. For more information on the ANOVA family of methods, please refer to … To run an ANOVA model, you can simply use the aov function. In the example below, we are analyzing whether family size (although not fully continuous it is still useful for the example) differs by race. df$race &lt;- factor(df$ridreth1, labels=c(&quot;white&quot;, &quot;black&quot;, &quot;mexicanamerican&quot;, &quot;asian&quot;, &quot;other&quot;)) df$famsize &lt;- as.numeric(df$dmdfmsiz) fit &lt;- aov(famsize ~ race, df) anova(fit) ## Analysis of Variance Table ## ## Response: famsize ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## race 4 1735 433.76 146.87 &lt; 2.2e-16 *** ## Residuals 9751 28798 2.95 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We make sure the variables are the right type, then we use the aov function. Inside of the function we have what is called a formula. It has the general structure: leftside ~ rightside. Generally, the left side is an outcome variable and the right side is the predictor (i.e. independent) variable. Here, we have race predicting famsize. We assign the model to the name fit which is a common way of denoting it is a model. Finally, we use the anova function to output a nice ANOVA table. In the output we see the normal ANOVA table and we can see the p-value (Pr(&gt;F)) is very, very small and thus is quite significant. We can look at how the groups relate using a box plot. We will be using some of the practice you got in Chapter 3 using ggplot2 for this. library(ggplot2) We also would like to make sure the assumptions look like they are being met. In R we can get some simple graphical checks using plot. Linear Modeling Linear regression is nearly identical to ANOVA. In fact, the How it relates to ANOVA Assumptions Compare Models Do it in R When Assumptions Fail There are many things we can try when our assumptions fail. In my opinion, the best and most interpretable way is to use a Generalized Linear Model (GLM) which is discussed in the next chapter. There are a few other things you can try which I’ll show here. But, keep in mind that these things can cause other problems. For example, to fix normality we may accidentally cause heteroskedasticity. With that in mind, here are some common methods to help a model fit better. 0.0.1 Log-Linear, Log-Log, Linear-Log, Other Sounds like a great tongue-twister? Well, it is but it’s also three ways of specifying (i.e. deciding what is in) your model better. Log-Linear is where we adjust the outcome variable by a natural log transformation. This is done easily in R: df$log_outcome &lt;- log(df$outcome) lm(log_outcome ~ var1, data=df) Log-Log is where we adjust both the outcome and the predictor variable with a log transformation. This is also easily done: df$log_outcome &lt;- log(df$outcome) df$log_var1 &lt;- log(df$var1) lm(log_outcome ~ log_var1, data=df) Linear-Log is where we adjsut just the predictor variable with a log transformation. And, you guessed it, this is easily done in R: df$log_var1 &lt;- log(df$var1) lm(outcome ~ log_var1 + var2, data=df) Other methods such as square rooting the outcome or using some power function (e.g. square, cube) are also quite common. There are functions that look for the best transformation to use. However, I will not cover it here since I think GLM’s are better. So if you want to learn about other ways to help your linear model go to the next chapter. "]
]
